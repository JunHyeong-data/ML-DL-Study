# 의사결정나무(Decision Tree) 모델 개요

## 1. 의사결정나무란?
<img width="1494" height="700" alt="image" src="https://github.com/user-attachments/assets/86892b3e-e016-4ce9-b057-592f19cb2d4b" />

의사결정나무(Decision Tree)는  
**데이터에 내재된 패턴을 변수들의 조합으로 표현하여, 예측 또는 분류 문제를 나무(tree) 구조로 모델링하는 방법**이다.

핵심 아이디어는 매우 직관적이다.
- 질문을 하나 던진다 (예 / 아니오)
- 그 대답에 따라 데이터를 둘로 나눈다
- 이 과정을 반복하면서 점점 더 **균일한 데이터 집합**으로 만든다

이 과정이 마치 게임 **‘20 Questions(스무고개)’** 와 유사하다.

---

## 2. 직관적 예시: 20 Questions와 Akinator

20 Questions 게임에서는 어떤 인물이나 사물을 하나 떠올리고, 다른 사람이 **예 / 아니오 질문**만으로 정답을 맞힌다.

의사결정나무도 마찬가지다.
- “여자인가?”
- “한국인인가?”
- “배우인가?”

이러한 질문들이 **트리의 노드(Node)** 가 되고, 질문에 대한 답에 따라 **가지(Branch)** 가 나뉘며, 최종적으로 하나의 **결론(Leaf Node)** 에 도달한다. Akinator와 같은 게임은 실제로 의사결정나무 또는 그 변형된 알고리즘을 사용한다.

---

## 3. 의사결정나무의 기본 구성 요소
<img width="1258" height="1066" alt="image" src="https://github.com/user-attachments/assets/d9c11972-7017-4913-b142-2e4ca8abc407" />

의사결정나무는 다음 세 가지 노드로 구성된다.

### (1) 뿌리 노드 (Root Node)
- 트리의 가장 위에 있는 노드
- 전체 데이터를 포함하며, **항상 하나만 존재**한다.

### (2) 중간 노드 (Internal Node)
- 하나 이상의 분할이 이루어진 노드이며, 아직 최종 결론에 도달하지 않은 상태이다.

### (3) 끝 노드 (Leaf Node)
- 더 이상 분할이 이루어지지 않는 노드이며, 최종 예측 또는 분류 결과를 담고 있다.

---

## 4. 이진 분할(Binary Split)

의사결정나무의 핵심 연산은 **이진 분할**이다.
- 데이터를 **두 개의 부분집합**으로 나눈다.
- 기준은 항상 다음과 같은 형태를 가진다.
  $$x_j \le s \quad \text{또는} \quad x_j > s$$

여기서  
- $x_j$ : 분할에 사용하는 변수  
- $s$ : 분할 기준값 (split point)

---

## 5. 데이터 개수 관점에서 본 분할

예를 들어 전체 데이터가 60개라고 하자.

1. **첫 분할:** 60 → 48, 12 (12는 더 이상 분할하지 않으면 **끝 노드**)
2. **두 번째 분할:** 48 → 23, 25
3. **세 번째 분할:** 23 → 10, 13
4. **네 번째 분할:** 25 → 14, 11

최종적으로 끝 노드에 포함된 데이터 개수는 $10, 13, 14, 11, 12$이며, 이들의 합은 다시 60이 된다.
즉, **전체 데이터는 이진 분할을 통해 여러 개의 균일한 부분집합으로 나뉜다.**

---

## 6. 데이터 공간에서의 직관적 이해
<img width="1144" height="1120" alt="image" src="https://github.com/user-attachments/assets/f4201ac4-6492-487b-80ac-91b48f9640a0" />

### 6.1 2차원 예시
변수가 2개(예: weight, acceleration)인 경우, 의사결정나무는 **공간을 사각형으로 나누는 것**과 같다.
- 처음에는 모든 데이터가 하나의 큰 박스에 있음
- 분할을 통해 작은 박스들로 나뉨
- 각 박스 안의 데이터는 점점 더 **동질적**이 됨



### 6.2 3차원 예시
변수가 3개면, 공간은 사각형이 아니라 **부피(volume)** 로 분할된다.

---

## 7. 공간 분할 vs 트리 표현

의사결정나무는 두 가지 방식으로 표현할 수 있다.

### (1) 데이터 공간 분할
- 2D, 3D 공간에서 박스로 나누어 표현한다.
- 직관적이지만 **차원이 커지면 표현이 불가능**하다.

### (2) 트리(Tree) 구조 표현
- 변수 개수와 무관하게 표현 가능하다.
- 실제로 가장 많이 사용하는 표현 방식이다.

👉 **차원이 커질수록 트리 표현이 필수적이다.**

---

## 8. 예측 나무(Regression Tree)

의사결정나무는 **분류 문제(Classification)**와 **예측 문제(Regression)** 모두에 사용할 수 있다. 이번 장에서는 **예측 나무(Regression Tree)** 에 집중한다.

---

## 9. 예측 나무의 기본 아이디어
<img width="1492" height="1063" alt="image" src="https://github.com/user-attachments/assets/59b1d276-35e4-436a-84a7-8dbb35bee777" />

### 데이터 구성
- 입력 변수: $x_1, x_2$
- 출력 변수: $y$ (연속값)

### 핵심 아이디어
1. 입력 공간을 여러 개의 영역 $R_1, R_2, \dots, R_M$ 으로 분할한다.
2. 각 영역에 대해 하나의 **상수값**으로 예측한다.

---

## 10. 예측 값은 어떻게 정해질까?

어떤 새로운 데이터 $x$ 가 특정 영역 $R_m$ 에 속하면,
> 그 영역에 속한 **관측치들의 $y$ 평균값**으로 예측한다.

즉,
$$\hat{y} = \frac{1}{|R_m|} \sum_{i \in R_m} y_i$$
이 방식이 **제곱 오차(Squared Error)를 최소화**하는 최적의 선택이다.

---

## 11. 함수 형태로 표현한 예측 나무
<img width="1492" height="980" alt="image" src="https://github.com/user-attachments/assets/98669663-a3aa-4fe3-baae-0379a2d3901e" />

예측 나무는 다음과 같이 표현할 수 있다.
$$f(x) = \sum_{m=1}^{M} c_m \cdot I(x \in R_m)$$
<img width="1467" height="1076" alt="image" src="https://github.com/user-attachments/assets/08386501-93c3-4c03-a620-cd95e3f5207c" />

여기서
- $I(\cdot)$ : 인디케이터 함수 (조건이 참이면 1, 거짓이면 0)
- $c_m$ : 영역 $R_m$ 의 평균값
<img width="1472" height="1039" alt="image" src="https://github.com/user-attachments/assets/022218f8-58ef-4d12-85b8-50aa5761ee88" />

---

## 12. 분할 변수와 분할 점은 어떻게 정할까?

의사결정나무 학습의 핵심 질문은 다음 두 가지다.
1. 어떤 변수로 나눌 것인가? (**분할 변수**)
2. 어떤 값에서 나눌 것인가? (**분할 점**)

---

## 13. 분할 기준 선택 방법
<img width="1502" height="1020" alt="image" src="https://github.com/user-attachments/assets/8b2a5eb7-da6b-4813-9384-58d3b7a0db82" />

모든 가능한 경우를 고려한다. 예를 들어, 변수가 $x_1, x_2, x_3$이고 가능한 분할 점이 $\{2, 3, 5, 6, 7\}$이라면, $x_1 \le 2, x_1 \le 3, x_2 \le 5 \dots$ 등 **모든 조합을 시험**한다.

각 경우에 대해 **분할 후 제곱 오차의 합(RSS)** 을 계산하고 가장 작은 경우를 선택한다.
👉 이는 **그리디 서치(Greedy Search)** 방식이다.

---

## 14. 전체 학습 과정 요약

1. 모든 데이터를 포함하는 뿌리 노드에서 시작한다.
2. 모든 분할 후보를 평가한다.
3. 오차를 최소화하는 분할을 선택한다.
4. 분할된 각 노드에서 동일한 과정을 반복한다.
5. 종료 조건에 도달하면 끝 노드를 생성한다.

---

## 15. 정리
- 의사결정나무는 질문 기반 모델이다.
- 이진 분할을 통해 데이터를 점점 균일하게 만든다.
- 예측 나무에서는 리프 노드의 평균값으로 예측한다.
- 차원이 커질수록 트리 표현이 강력하다.

👉 **다음 강의:** 분류 나무(Classification Tree)와 정보 이득(Information Gain), 엔트로피, 지니 지수에 대해 다룬다.
