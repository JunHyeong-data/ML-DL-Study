# 분류 의사결정나무 (Classification Tree)

## 1. 강의 개요
이번 강의에서는 의사결정나무(Decision Tree) 중 목표변수 $y$가 범주형(categorical)인 경우, 즉 **분류 나무(Classification Tree)**를 다룬다. 지난 강의에서는 $y$가 연속형인 예측(회귀) 나무를 다루었으며, 이번에는 분류 문제의 메커니즘에 초점을 맞춘다.

---

## 2. 의사결정나무의 기본 개념 복습
<img width="1500" height="992" alt="image" src="https://github.com/user-attachments/assets/6e318097-c69c-4fbb-8960-e8c8f139ef6a" />

### (1) 데이터 구조
- **입력 변수:** $\mathbf{x} = (x_1, x_2, \dots, x_p)$
- **출력 변수:** - 회귀 나무: $y \in \mathbb{R}$
  - 분류 나무: $y \in \{1, 2, \dots, K\}$

### (2) 핵심 아이디어
의사결정나무의 핵심은 데이터를 점점 더 **‘균일(homogeneous)’**하게 분할하는 것이다.
- **회귀 나무:** 비슷한 값을 갖는 관측치끼리 묶음
- **분류 나무:** 비슷한 범주(class)를 갖는 관측치끼리 묶음

---

## 3. 트리 구조 용어
- **루트 노드 (Root Node):** 트리의 시작점
- **중간 노드 (Internal Node):** 분할이 일어나는 노드
- **끝 노드 / 터미널 노드 (Terminal Node, Leaf Node):** 더 이상 분할되지 않는 노드이며, 최종 예측이 이루어지는 지점이다.

---

## 4. 이진 분할(Binary Split)
모든 분할은 이진 분할로 이루어진다.
$$x_j \le s \quad \text{vs} \quad x_j > s$$
- $x_j$: 분할에 사용할 변수
- $s$: 분할 기준값(split point)

---

## 5. 분류 나무의 직관적 예시
<img width="1472" height="958" alt="image" src="https://github.com/user-attachments/assets/542a5e4f-ea7f-42d1-b2a7-eaf768a3c396" />

1. **데이터 상황:** 입력 변수 $x_1, x_2$, 출력 변수는 범주형(예: 빨간색/초록색)이다. 초기에는 두 범주가 섞여 있다.
2. **분할 과정:** $x_1 \le t_1$ 기준으로 1차 분할 후, 여전히 혼재된 영역에 대해 $x_2 \le t_2$ 기준으로 추가 분할을 수행한다.
3. **결과:** 반복적 분할을 통해 각 영역이 특정 범주에 치우치도록 여러 개의 작은 영역(region)을 만든다.



---

## 6. 새로운 데이터의 분류 방법
새로운 관측치 $\mathbf{x}_{new}$가 들어오면:
1. 루트 노드부터 시작하여 분할 조건을 따라 내려간다.
2. 하나의 끝 노드 $R_m$에 도달한다.
3. 해당 노드의 **다수 클래스(majority class)**로 분류한다.

---

## 7. 수학적 표기 (Notation)
- **클래스 집합:** $\{1, 2, \dots, K\}$
- **터미널 노드:** $R_m$ ($m$번째 터미널 노드)
- **관측치 개수:** $N_m$ (노드 $R_m$에 속한 데이터 수)
- **클래스 비율:** $$p_{mk} = \frac{\text{노드 } R_m \text{에서 클래스 } k \text{인 관측치 수}}{N_m}$$

---

## 8. 노드에서의 분류 규칙 (Majority Vote)

노드 $R_m$에서의 최종 분류 클래스 $\hat{k}_m$은 가장 비율이 높은 클래스를 선택한다.
$$\hat{k}_m = \arg\max_{k} p_{mk}$$

---

## 9. 분류 나무의 함수 표현
<img width="1503" height="1079" alt="image" src="https://github.com/user-attachments/assets/7341b9b2-a96e-4c09-88b3-33be7955676b" />
<img width="1449" height="926" alt="image" src="https://github.com/user-attachments/assets/24d8b523-a377-4e0e-b07d-2557e6dfbe58" />

$$\hat{y}(\mathbf{x}) = \sum_{m=1}^{M} \hat{k}_m \cdot \mathbf{1}(\mathbf{x} \in R_m)$$
- $M$: 터미널 노드 개수
- $\mathbf{1}(\cdot)$: 인디케이터 함수 (조건 만족 시 1, 아니면 0)

---

## 10. 분류 나무의 비용 함수 (Impurity Measure)
<img width="1498" height="1122" alt="image" src="https://github.com/user-attachments/assets/027293c0-cb2b-46df-a912-0b5d4346068e" />

분류 문제에서는 수치적 오차 대신 **불순도(impurity)**를 사용한다.

### (1) 오분류율 (Misclassification Rate)
$$\text{Error}(R_m) = 1 - \max_k p_{mk}$$
- 가장 단순하지만 실제 학습(미분 등)에는 잘 사용되지 않는다.

### (2) 지니 지수 (Gini Index)
$$G(R_m) = \sum_{k=1}^{K} p_{mk}(1 - p_{mk})$$
- 값이 작을수록 순수하며, CART 알고리즘의 기본 지표이다.

### (3) 엔트로피 (Entropy)
$$H(R_m) = -\sum_{k=1}^{K} p_{mk} \log p_{mk}$$
- 정보이론 기반의 불확실성 척도이다.

---

## 11. 분할 변수와 분할점 선택
<img width="1467" height="906" alt="image" src="https://github.com/user-attachments/assets/b46829bd-7f21-4b93-915f-3966e0e5016e" />
<img width="1515" height="531" alt="image" src="https://github.com/user-attachments/assets/4289ac04-77db-45d2-baa2-82c7c0c8e72f" />

분할 후 전체 불순도가 최소가 되도록 하는 변수 $j$와 기준값 $s$를 찾는다.
$$\min_{j, s} \Big[ \frac{N_L}{N} I(R_L) + \frac{N_R}{N} I(R_R) \Big]$$
- $I(\cdot)$: 불순도 함수 (Gini, Entropy 등)
<img width="1385" height="973" alt="image" src="https://github.com/user-attachments/assets/57a44dd1-cc15-4def-9172-2963219d9dd3" />
<img width="1493" height="1066" alt="image" src="https://github.com/user-attachments/assets/dc7e9f4e-c9ef-4b93-8ddd-a9cc45617e81" />

---

## 12. 정보 이득 (Information Gain)
<img width="1480" height="870" alt="image" src="https://github.com/user-attachments/assets/03802c69-a006-466d-88bf-40a6c545a7b9" />
<img width="1523" height="1046" alt="image" src="https://github.com/user-attachments/assets/aedfc187-3716-4a82-bfa3-b251c382cdb7" />

엔트로피 감소량이 클수록 중요한 변수로 판단한다.
$$IG(S, A) = H(S) - \sum_{v \in A} \frac{|S_v|}{|S|} H(S_v)$$

---

## 13. 분류 나무의 한계와 해결책
<img width="1464" height="1026" alt="image" src="https://github.com/user-attachments/assets/14a905af-e2d5-4285-9e14-3cf585d9ca57" />

- **한계:** 데이터의 작은 변화에 민감하며(High Variance), 깊어질수록 과적합(Overfitting) 위험이 크다.
- **해결책 (랜덤 포레스트):** 여러 개의 트리를 생성하여 결과를 앙상블(투표)함으로써 분산을 감소시키고 성능을 안정화한다.

---

## 14. 정리
- 분류 나무는 범주형 $y$를 예측하는 모델이다.
- 이진 분할과 불순도 최소화 과정을 통해 다수 클래스를 예측한다.
- 단일 트리의 불안정성은 랜덤 포레스트 등을 통해 보완한다.
