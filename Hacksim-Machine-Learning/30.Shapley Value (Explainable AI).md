# 📘 Explainable AI (XAI) & SHAP Value 쉽게 이해하기

## 1. Explainable AI란 무엇인가?

### ✅ Explainable AI (설명 가능한 인공지능)

* **Explainable AI (XAI)**란
  👉 *모델이 왜 그런 예측을 했는지 설명할 수 있는 인공지능*을 의미한다.

### ❓ 왜 필요한가?

요즘 많이 쓰이는 딥러닝 모델은 보통 다음과 같은 구조를 가진다.

```
입력층 → 여러 개의 은닉층(Hidden Layer) → 출력층
```

* 예측 성능은 매우 좋음
* 하지만 문제는 ❗
  **왜 그런 예측이 나왔는지 알기 어렵다**
* 이런 모델을 흔히 **블랙박스 모델(Black-box Model)**이라고 부른다.

📌 **XAI의 목표**

* 예측 결과는 유지하면서
* *왜 이런 결과가 나왔는지 설명 가능하게 만들자*

---

## 2. XAI 방법론 중 하나: SHAP Value
<img width="1391" height="886" alt="image" src="https://github.com/user-attachments/assets/dab7edd1-2107-4eb0-9016-ae443f954f2c" />

### ✅ SHAP Value란?

* **SHAP (SHapley Additive exPlanations)**
* 2017년 NIPS(현 NeurIPS)에서 발표
* **게임 이론(Game Theory)** 기반의 설명 기법

📌 핵심 아이디어

> *각 입력 변수가 예측 결과에 얼마나 기여했는지 수치로 나타내자*

---

## 3. SHAP Value를 직관적으로 이해하는 예제
<img width="1507" height="1070" alt="image" src="https://github.com/user-attachments/assets/ca7862ff-69a5-4fe4-ab2e-42233168d005" />

### 🎓 시험 점수 예제

#### 상황

* 민철, 은영, 지연, 지오
* 네 명이 함께 시험을 봐서 **100점**

| 빠진 사람 | 시험 점수 |
| ----- | ----- |
| 없음    | 100   |
| 민철    | 20    |
| 은영    | 80    |
| 지연    | 70    |
| 지오    | 99    |

#### 질문

> **누가 시험 점수에 가장 크게 기여했을까?**

#### 해석

* 민철이 빠지면 점수가 **100 → 20**
* 지오가 빠져도 **100 → 99**

📌 결론
👉 **민철의 기여도가 가장 크다**

이 개념이 바로 **SHAP Value의 핵심 아이디어**다.

---

## 4. SHAP Value의 핵심 개념 정리

* 어떤 변수를 **제외했을 때**
* 예측 값이 **얼마나 변하는지**를 본다
* 모든 가능한 변수 조합을 고려해서
* 공정하게 평균을 낸다

👉 그래서 **한 변수의 공헌도**를 수치로 표현 가능

---

## 5. 간단한 수치 예제로 SHAP 계산 이해하기
<img width="1467" height="1100" alt="image" src="https://github.com/user-attachments/assets/d4fd4746-52c6-4e58-b107-d8b9cc9c297a" />
<img width="1507" height="992" alt="image" src="https://github.com/user-attachments/assets/53dbc9b2-ab75-4266-a4a1-50802f2e95a3" />
<img width="1517" height="983" alt="image" src="https://github.com/user-attachments/assets/a5ec59c1-080f-489e-a5d7-c8fed9d3e879" />
<img width="1538" height="967" alt="image" src="https://github.com/user-attachments/assets/17042f0a-658b-444a-b144-b5c3f37774c9" />
<img width="1364" height="1047" alt="image" src="https://github.com/user-attachments/assets/df7fb797-94bf-40fa-a442-8e23dcef3880" />
<img width="1479" height="992" alt="image" src="https://github.com/user-attachments/assets/ea62b4a4-148a-41a0-8669-005189ef6775" />
<img width="1436" height="1077" alt="image" src="https://github.com/user-attachments/assets/6bbde608-7551-4826-9e6a-8d38d41977b8" />
<img width="1435" height="992" alt="image" src="https://github.com/user-attachments/assets/0af31f65-6cbd-47b8-b799-76a8f29a7fca" />

### 🎯 가정

* 변수 3개: `x1, x2, x3`
* 총 경우의 수: `2³ = 8`

| Case | 사용 변수      | 예측값 |
| ---- | ---------- | --- |
| 1    | 없음         | 28  |
| 2    | x1         | 32  |
| 3    | x2         | 30  |
| 4    | x3         | 31  |
| 5    | x1, x2     | 33  |
| 6    | x1, x3     | 35  |
| 7    | x2, x3     | 32  |
| 8    | x1, x2, x3 | 39  |

---

### 🔍 x1의 SHAP Value 계산 아이디어

1. **x1이 포함된 경우**
2. **x1을 뺐을 때 예측값 차이**
3. 모든 경우를 고려
4. 가중 평균 계산

📌 결과

* `SHAP(x1) = 3`
* `SHAP(x2) = 2`
* `SHAP(x3) = 2`

👉 **x1이 가장 중요한 변수**

---

## 6. SHAP Value의 중요한 성질

### ✅ 합의 법칙 (Additivity)

```
SHAP(x1) + SHAP(x2) + SHAP(x3)
=
(모든 변수 사용 예측값) - (아무 변수도 없을 때 예측값)
```

즉,

```
3 + 2 + 2 = 39 - 28 = 11
```

📌 **SHAP Value는 예측을 정확히 분해한다**

---

## 7. 실제 데이터 예제: 사망률 분석
<img width="1473" height="1102" alt="image" src="https://github.com/user-attachments/assets/6771dfa0-24ae-45c3-a9c2-f1e21976c257" />

### 📊 SHAP Summary Plot 해석

* **색상**

  * 🔴 빨강: 값이 큼
  * 🔵 파랑: 값이 작음

* **x축**

  * 오른쪽: 사망률 ↑
  * 왼쪽: 사망률 ↓

#### 해석 예시

* 나이 ↑ → 사망률 기여 ↑
* 남성 → 사망률 기여 ↑
* BMI 낮음 → 사망률 기여 ↑

📌 의료적으로 맞는지 여부와는 별개
👉 **모델이 이렇게 판단했다는 것**

---

## 8. 이미지 데이터에서의 SHAP
<img width="1429" height="1099" alt="image" src="https://github.com/user-attachments/assets/a97b6080-d377-47f8-a59b-6140493a9149" />

### 🖼️ 이미지 분류 예제

* 고양이 vs 미어캣

📌 SHAP 활용

* 픽셀 하나하나를 변수로 간주
* 어떤 픽셀이 분류에 중요한지 시각화

🔥 **붉은 영역**

* 모델이 중요하게 본 부분

---

## 9. SHAP vs LIME 비교

| 구분     | SHAP   | LIME   |
| ------ | ------ | ------ |
| 데이터 범위 | 전체 데이터 | 국소적    |
| 안정성    | 높음     | 비교적 낮음 |
| 계산량    | 큼      | 작음     |
| 이론적 기반 | 게임 이론  | 근사 모델  |

---

## 10. SHAP의 장점과 단점
<img width="1496" height="1012" alt="image" src="https://github.com/user-attachments/assets/9c941d53-17ca-497d-8efc-d5eb294d0ba2" />

### ✅ 장점

* 모델 종류와 무관 (Model-Agnostic)
* 개별 관측치 설명 가능
* 평균 → 전체 변수 중요도 가능

### ❌ 단점

* 계산 비용 큼
* 변수 많을수록 느림
* **인과관계 해석 불가**

⚠️ 중요

> SHAP은 **원인 분석이 아니라 결과 해석 도구**다

---

## 11. SHAP의 위치 정리

* SHAP은 **사후 해석(Post-hoc Explanation)**
* 모델 학습이 끝난 후 분석
* 인과관계 ❌
* 예측 기여도 ⭕

---

## 12. Model-Agnostic Explainability

### 모델과 무관한 해석 기법

* SHAP
* LIME

### 모델 내부 구조 활용

* Attention
* Feature Importance (Tree 기반)

---

## ✨ 한 줄 요약

> **SHAP Value는 “이 예측에서 어떤 변수가 얼마나 기여했는가”를 공정하게 계산하는 설명 기법이다.**

---
