# K-최근접 이웃 (K-Nearest Neighbors, KNN)

## 1. 머신러닝 방법론의 분류
분류 및 예측을 위한 머신러닝 방법은 크게 두 가지로 나뉜다.
<img width="1508" height="898" alt="image" src="https://github.com/user-attachments/assets/3a95570a-f6d3-4b85-b0e1-f14c3b1642bb" />

### 1.1 모델 기반 학습 (Model-Based Learning)
학습 데이터를 이용해 모델을 먼저 생성하고, 이후 새로운 데이터가 들어오면 모델을 통해 예측한다.
- **예시:** 선형 회귀, 로지스틱 회귀, 신경망, 의사결정나무, 서포트 벡터 머신
- **과정:** 학습 데이터로 모델 학습 $\rightarrow$ 모델 완성 $\rightarrow$ 새로운 데이터 예측

### 1.2 인스턴스 기반 학습 (Instance-Based Learning)
모델을 미리 만들지 않고, 새로운 데이터가 들어올 때마다 학습 데이터를 직접 활용해 예측한다.
👉 **KNN은 인스턴스 기반 학습에 해당한다.**

---

## 2. KNN의 기본 개념
<img width="1440" height="947" alt="image" src="https://github.com/user-attachments/assets/a29de213-873d-470e-b3d9-0e375fe2ba95" />

KNN은 새로운 데이터가 주어졌을 때 **가장 가까운 K개의 학습 데이터(이웃)**를 기준으로 분류 또는 예측을 수행하는 알고리즘이다.
- $K$: 고려할 이웃의 개수
- “모델이 없는 모델”이라고도 불림

---

## 3. KNN의 특징 (학습 관점)
<img width="1440" height="904" alt="image" src="https://github.com/user-attachments/assets/558ac2bd-256e-4b2e-ae08-50537d528384" />

- **Instance-Based Learning:** 관측치 자체를 사용해 예측
- **Memory-Based Learning:** 모든 학습 데이터를 저장하고 사용
- **Lazy Learning:** 사전에 학습하지 않고, 테스트 데이터가 들어올 때 계산 수행

---

## 4. KNN 분류 (Classification)
### 4.1 기본 아이디어
1. 새로운 데이터 $x$ 선택
2. 학습 데이터와의 거리 계산
3. 가장 가까운 $K$개 이웃 선택
4. **다수결(Majority Vote)**로 클래스 결정



### 4.2 예시
<img width="1400" height="1059" alt="image" src="https://github.com/user-attachments/assets/59d36d04-fe9a-48c6-9ebe-be694f614e71" />

- **$K = 1$**: 가장 가까운 1개의 데이터 클래스 사용
- **$K = 3$**: 가장 가까운 3개의 데이터 중 가장 많은 클래스 선택

---

## 5. KNN 예측 (Regression)
KNN은 연속형 $Y$값 예측에도 사용 가능하다.
<img width="1492" height="1068" alt="image" src="https://github.com/user-attachments/assets/6bd187d5-0313-43cd-b998-f4707fcc2b52" />
<img width="1436" height="667" alt="image" src="https://github.com/user-attachments/assets/3e7ff3a1-4d2d-4070-9839-e7df707f9a22" />

### 5.1 기본 아이디어
- 가장 가까운 $K$개 이웃 선택 후 이웃들의 $Y$값을 이용해 예측

### 5.2 예측 방법
- **기본값: 평균(mean)**
$$\hat{y} = \frac{1}{K}\sum_{i=1}^{K} y_i$$
※ 중앙값, 최소값 등도 가능하지만 경험적으로 평균이 가장 성능이 좋음

---

## 6. 하이퍼파라미터 (Hyperparameters)
KNN에는 학습 파라미터는 없고, 하이퍼파라미터만 존재한다.
<img width="1399" height="657" alt="image" src="https://github.com/user-attachments/assets/a8006095-1d0e-43a5-9d3f-22bd415a2661" />

### 6.1 K (이웃의 개수)
<img width="1380" height="900" alt="image" src="https://github.com/user-attachments/assets/8d64dd15-3572-4f05-a42d-b0527841f3ab" />

- **$K$가 작을수록:** 로컬 패턴 반영, 과적합(overfitting) 위험
- **$K$가 클수록:** 글로벌 패턴 반영, 과소적합(underfitting) 위험

### 6.2 K 선택 방법
<img width="1428" height="1071" alt="image" src="https://github.com/user-attachments/assets/2dd383ac-6f7f-4f32-9056-993f6fcf38b8" />
- 이론적으로 정해진 값은 없음
- Trial & Error, Grid Search
- 훈련/검증 데이터 오류를 비교하여 선택

---

## 7. 성능 평가
### 7.1 분류 문제
- **Misclassification Error**
$$\text{Error} = \frac{1}{n}\sum I(y_i \neq \hat{y}_i)$$

### 7.2 예측 문제
- **SSE (Sum of Squared Errors)**
$$\sum (y_i - \hat{y}_i)^2$$

---

## 8. K 값에 따른 성능 변화

- **$K \downarrow$:** Training Error $\downarrow$, Testing Error $\uparrow$ (과적합)
- **$K \uparrow$:** Training Error $\uparrow$, Testing Error $\downarrow \rightarrow$ 다시 $\uparrow$ (과소적합)
👉 **Training & Testing Error가 동시에 작은 $K$ 선택**



---

## 9. 거리 측도 (Distance Metric)
KNN의 핵심은 거리 계산이다.

### 9.1 유클리드 거리 (Euclidean Distance)
<img width="1476" height="911" alt="image" src="https://github.com/user-attachments/assets/f668e666-3616-4e11-b888-15a0c7ff838e" />

$$d(x,y) = \sqrt{\sum (x_i - y_i)^2}$$
- 직선 거리, 가장 많이 사용됨

### 9.2 맨하탄 거리 (Manhattan Distance)
<<img width="1404" height="1018" alt="image" src="https://github.com/user-attachments/assets/619b53ec-ae43-475c-9949-96264b55a503" />

$$d(x,y) = \sum |x_i - y_i|$$
- 격자 이동 거리, 유클리드 거리보다 항상 크거나 같음

### 9.3 마할라노비스 거리 (Mahalanobis Distance)
<img width="1410" height="1075" alt="image" src="https://github.com/user-attachments/assets/a685d407-8f75-4e73-a4d6-bb3b72220c95" />

<img width="1474" height="1065" alt="image" src="https://github.com/user-attachments/assets/5b0c441a-b3b7-4474-bc75-15d74b701dfd" />

$$d(x,y) = \sqrt{(x-y)^T \Sigma^{-1}(x-y)}$$
- 분산·공분산 고려, 상관관계가 있는 데이터에 효과적

### 9.4 코릴레이션 거리 (Correlation Distance)
<img width="1461" height="1002" alt="image" src="https://github.com/user-attachments/assets/0bb12b5b-a232-4aa3-aafd-82f75eb98855" />

$$d = 1 - \text{corr}(x,y)$$
- 전체 패턴 유사도 비교, 시계열이나 신호 데이터에 적합
<img width="1459" height="962" alt="image" src="https://github.com/user-attachments/assets/6c54bd84-8946-476e-94f4-a1d8cb0f0f7a" />

<img width="1508" height="1037" alt="image" src="https://github.com/user-attachments/assets/3b3e94c9-5326-450b-9321-0df53c498997" />

---

## 10. 데이터 스케일 문제
- 변수마다 스케일이 다르면 거리 왜곡이 발생하므로 **정규화 / 표준화 필수**

---

## 11. Weighted KNN
<img width="1464" height="935" alt="image" src="https://github.com/user-attachments/assets/fe7f5b8f-348f-4271-8ef9-e24503c057bd" />

<img width="1464" height="972" alt="image" src="https://github.com/user-attachments/assets/329bcde1-d476-4cdb-9618-19b270b6ac93" />

<img width="1420" height="949" alt="image" src="https://github.com/user-attachments/assets/da6cd99c-cf73-4d07-b0fe-27a20243dde3" />

- **기본 KNN:** $\hat{y} = \text{mean}(y_1, y_2, \dots, y_K)$
- **Weighted KNN:** 가까운 이웃에 더 큰 가중치 부여 (거리의 역수 등 활용)

---

## 12. KNN의 장단점
<img width="1484" height="1000" alt="image" src="https://github.com/user-attachments/assets/640e7495-960e-4612-acd0-762941b4b33a" />

### 장점
- 개념이 단순함
- 모델 가정이 없음 (Non-parametric)
- 비선형 패턴 처리 가능

### 단점
- 계산량이 큼 (모든 데이터 거리 계산)
- 고차원 데이터에 취약 (차원의 저주)
- $K$와 거리 선택이 어려움

---

## 13. 요약
<img width="1485" height="1015" alt="image" src="https://github.com/user-attachments/assets/0a335313-9960-432a-b6ce-3e4691e865e9" />


- KNN은 모델을 만들지 않는 인스턴스 기반 학습
- 거리 기반 분류·예측 수행
- 핵심 결정 요소: $K$, 거리 측도
