# 불균형 데이터 처리 (Imbalanced Data Handling)

---

## 1. 불균형 데이터란?
<img width="1485" height="973" alt="image" src="https://github.com/user-attachments/assets/be965641-5bdd-4022-9ec4-34e453bb8797" />

**불균형 데이터(Imbalanced Data)** 란 클래스(class) 간 관측치 개수 차이가 매우 큰 데이터를 말한다.

---

### ✔ 정의

> 클래스별 관측치 수가 **현저하게 차이 나는 데이터**

$$|C_1| \gg |C_2|$$

---

### ✔ 용어 정리

- 범주 (Category)
- 클래스 (Class)
- 👉 동일한 의미로 사용됨

---

## 2. 불균형 데이터 예시

---

### 🏥 의료 분야
- 정상인 $\gg$ 질병 환자
- 우리가 관심 있는 대상: **질병 환자 (소수 클래스)**

### 🏭 제조업
- 양품 $\gg$ 불량품
- 관심 대상: **불량품**

### 💳 금융·카드사
- 정상 거래 $\gg$ 사기 거래
- 관심 대상: **사기·이상 거래**

📌 현실 세계의 대부분 데이터는 **균형 데이터보다 불균형 데이터가 훨씬 많다.**

---

## 3. 불균형 데이터의 문제점

---

### ✔ 무엇이 문제인가?
- 정상 분류도 중요하지만 일반적으로는 **소수 클래스(이상)를 정확히 찾는 것이 더 중요**하다.
- **이유:** 소수 클래스는 대부분 질병, 불량, 사기, 이상 징후처럼 **실제 관심 대상**이기 때문이다.

---

## 4. 분류 경계선 문제
<img width="1443" height="1103" alt="image" src="https://github.com/user-attachments/assets/0fb50cc9-ec43-4e14-b551-ed1c7d63a32b" />

---

### 데이터 분포 예
- 파란색: 정상 (다수 클래스)
- 빨간색: 이상 (소수 클래스)

### 이상적인 분류 경계선
- 미래에 나타날 수 있는 이상치까지 고려하여 정상과 이상을 균형 있게 분리한다.

### 현실적인 문제
- 학습 데이터에는 미래 이상치가 존재하지 않고 다수 클래스가 지나치게 많다.
👉 결과적으로: **분류 경계선이 정상 클래스 쪽으로 편향됨**



### 결과
- 실제 이상 데이터임에도 정상으로 오분류(False Negative)하는 상황이 발생한다.

---

## 5. 정확도(Accuracy)의 함정
<img width="1455" height="1060" alt="image" src="https://github.com/user-attachments/assets/ae3c4940-1164-4a4a-9a0b-84550d69cddc" />

---

### 혼동 행렬 예시

| 실제 / 예측 | 정상 | 이상 |
| :--- | :--- | :--- |
| **정상** | 10000 | 0 |
| **이상** | 5 | 5 |

### 정확도 계산
$$Accuracy = \frac{TP + TN}{Total} = \frac{10000 + 5}{10010} \approx 0.999$$

### 문제점
<img width="1300" height="855" alt="image" src="https://github.com/user-attachments/assets/8aa43439-0407-4fa9-9583-df709952dc1e" />

- 정확도는 99.9%로 매우 높지만, 정작 중요한 이상 데이터는 절반밖에 못 맞춘다.
📌 결론: **불균형 데이터에서는 정확도를 그대로 신뢰하면 안 된다.**

---

## 6. 불균형 데이터 해결 전략

---
<img width="1494" height="986" alt="image" src="https://github.com/user-attachments/assets/f20e5371-b6c9-41bb-b706-a4fb188a0881" />

### 크게 두 가지 접근
1. **데이터 조정** (Sampling)
2. **모델 자체 조정** (Cost-sensitive, One-class classification 등)

👉 본 강의에서는 **샘플링 기법** 중심으로 설명한다.

---

# 7. 샘플링 기법 개요

---

샘플링은 두 가지로 나뉜다.
<img width="1486" height="1041" alt="image" src="https://github.com/user-attachments/assets/bdc9f4b5-8c18-471a-a1a0-25e1ce934573" />

[Image comparing under-sampling and over-sampling techniques in machine learning]

- **🔹 언더 샘플링 (Under Sampling):** 다수 클래스 감소 $\rightarrow$ 소수 클래스와 개수 맞춤
- **🔹 오버 샘플링 (Over Sampling):** 소수 클래스 증가 $\rightarrow$ 다수 클래스와 개수 맞춤

---

# 8. 언더 샘플링 (Under Sampling)

---

## 8.1 Random Under Sampling
<img width="1453" height="958" alt="image" src="https://github.com/user-attachments/assets/aeb06e36-a6db-4c86-8f8c-4f3833f92cf3" />

- **개념:** 다수 클래스에서 무작위로 관측치 제거
- **특징:** 구현이 간단하지만 중요한 정보가 제거될 위험이 있다.

---

## 8.2 Tomek Links
<img width="1495" height="1106" alt="image" src="https://github.com/user-attachments/assets/c8e706a2-08e0-445c-afc5-c506f3395147" />
<img width="1504" height="897" alt="image" src="https://github.com/user-attachments/assets/5d16201d-34be-4a04-8b89-98e2352d82f3" />

- **아이디어:** 서로 다른 클래스의 두 점 $x_i, x_j$ 사이에 제3의 점이 존재하지 않을 때, 이를 Tomek Link라 한다.
- **수식:** $$d(x_i, x_j) < d(x_i, x_k) \quad \forall k$$
- **처리:** Tomek Link에 해당하는 **다수 클래스 관측치 제거** $\rightarrow$ 클래스 경계가 명확해짐



---

## 8.3 CNN (Condensed Nearest Neighbor)
<img width="1480" height="870" alt="image" src="https://github.com/user-attachments/assets/4e9b49f1-cb57-42d4-9441-392d34256143" />
<img width="1455" height="939" alt="image" src="https://github.com/user-attachments/assets/af5a4f6d-6f22-4a7d-b274-f02bf491dde5" />
<img width="1500" height="880" alt="image" src="https://github.com/user-attachments/assets/0d035a04-fe36-4cf3-9121-c195e7ca3610" />
<img width="1452" height="916" alt="image" src="https://github.com/user-attachments/assets/d13d9644-bd0c-4b51-a91b-fcecde59ffdd" />

- **절차:** 소수 클래스는 전체 유지, 다수 클래스는 하나만 랜덤 선택 후 1-NN 분류 시 오분류되는 데이터만 남긴다.
- **핵심:** 반드시 **1-NN**을 사용해야 한다. (k가 크면 소수 클래스 영향력이 사라져 제거 대상이 안 생김)

---

## 8.4 OSS (One-Sided Selection)
<img width="1461" height="964" alt="image" src="https://github.com/user-attachments/assets/213ed908-f46a-49fd-b3a3-36ae013f8757" />
<img width="1500" height="889" alt="image" src="https://github.com/user-attachments/assets/93deaf08-ddf5-49cf-b57e-94f9c166e1ff" />

- **개념:** **Tomek Links + CNN 결합**
- **처리:** Tomek Links로 경계를 정제하고, CNN으로 불필요한 다수 클래스 내부 데이터를 제거한다.

<img width="1440" height="866" alt="image" src="https://github.com/user-attachments/assets/eb1e1848-1ad7-4a06-9fa6-f0d121874734" />

---

# 9. 오버 샘플링 (Over Sampling)

---

## 9.1 Random Over Sampling
<img width="1470" height="906" alt="image" src="https://github.com/user-attachments/assets/e3eb60ae-4da6-40b4-8470-761fc3a27d7f" />
<img width="1483" height="888" alt="image" src="https://github.com/user-attachments/assets/460a5bbd-9bc1-4241-9992-c6b41f2b699f" />
<img width="1371" height="908" alt="image" src="https://github.com/user-attachments/assets/26564635-3aa3-4d6e-9f9d-0be756f4c64a" />
<img width="1048" height="654" alt="image" src="https://github.com/user-attachments/assets/74328aa6-2b9d-44a7-87de-74767e78307b" />

- **개념:** 소수 클래스 데이터를 **단순 복제**
- **단점:** 과적합(Overfitting) 위험이 크다.

---

# 10. SMOTE (Synthetic Minority Over-sampling Technique)
<img width="1504" height="943" alt="image" src="https://github.com/user-attachments/assets/f64bbf0f-e6b2-48f0-9147-25449af3a4f2" />

---

### 핵심 아이디어
> 단순 복제가 아니라 **가상의 새로운 데이터를 생성하자**

---

## 10.1 SMOTE 절차
<img width="1461" height="957" alt="image" src="https://github.com/user-attachments/assets/934d569e-5752-447e-a499-7d5f2d754586" />
<img width="1463" height="807" alt="image" src="https://github.com/user-attachments/assets/ccea2e4b-babc-40d0-ba0b-742c14f80a92" />
<img width="1463" height="968" alt="image" src="https://github.com/user-attachments/assets/32f6d7e3-9903-41f7-8d57-f2113531d489" />
<img width="1448" height="951" alt="image" src="https://github.com/user-attachments/assets/07aa84c2-5e72-4a9c-9da6-8d9981edfe8f" />
<img width="1489" height="870" alt="image" src="https://github.com/user-attachments/assets/9dca6e52-5dbb-4eed-a032-aa4e16f13b49" />

1. 소수 클래스 샘플 $x_i$ 선택
2. $k$개의 최근접 소수 클래스($k \ge 2$) 중 하나인 $x_{nn}$ 랜덤 선택
3. 다음 식으로 가상 데이터 생성:
$$x_{new} = x_i + \lambda (x_{nn} - x_i), \quad \lambda \sim U(0,1)$$



---

## 10.2 k = 1 이면 안 되는 이유
<img width="1413" height="912" alt="image" src="https://github.com/user-attachments/assets/7e286b97-2b91-426d-babd-58ba47913df6" />

- $k=1$이면 데이터가 새로운 평면을 형성하지 못하고 일직선 형태로만 생성되어 분포 확장의 의미가 퇴색된다. 반드시 **$k \ge 2$**여야 한다.

---

# 11. 전체 요약

---

## ✔ 불균형 데이터 핵심 정리

| 방법 | 설명 |
| :--- | :--- |
| **언더 샘플링** | 다수 클래스 감소 (Tomek Links, OSS 등) |
| **오버 샘플링** | 소수 클래스 증가 (Random, SMOTE 등) |
| **SMOTE** | 가상 데이터 생성으로 과적합 방지 |

### 실무 추천
- 소규모 데이터 $\rightarrow$ SMOTE
- 대규모 데이터 $\rightarrow$ 언더 샘플링
- 항상 여러 기법을 시도한 후 성능을 비교해야 한다.

✅ **불균형 데이터 처리는 분류 문제에서 가장 중요한 전처리 단계 중 하나이다.**

# 불균형 데이터 처리 (2)
## Borderline-SMOTE · ADASYN · GAN · Cost-sensitive · One-class Classification

---

# 1. SMOTE의 한계

기본 **SMOTE**는 모든 소수 클래스 샘플에 대해 동일하게 오버샘플링을 수행하며, 클래스 내부 깊숙한 영역까지 동일하게 증폭시킨다는 특징이 있다.

### ❗ 문제점
- 클래스 내부 중앙부 샘플은 이미 정보가 충분하다.
- 실제 분류기가 학습하기 어려운 지점은 클래스 간의 **경계(boundary)** 영역이다.

📌 따라서 다음과 같은 의문이 생긴다.
> “아무 데나 샘플링하지 말고 **경계선 근처에서만 샘플링하면 더 좋지 않을까?**”

---

# 2. Borderline-SMOTE
<img width="1481" height="944" alt="image" src="https://github.com/user-attachments/assets/3a5963ed-d110-4fd7-add9-5f6547fa2db0" />

## 2.1 핵심 아이디어
> 소수 클래스 중에서도 **경계선에 위치한 샘플만 선택하여 오버샘플링하자.**

- 정상 클래스 ↔ 이상 클래스 사이의 경계는 분류기가 가장 헷갈리는 영역이며, 이 영역의 데이터가 모델 성능 향상에 가장 중요하다.

## 2.2 Borderline의 정의
컴퓨터는 “경계선”을 직접 알 수 없으므로, **k-최근접 이웃(k-NN)**을 이용해 주변 이웃의 구성을 보고 경계 여부를 판단한다.

## 2.3 샘플 유형 구분
$k$개의 이웃($k=5$ 권장) 중 **다수 클래스 개수**에 따라 샘플을 다음 세 가지로 분류한다.



### ✅ Safe sample (안전 샘플)
<img width="1493" height="937" alt="image" src="https://github.com/user-attachments/assets/3570ad00-6ab8-4941-857e-7b6956ae26ab" />

$$\text{다수 클래스 수} = 0$$
- 주변이 전부 소수 클래스이며 경계선과 멀리 떨어져 있다. 오버샘플링의 실익이 적다.

### ⚠️ Danger sample (경계 샘플)
<img width="1472" height="931" alt="image" src="https://github.com/user-attachments/assets/e4c296c7-f80a-424d-b387-de74ffce3c28" />

$$0 < \text{다수 클래스 수} < k$$
- 이웃 중 일부는 정상, 일부는 이상이다. **경계선에 위치**한 핵심 타겟이다.

### ❌ Noise sample (노이즈 샘플)
<img width="1482" height="934" alt="image" src="https://github.com/user-attachments/assets/7637bd75-9ca3-4d2b-a7a5-40b89c006afd" />

$$\text{다수 클래스 수} = k$$
- 주변이 전부 다수 클래스인 고립된 점으로, 이상치(Outlier)일 가능성이 커 제외한다.

<img width="1485" height="951" alt="image" src="https://github.com/user-attachments/assets/1663d90c-7444-49e6-8aac-6441ef838f2a" />
<img width="1472" height="933" alt="image" src="https://github.com/user-attachments/assets/b85b8cbd-0734-4dd6-80ab-ea5913f9d4c2" />
<img width="1504" height="931" alt="image" src="https://github.com/user-attachments/assets/4a74b079-0f99-4440-b81a-5480cf71de52" />

## 2.4 요약 및 결과
| 유형 | 의미 | 처리 |
| :--- | :--- | :--- |
| **Safe** | 내부 영역 | 제외 |
| **Danger** | 경계선 | **SMOTE 적용** |
| **Noise** | 이상치 | 제외 |

- **결과:** 데이터 생성 위치가 경계선 주변에 집중되어 불필요한 내부 증폭을 막고 분류 경계선을 개선한다.

---

# 3. ADASYN (Adaptive Synthetic Sampling)
<img width="1478" height="988" alt="image" src="https://github.com/user-attachments/assets/b39cd72f-bfd1-45e8-a41f-69a3844d5e45" />
<img width="1497" height="953" alt="image" src="https://github.com/user-attachments/assets/ba107b06-4763-4245-b98b-7316f64a033e" />
<img width="1505" height="937" alt="image" src="https://github.com/user-attachments/assets/f9e0a204-3c3d-4c9b-b11f-d34158f7579d" />
<img width="1505" height="878" alt="image" src="https://github.com/user-attachments/assets/a8a9f81f-8f43-448d-a75a-a138ea0fcd0c" />
<img width="1487" height="859" alt="image" src="https://github.com/user-attachments/assets/fb2907e3-85e1-4ce4-b100-c717e30e9825" />
<img width="1496" height="927" alt="image" src="https://github.com/user-attachments/assets/8d1bfa1e-5f08-457c-b10b-d2883b3ed23d" />
<img width="1442" height="919" alt="image" src="https://github.com/user-attachments/assets/c7aab2f8-c48d-462c-a40e-cf7f325ad01b" />
<img width="1490" height="893" alt="image" src="https://github.com/user-attachments/assets/fed21c3f-1431-4f0d-b847-ce9960b5b1f5" />
<img width="1514" height="949" alt="image" src="https://github.com/user-attachments/assets/70b3674f-3868-4d04-ae49-b26cbcb5b73a" />

## 3.1 핵심 아이디어
> 샘플링 개수를 **데이터의 위치(난이도)마다 다르게 설정하자.**

- **Borderline-SMOTE:** 경계선 샘플에만 집중한다.
- **ADASYN:** **분류하기 어려운 샘플일수록 더 많은 가상 데이터를 생성**한다.

## 3.2 ADASYN 절차
1. **k-NN 탐색:** 모든 소수 클래스 $x_i$에 대해 $k$개의 이웃을 탐색한다.
2. **다수 클래스 비율($r_i$) 계산:**
   $$r_i = \frac{\text{주변 다수 클래스 수}}{k}$$
   ($r_i$가 클수록 경계에 가깝거나 주변에 다수 클래스가 많아 분류가 어렵다는 뜻이다.)
3. **정규화(Scaling):** 전체 샘플링 비율의 합이 1이 되도록 만든다.
   $$\hat{r}_i = \frac{r_i}{\sum r_i}$$
4. **생성 개수 결정:** 총 생성할 샘플 수 $G$에 대해 해당 지점의 생성 수 $g_i$를 결정한다.
   $$g_i = \hat{r}_i \times G$$



---

# 4. GAN 기반 오버샘플링
<img width="1466" height="947" alt="image" src="https://github.com/user-attachments/assets/01fef9da-82a4-45bb-8bac-6c2c1aa204c4" />
<img width="1514" height="1112" alt="image" src="https://github.com/user-attachments/assets/8f454eb4-58b9-4c09-a59e-d46ced25d77e" />
<img width="1508" height="839" alt="image" src="https://github.com/user-attachments/assets/48c477e8-bae9-4e0f-8372-1007608d81a3" />
<img width="1465" height="930" alt="image" src="https://github.com/user-attachments/assets/8f5f5d14-bc7c-4f05-bf96-3579aac7f6f6" />
<img width="1473" height="1074" alt="image" src="https://github.com/user-attachments/assets/2ef8327a-25b5-464b-a27a-3ccf8ee6cdc0" />

## 4.1 개념
기존 SMOTE 계열이 선형 보간법을 사용한다면, **GAN(Generative Adversarial Networks)**은 데이터의 **분포 자체를 학습**하여 가상 데이터를 생성한다.

## 4.2 구조
- **Generator(생성자):** 랜덤 노이즈로부터 실제와 유사한 가짜 데이터를 생성한다.
- **Discriminator(판별자):** 입력된 데이터가 진짜인지 생성자가 만든 가짜인지 판별한다.



## 4.3 특징
- 이미지나 시계열 등 고차원 데이터에서 매우 자연스러운 샘플 생성이 가능하다.
- 단, 학습이 불안정하고 계산 비용이 크며 데이터가 너무 적으면 오히려 역효과가 날 수 있다.

<img width="1496" height="1041" alt="image" src="https://github.com/user-attachments/assets/7fb72afb-8739-48d0-86ab-5285d7bf46eb" />

---

# 5. 비용 기반 학습 (Cost-sensitive Learning)
<img width="1501" height="1084" alt="image" src="https://github.com/user-attachments/assets/35f15bdb-b7ab-41a9-89fb-9a8be3c56073" />
<img width="1489" height="1080" alt="image" src="https://github.com/user-attachments/assets/5ae4cf92-49ad-4c97-b6f8-91ecf7510774" />
<img width="1492" height="1123" alt="image" src="https://github.com/user-attachments/assets/92688087-14c2-4d67-9a84-3b85767d44f0" />

## 5.1 핵심 아이디어
> 데이터 자체가 아니라 **알고리즘의 손실 함수(Loss Function)를 조정**하자.

- 현실(의료, 금융 등)에서는 **False Negative(이상을 정상으로 판단)**의 비용이 훨씬 크다.
- 손실 함수에 가중치를 부여하여 소수 클래스를 틀렸을 때 더 큰 패널티를 준다.
$$Loss = w_1 \cdot FN + w_2 \cdot FP \quad (w_1 \gg w_2)$$

---

# 6. 단일 클래스 분류 (One-Class Classification)
<img width="1450" height="748" alt="image" src="https://github.com/user-attachments/assets/a9d30f3c-15c5-4c45-a21f-9f41fc50da20" />
<img width="1426" height="1049" alt="image" src="https://github.com/user-attachments/assets/a6ed555b-4d87-419f-a63f-ee4515778af7" />
<img width="1473" height="1073" alt="image" src="https://github.com/user-attachments/assets/3aaba896-1599-4cb4-8d30-596766b82c3a" />

## 6.1 아이디어
> 이상 데이터가 너무 적으므로 **정상 데이터의 특징만 학습**하여 경계를 설정하자.

- **절차:** 정상 데이터의 분포 경계(Boundary)를 학습한 뒤, 이 경계 밖에 위치하는 데이터를 이상치로 판단한다.
- **대표 모델:** One-Class SVM, Isolation Forest, Autoencoder



---

# 7. 전체 요약

| 구분 | 방법 | 특징 |
| :--- | :--- | :--- |
| **데이터 조정** | Borderline-SMOTE | 경계선(Danger) 샘플 집중 생성 |
| **데이터 조정** | ADASYN | 주변 다수 클래스 비율에 비례하여 가변 생성 |
| **분포 학습** | GAN | 딥러닝 기반으로 실제와 유사한 분포 생성 |
| **모델 조정** | Cost-sensitive | 오분류 비용 가중치 부여 |
| **정상 기반** | One-class | 정상 데이터의 경계만 학습 (비지도/준지도) |

📌 실무에서는 특정 기법 하나만 고집하기보다 **SMOTE + XGBoost(weight)** 등 데이터 조정과 모델 조정을 조합하여 최적의 전략을 선택하는 것이 핵심이다.

---
📘 본 문서는 강의 내용을 수식과 핵심 개념 중심으로 재구성한 마크다운 정리 노트입니다.
