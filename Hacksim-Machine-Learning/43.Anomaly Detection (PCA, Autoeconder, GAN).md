# Reconstruction-based Anomaly Detection
재구축 기반 이상치 탐지

---

## 1. 개요
<img width="1401" height="966" alt="image" src="https://github.com/user-attachments/assets/1dda3eba-8161-4a19-9102-4db74188d9ef" />

재구축 기반 이상치 탐지(Reconstruction-based Anomaly Detection)는  
**정상 데이터만으로 모델을 학습한 뒤**,  
입력 데이터를 압축 → 복원했을 때 발생하는 **재구축 오차(Reconstruction Error)**를 이용해  
이상치를 탐지하는 방법이다.

특징:
- 개념이 단순함
- 실제 현업 프로젝트에서 활용도가 높음
- 성능이 안정적인 경우가 많음

---

## 2. 핵심 아이디어

### 2.1 압축(Encoding)

- 원본 데이터:
$$x \in \mathbb{R}^p$$

- 압축된 데이터 (잠재 벡터):
$$z \in \mathbb{R}^d \quad (d \ll p)$$

➡ 변수 개수를 줄이는 것이 **압축**

---

### 2.2 복원(Decoding)

- 압축된 표현 $z$를 이용해 원래 차원으로 복원:
$$\hat{x} \in \mathbb{R}^p$$

- $x$와 $\hat{x}$는 같은 차원이지만 **완전히 같을 수는 없음**
  - 압축 과정에서 정보 손실 발생

---

## 3. 재구축 오차 (Reconstruction Error)
<img width="1388" height="941" alt="image" src="https://github.com/user-attachments/assets/829647ce-0d2a-4e1a-9737-176330bee425" />

$$\text{Reconstruction Error} = \|x - \hat{x}\|$$

- 작음 → 잘 복원됨 → 정상
- 큼 → 잘 복원되지 않음 → 이상치

➡ 이 값이 **Anomaly Score**로 사용됨

---

## 4. 왜 정상 데이터만 사용하는가?

- 모델은 **정상 데이터의 구조만 학습**
- 정상과 유사한 데이터:
  - 압축 + 복원이 잘 됨
  - 재구축 오차 작음
- 비정상 데이터:
  - 정상 데이터의 분포에서 벗어남
  - 복원이 잘 안 됨
  - 재구축 오차 큼

➡ 임계값(threshold) 초과 시 이상치로 판정

---

## 5. 전체 흐름 요약

1. 정상 데이터만 사용하여 모델 학습
2. 데이터 압축 (차원 축소)
3. 압축된 데이터 복원
4. 원본과 복원본의 차이 계산
5. 재구축 오차가 큰 관측치를 이상치로 판단

---

## 6. 대표적인 재구축 기반 방법
<img width="1411" height="966" alt="image" src="https://github.com/user-attachments/assets/309a6903-7350-4a2c-9042-67e0d04eb0a5" />

### 6.1 PCA (Principal Component Analysis)

- 선형 재구축 기반 방법
- 정상 데이터의 분산(정보)을 최대한 보존하는 축을 학습
- 차원 축소 후 다시 원래 공간으로 복원

---

### 6.2 Autoencoder

- 신경망 기반 비선형 재구축 모델
- Encoder: 압축
- Decoder: 복원
- 복잡한 데이터 구조에 강함

---

## 7. PCA 기반 재구축 개념
<img width="1401" height="971" alt="image" src="https://github.com/user-attachments/assets/33921771-9930-4d41-8e7a-83ef0ac1a692" />

### 7.1 PCA의 본질

- 원래 변수 $x_1, x_2, \dots, x_p$를
- 선형 결합하여 새로운 축(주성분) 생성

$$z = W^T x$$

- 주성분(PC)은 **잠재 변수(Latent Variable)**

---

### 7.2 분산 관점 해석

- 분산 = 정보량
- 첫 번째 주성분(PC1):
  - 가장 큰 분산을 설명
  - 가장 많은 정보를 보존
- 이후 PC들은 잔여 분산을 순차적으로 설명

---

### 7.3 PCA 압축과 복원
<img width="1398" height="946" alt="image" src="https://github.com/user-attachments/assets/4e95b2e3-85da-41c4-808b-aec20155db4f" />
<img width="1404" height="1007" alt="image" src="https://github.com/user-attachments/assets/a404ffb5-9f01-4d17-995f-2e7bd761e40f" />
<img width="1395" height="997" alt="image" src="https://github.com/user-attachments/assets/22ed0a04-416a-42ec-ac35-2550045fe02c" />
<img width="1396" height="995" alt="image" src="https://github.com/user-attachments/assets/ecb7cc07-2ac4-40c0-8fe5-d720178c6b8e" />

- 압축:
$$x \rightarrow z \in \mathbb{R}^d$$

- 복원:
$$z \rightarrow \hat{x} \in \mathbb{R}^p$$

- $d \ll p$일수록:
  - 압축 효과 ↑
  - 정보 손실 ↑

---

## 8. PCA 기반 이상치 탐지

### 아이디어

- 정상 데이터만으로 PCA 학습
- 정상 데이터:
  - 주성분 공간에서 잘 표현됨
  - 복원 오차 작음
- 비정상 데이터:
  - 주성분으로 설명되지 않음
  - 복원 오차 큼

---

### Anomaly Score

$$\text{Score}_i = \|x_i - \hat{x}_i\|$$

- 관측치마다 계산 가능
- Threshold 초과 시 이상치

---

## 9. 잠재 변수 (Latent Representation)

- PCA의 주성분
- Autoencoder의 Bottleneck 벡터
- 공통점:
  - 원래 변수들의 조합
  - 직접 관측되지 않음
  - 데이터의 핵심 구조를 요약

➡ 이상치 탐지는 **잠재 공간에서의 표현 실패**를 이용

---

## 10. 요약

- 재구축 기반 이상치 탐지는
  - 압축 → 복원 → 오차 계산의 구조
- 정상 데이터만으로 학습
- 핵심 지표는 **Reconstruction Error**
- PCA: 선형, 해석 용이
- Autoencoder: 비선형, 표현력 우수

---
# Autoencoder & GAN-based Anomaly Detection
오토인코더 및 GAN 기반 이상치 탐지 정리

---

## 1. Autoencoder 개념 정리
<img width="1415" height="1003" alt="image" src="https://github.com/user-attachments/assets/11493445-2a27-4ef6-af72-5aeb61ce7095" />

### 1.1 Encoder / Decoder의 의미

- **Encode**: 정보를 압축하여 코드(code)로 표현  
- **Decode**: 압축된 코드를 다시 원래 데이터로 복원

용어의 어원:
- *en-* : 감싸다, 안에 넣다
- *de-* : 풀다, 펼치다

➡ Autoencoder는  
**입력 데이터를 스스로 압축하고 다시 복원하는 모델**

---

### 1.2 Autoencoder 구조

- Encoder: 입력 → 잠재 벡터 (Latent Vector)
- Decoder: 잠재 벡터 → 복원 데이터



$$x \xrightarrow{\text{Encoder}} z \xrightarrow{\text{Decoder}} \hat{x}$$

- $x$: 원본 데이터  
- $\hat{x}$: 복원된 데이터  
- $z$: 잠재 공간(latent space)의 표현

---

## 2. 이상치 탐지에서 Autoencoder의 핵심 아이디어

### 2.1 기본 가정

- **정상 데이터만 사용하여 모델을 학습**
- 모델은 정상 데이터의 구조만 학습함

---

### 2.2 정상 vs 이상 데이터의 차이
<img width="1404" height="978" alt="image" src="https://github.com/user-attachments/assets/bf870813-afb9-44b2-959a-9247206fea71" />

- 정상 데이터:
  - 잠재 공간에서 잘 표현됨
  - 복원 결과가 원본과 유사
  - 재구축 오차 작음

- 이상 데이터:
  - 정상 분포에서 벗어남
  - 복원이 부정확
  - 재구축 오차 큼

---

### 2.3 Reconstruction Error (이상 점수)
<img width="1390" height="994" alt="image" src="https://github.com/user-attachments/assets/43610abc-ef4d-40fd-a33e-3f249fdc2083" />
<img width="1388" height="967" alt="image" src="https://github.com/user-attachments/assets/aba7ff22-1420-41ed-9c5e-9d270fd3ef85" />

$$\text{Anomaly Score} = \|x - \hat{x}\|_2^2$$

- 주로 **L2 norm (제곱 오차)** 사용
- 값이 클수록 이상 가능성 증가

---

## 3. 예시: 사과 비유

- 정상 데이터: **빨간 사과**
- 학습된 Autoencoder는 빨간 사과만 잘 복원

| 입력 | 복원 결과 | 오차 |
|:---|:---|:---|
| 빨간 사과 | 빨간 사과 | 작음 |
| 초록 사과 | 이상한 색 사과 | 큼 |

➡ **정상 분포에서 벗어날수록 오차 증가**

---

## 4. Threshold 설정

재구축 오차를 기준으로 이상 판단:

- 평균 기반
- 상위 95% / 99% 분위수
- 정규분포 가정 후 임계값 설정

$$\text{Anomaly if } \text{Score} > \tau$$

---

## 5. LSTM Autoencoder (시계열 이상치 탐지)
<img width="1412" height="993" alt="image" src="https://github.com/user-attachments/assets/740a42e7-eea0-49ec-903c-1ad9b39ac384" />

### 5.1 시퀀스 데이터란?

- 현재 값이 **이전 값들과 의존 관계**를 가짐
- 시간 순서가 중요한 데이터

⚠️ 모든 시계열 데이터가 LSTM이 필요한 것은 아님

---

### 5.2 LSTM Autoencoder 구조
<img width="1406" height="907" alt="image" src="https://github.com/user-attachments/assets/9322f87b-f00a-4944-b1a2-9db6f4e44b1a" />
<img width="1397" height="995" alt="image" src="https://github.com/user-attachments/assets/92278a4a-1859-4fd7-8190-a123d85ee40e" />
<img width="1393" height="977" alt="image" src="https://github.com/user-attachments/assets/c23b90af-188e-4510-a64e-12808734942e" />
<img width="1409" height="947" alt="image" src="https://github.com/user-attachments/assets/27b3f04a-bd77-4b5f-a49c-6170b2e7c05c" />
<img width="1400" height="933" alt="image" src="https://github.com/user-attachments/assets/513e9b2f-06b8-428f-92c9-2bbff02bc982" />

#### Encoder (LSTM)
- 입력 시퀀스:
$$x_1, x_2, \dots, x_T$$
- 마지막 hidden state → 잠재 벡터

#### Decoder (LSTM)
- 잠재 벡터로부터 시퀀스 복원
- 보통 **역순으로 복원**

---

### 5.3 학습 목표

$$\min \sum_{t=1}^{T} \|x_t - \hat{x}_t\|^2$$

- 정상 시계열만으로 학습
- 복원 오차 기반 이상 탐지

---

## 6. Robust Autoencoder (RPCA + Autoencoder)
<img width="1394" height="975" alt="image" src="https://github.com/user-attachments/assets/2a9ff862-7781-4d6b-b5df-e97645578245" />
<img width="1394" height="979" alt="image" src="https://github.com/user-attachments/assets/5dfb3936-adbb-4636-819e-51400cab3954" />
<img width="1412" height="972" alt="image" src="https://github.com/user-attachments/assets/87048540-9bc3-4762-804e-a5ace0c03304" />
<img width="1396" height="978" alt="image" src="https://github.com/user-attachments/assets/0532cdad-d617-486a-87ba-9492c0c91e3a" />
<img width="1396" height="976" alt="image" src="https://github.com/user-attachments/assets/a0637d18-6095-4257-a162-0af0839e7b94" />

### 6.1 문제점

- 정상 데이터에도 미세한 이상치가 섞일 수 있음

---

### 6.2 해결 아이디어 (Robust PCA)

$$X = L + S$$

- $L$: 저차원 정상 구조
- $S$: 희소한 이상치

➡ **정제된 정상 데이터($L$)**만으로 Autoencoder 학습

---

### 6.3 장점

- 더 안정적인 재구축 오차
- 이상치 민감도 개선

---

## 7. GAN (Generative Adversarial Network) 개요
<img width="1401" height="990" alt="image" src="https://github.com/user-attachments/assets/bb5849e1-fe7f-4afd-9b99-374f7d2b4ef5" />
<img width="1398" height="1005" alt="image" src="https://github.com/user-attachments/assets/b62b3e73-3bf0-49ea-93ff-0a02f012aff3" />
<img width="1405" height="974" alt="image" src="https://github.com/user-attachments/assets/6538ec48-d9e5-41cb-ba94-b9a95e3b06cd" />

### 7.1 구성 요소

- Generator (G): 가짜 데이터 생성
- Discriminator (D): 진짜/가짜 판별



➡ 두 모델은 **적대적 관계**

---

### 7.2 학습 목표

$$\min_G \max_D \; \mathbb{E}[\log D(x)] + \mathbb{E}[\log(1 - D(G(z)))]$$

---

## 8. GAN 기반 이상치 탐지
<img width="1390" height="981" alt="image" src="https://github.com/user-attachments/assets/d4373806-e3c7-4115-b638-60b5ec60e5d6" />
<img width="1394" height="998" alt="image" src="https://github.com/user-attachments/assets/9cb34cc5-55ba-4986-9bed-dda0727fc17e" />
<img width="1395" height="1004" alt="image" src="https://github.com/user-attachments/assets/6dd4cd60-251d-4f1e-8741-4d35e9813417" />
<img width="1413" height="1003" alt="image" src="https://github.com/user-attachments/assets/e40311c6-8eb9-435a-93eb-2f5cabfc3fa8" />
<img width="1404" height="999" alt="image" src="https://github.com/user-attachments/assets/d5f55b9e-aa07-4213-84dd-fe169019b401" />
<img width="1399" height="995" alt="image" src="https://github.com/user-attachments/assets/db996976-b37f-483b-aa62-25105b704d3b" />
<img width="1408" height="1009" alt="image" src="https://github.com/user-attachments/assets/cc5e37c6-a549-4d5b-a4d4-6f47565599ae" />
<img width="1396" height="1004" alt="image" src="https://github.com/user-attachments/assets/d61b084e-f0ea-4e9d-8c51-622f1d26dec3" />
<img width="1380" height="979" alt="image" src="https://github.com/user-attachments/assets/ae9da903-badc-44f0-9655-f062a207b9f6" />
<img width="1413" height="1003" alt="image" src="https://github.com/user-attachments/assets/bed16b58-ffcf-48d4-aa30-9effe0fad99d" />
<img width="1407" height="1003" alt="image" src="https://github.com/user-attachments/assets/a82c6803-28fe-4612-b0f4-6d288561e12d" />
<img width="1421" height="999" alt="image" src="https://github.com/user-attachments/assets/3ac87257-5bd0-4958-a725-e035af8b3085" />
<img width="1433" height="1056" alt="image" src="https://github.com/user-attachments/assets/4a4629c7-436c-4f07-901d-558416c7dcc1" />
<img width="1399" height="1015" alt="image" src="https://github.com/user-attachments/assets/3ecd8057-da10-4bf8-995d-c3d7fdc2dddf" />
<img width="1396" height="998" alt="image" src="https://github.com/user-attachments/assets/d2ea94c7-c6be-447a-a9c2-5136eb79e8ee" />

### 8.1 핵심 아이디어

- 정상 데이터만으로 GAN 학습
- Generator는 **정상 데이터 생성에 특화**

---

### 8.2 이상 점수 계산

1. 입력 이미지 $x$
2. Generator로 생성된 이미지 $G(z)$
3. 차이 계산

$$\text{Score} = \|x - G(z)\| + \|f(x) - f(G(z))\|$$

- 픽셀 차이 + 특징 공간 차이

---

## 9. GANomaly / AnoGAN 계열

- 입력 → 잠재 벡터 → 생성 이미지
- 입력 잠재 벡터와 생성 이미지의 잠재 벡터 차이 사용

$$\text{Anomaly Score} = \|z - \hat{z}\|$$

---

## 10. 전체 요약

- 이상치 탐지의 공통 원칙:
  - 정상 데이터만 학습
  - 정상 구조를 벗어나면 오차 증가

- 주요 방법:
  - PCA
  - Autoencoder
  - LSTM Autoencoder
  - Robust Autoencoder
  - GAN 기반 방법

- 핵심 개념:
  - Latent Space
  - Reconstruction Error
  - Thresholding

---

## 학습 팁

- 논문은 전부 읽을 필요 없음
- 제목 → 스킴 → 관심 있으면 정독
- 자신의 데이터 유형과 맞는 방법 선택

---
