Boosting Algorithm (부스팅 알고리즘)

---

## 1. 부스팅(Boosting) 개요

부스팅은 **앙상블(Ensemble) 계열 모델** 중 하나이다. 앙상블 모델의 대표적인 예로는 다음이 있다.

- Random Forest (배깅 기반)
- Boosting (순차 학습 기반)

### ✅ 핵심 차이

| 구분 | Random Forest | Boosting |
| :--- | :--- | :--- |
| **모델 생성** | 병렬 (Parallel) | 순차 (Sequential) |
| **데이터 처리** | 부트스트랩 샘플링 | 가중치 재조정 |
| **핵심 목적** | 분산(Variance) 감소 | 편향(Bias) 감소 |

---

## 2. Boosting의 핵심 아이디어
<img width="1509" height="823" alt="image" src="https://github.com/user-attachments/assets/03d142e9-fecd-4bab-acb6-7863a9fc257b" />

### ✔ 핵심 키워드: **순차적(Sequential)**
부스팅은 모델을 한 번에 만드는 것이 아니라, **이전 모델의 실수를 보완하며 순차적으로 모델을 구축**한다.



---

## 3. Weak Learner (약한 학습기)

부스팅에서 사용하는 베이스 모델은 매우 단순한 **약한 분류기 (Weak Learner)**이다.
- **정의:** 이진 분류 문제에서 정확도가 0.5보다 약간 좋은 모델
- **대표 모델:** Decision Stump (깊이 1 의사결정트리), 얕은 Decision Tree

---

## 4. Boosting의 작동 원리

부스팅은 다음 과정을 반복한다.
1. 모든 관측치에 동일한 가중치 부여
2. 모델 학습
3. 잘못 분류된 데이터에 가중치 증가
4. 다음 모델은 오류가 컸던 데이터에 집중
5. 이를 반복하여 점점 강한 모델 생성

---

## 5. AdaBoost (Adaptive Boosting)
<img width="1507" height="881" alt="image" src="https://github.com/user-attachments/assets/653e0fbf-4bd7-4643-b3c8-e115463df49a" />

부스팅의 가장 기본이 되는 알고리즘이 **AdaBoost**이다. 이후 발전한 알고리즘들(Gradient Boosting, XGBoost, LightGBM, CatBoost)의 핵심 아이디어는 모두 여기서 출발한다.

---

## 6. AdaBoost 알고리즘 구조
<img width="1513" height="1055" alt="image" src="https://github.com/user-attachments/assets/96eae824-dbf6-4fe5-9299-0df0abf29ce0" />
<img width="1477" height="744" alt="image" src="https://github.com/user-attachments/assets/3aa1d20b-6557-46e5-a70f-4334c46076d0" />
<img width="1495" height="1068" alt="image" src="https://github.com/user-attachments/assets/b4817473-4751-4153-972b-2966981152a0" />
<img width="1472" height="1083" alt="image" src="https://github.com/user-attachments/assets/dbb7ddd2-f332-4629-8aea-a70bcf7c4a4f" />
<img width="1510" height="1053" alt="image" src="https://github.com/user-attachments/assets/a9c7536c-e1ea-41ae-a148-6fd4269e2baf" />
<img width="1487" height="1037" alt="image" src="https://github.com/user-attachments/assets/c4784304-1261-44c1-89a9-2476533bae0f" />

### Step 1. 초기 가중치 설정
전체 데이터 개수를 $N$이라 할 때, 모든 관측치에 동일한 가중치를 부여한다.
$$w_i = \frac{1}{N}$$

### Step 2. 가중치가 있는 손실함수 정의
$$L = \sum_{i=1}^{N} w_i \cdot I(y_i \neq h(x_i))$$
- $h(x_i)$: 모델 예측값
- $y_i$: 실제 레이블
- $I(\cdot)$: Indicator Function (조건이 참이면 1, 거짓이면 0)

### Step 3. 가중 손실(Error) 계산
$$\text{Error}_t = \sum_{i=1}^{N} w_i I(y_i \neq h_t(x_i))$$

### Step 4. 모델 가중치($\alpha$) 계산
$$\alpha_t = \frac{1}{2} \ln \left( \frac{1-\text{Error}_t}{\text{Error}_t} \right)$$
- **의미:** 에러가 낮을수록(0에 가까울수록) $\alpha$ 값은 매우 커지며, 이는 해당 모델의 결정권이 높음을 의미한다.

---

## 7. 관측치 가중치 업데이트
$$w_i^{(t+1)} = w_i^{(t)} \cdot \exp \left( \alpha_t \cdot I(y_i \neq h_t(x_i)) \right)$$

### 해석
- **정분류:** 가중치 변화 없음
- **오분류:** 가중치 증가
👉 **틀린 데이터에 더 많은 집중을 하게 됨**



---

## 8. 알고리즘 반복
위 과정을 다음과 같이 순차적으로 반복한다.
$$\text{초기 가중치} \rightarrow h_1 \text{ 학습} \rightarrow \text{가중치 업데이트} \rightarrow h_2 \text{ 학습} \rightarrow \dots \rightarrow h_M \text{ 학습}$$

---

## 9. 최종 예측 모델
모든 약한 학습기를 가중합하여 최종 모델을 만든다.
$$H(x) = \text{sign} \left( \sum_{t=1}^{M} \alpha_t h_t(x) \right)$$

---

## 10. AdaBoost 전체 흐름 요약
<img width="1454" height="1045" alt="image" src="https://github.com/user-attachments/assets/46088926-6e28-4698-b087-5addf51bd88f" />
<img width="1508" height="988" alt="image" src="https://github.com/user-attachments/assets/2a73a6ec-20ab-4d54-94be-dd62aaac8e8f" />

1. 동일 가중치 부여
2. 약한 분류기 학습
3. 오분류 데이터 가중치 증가
4. 어려운 데이터에 집중하여 다음 학습 수행
5. 여러 약한 모델을 가중합하여 강한 모델 생성

---

## 11. Random Forest vs Boosting 비교

| 항목 | Random Forest | Boosting |
| :--- | :--- | :--- |
| **샘플링** | Bootstrap | 가중치 조정 |
| **학습 방식** | 병렬 | 순차 |
| **목적** | 분산 감소 | 편향 감소 |
| **오분류 활용** | 사용 안 함 | 적극 활용 |

---

## 12. 핵심 요약
부스팅은 **이전 모델의 실수를 다음 모델이 보완**하는 순차 학습 방식의 앙상블 기법이다. 잘못 분류된 데이터에 점점 더 집중함으로써 약한 모델들을 모아 하나의 강력한 모델을 구축한다.

✅ **한 줄 정리: Boosting = “틀린 데이터를 점점 더 중요하게 보면서 모델을 강화하는 기법”**

---
# Gradient Boosting Machine (GBM)

---

## 1. Gradient Boosting 개요
<img width="1486" height="1042" alt="image" src="https://github.com/user-attachments/assets/f4aeccb4-7d37-4ca4-9f6e-3e0383d21a16" />

AdaBoost 이후 등장한 대표적인 부스팅 알고리즘이 **Gradient Boosting Machine (GBM)** 이다. GBM 역시 **부스팅(Boosting)** 계열 알고리즘으로, 다음의 특징을 가진다.

- 여러 개의 약한 모델(Weak Learner)을 생성
- **순차적으로 학습**하며 이전 모델의 **실수를 보완**
- 최종적으로 강력한 모델을 구축

---

## 2. AdaBoost와의 핵심 차이점

| 구분 | AdaBoost | Gradient Boosting |
| :--- | :--- | :--- |
| **보완 대상** | 오분류 샘플 | 예측 오차 (Residual) |
| **핵심 아이디어** | 가중치(Weight) 조정 | 손실함수의 기울기(Gradient) |
| **수학적 기반** | 가중치 기반 업데이트 | 미분(Gradient) 기반 업데이트 |
| **주 용도** | 분류 (Classification) | 회귀 및 분류 모두 가능 |

---

## 3. GBM의 핵심 아이디어

Gradient Boosting은 다음과 같은 질문에서 출발한다.
> **“모델이 놓친 부분을 어떻게 보완할 것인가?”**

### 답:
> **이전 모델이 해결하지 못한 오차(Residual)를 다음 모델이 직접 학습한다.**

---

## 4. Residual(잔차)의 정의
<img width="1425" height="1044" alt="image" src="https://github.com/user-attachments/assets/07d699ec-bf18-4361-9d9f-d2ece746a41e" />

실제값과 예측값의 차이를 의미한다.
$$\text{Residual} = y - \hat{y}$$
- $y$: 실제 값
- $\hat{y} = f(x)$: 모델의 예측값

이 **잔차(Residual)가 바로 다음 트리가 학습할 새로운 타깃(Target)**이 된다.

---

## 5. Gradient Boosting 학습 과정
<img width="1514" height="1060" alt="image" src="https://github.com/user-attachments/assets/7464a610-8216-4018-b4e4-f1306d8af6de" />

### Step 1. 첫 번째 모델 학습
원본 데이터 $(x_i, y_i)$를 사용하여 첫 번째 결정트리 모델 $f_1(x)$을 학습한다.

### Step 2. 첫 번째 잔차 계산
$$r_1 = y - f_1(x)$$
계산된 잔차 $r_1$이 다음 단계의 새로운 타깃값이 된다.

### Step 3. 두 번째 모델 학습
새로운 데이터 세트 $(x_i, r_{1,i})$를 사용하여 두 번째 모델 $f_2(x)$를 학습한다.

### Step 4. 두 번째 잔차 계산
$$r_2 = r_1 - f_2(x)$$

### Step 5. 반복
이 과정을 $M$번 반복하여 잔차를 계속해서 줄여나간다.
$$r_m = r_{m-1} - f_m(x)$$

---

## 6. 최종 모델
<img width="1506" height="1032" alt="image" src="https://github.com/user-attachments/assets/5db8265a-faf6-4688-b990-e52b885cac61" />

학습된 모든 트리를 순차적으로 더한 것이 최종 예측 모델이 된다.
$$F(x) = f_1(x) + f_2(x) + f_3(x) + \dots + f_M(x)$$

---

## 7. 시각적 직관
<img width="1496" height="1074" alt="image" src="https://github.com/user-attachments/assets/85f978b9-fbda-45ba-94b0-50d049e03b6e" />

1. **첫 번째 트리:** 전체 데이터를 대략적으로 피팅하는 거친(Rough) 모델을 만든다.
2. **이후 트리들:** 이전 모델들이 놓친 잔차(Residual)를 순차적으로 학습한다.
3. **반복 결과:** 트리가 추가될수록 잔차의 크기는 줄어들고, 전체 모델의 예측 오차는 0에 가까워진다.



---

## 8. 왜 “Gradient”인가?

손실함수를 평균제곱오차(MSE)로 정의했을 때의 관계를 보면 명확해진다.
$$L = \frac{1}{n} \sum (y - f(x))^2$$

이를 모델의 예측값 $f(x)$에 대해 미분하면 다음과 같다.
$$\frac{\partial L}{\partial f(x)} = -2(y - f(x))$$

여기서 상수를 제외하면 다음의 관계가 성립한다.
$$\text{Negative Gradient} \propto y - f(x)$$

👉 **즉, 음의 기울기(Negative Gradient)가 곧 잔차(Residual)가 된다.**



---

## 9. 핵심 정리

- **Gradient = 미분**
- **Negative Gradient = 잔차(Residual)**

따라서, **Gradient Boosting은 곧 잔차를 줄여나가는 부스팅(Residual Boosting)**이라고 이해해도 무방하다.

---

## 10. GBM 알고리즘 요약

1. 초기 모델 생성 (보통 타깃의 평균값 등으로 시작)
2. 현재 모델의 예측값 계산
3. 손실함수의 음의 기울기(잔차) 계산
4. **그 잔차를 타깃으로** 새로운 트리를 학습
5. 기존 모델에 새로운 트리를 더함
6. 위 과정을 $M$번 반복하여 모델을 강화함

---

## 11. 전체 흐름 요약 그림
<img width="1482" height="1033" alt="image" src="https://github.com/user-attachments/assets/3b4126f7-383e-4556-9772-e7c76e237695" />



✅ **GBM은 현대 머신러닝에서 가장 강력한 성능을 내는 알고리즘 중 하나로, 이후 XGBoost와 LightGBM의 모태가 된다.**
