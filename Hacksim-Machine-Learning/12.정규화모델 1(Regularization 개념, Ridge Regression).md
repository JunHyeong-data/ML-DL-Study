# 정규화(Regularization) 모델 개요 및 Ridge Regression

## 1. 좋은 모델이란 무엇인가?
<img width="1297" height="804" alt="image" src="https://github.com/user-attachments/assets/5dfb3ebe-74d5-4cf2-8267-981301d9a9e0" />

모델이 “좋다”고 말하기 위해서는 다음 두 가지 조건을 모두 만족해야 한다.

1. **현재 데이터(Training data)를 잘 설명해야 한다.**
   - 즉, 모델을 학습할 때 사용한 데이터에 대해 오차(training error)가 작아야 한다.
2. **미래 데이터(Test data)에 대한 예측 성능이 좋아야 한다.**
   - 현재 데이터에만 잘 맞고 새로운 데이터에 대해 성능이 급격히 떨어지는 모델은 좋은 모델이 아니다.

이 두 조건을 동시에 만족하는 모델이 전반적으로 좋은 모델이라고 할 수 있다.

---

## 2. 예측 오차와 MSE (Mean Squared Error)
<img width="1337" height="1063" alt="image" src="https://github.com/user-attachments/assets/2d62e29b-d167-4b4e-8e5d-7b4cad778550" />

연속형 타깃 변수 $y$를 예측하는 회귀 문제에서는 일반적으로 MSE를 사용하여 오차를 정의한다.

$$
\text{MSE} = \mathbb{E}\left[(y - \hat{y})^2\right]
$$

여기서
- $y$: 실제 값
- $\hat{y}$: 모델이 예측한 값

이 MSE의 기대값을 전개하면 다음과 같이 세 가지 항으로 분해된다.

$$
\mathbb{E}[(y - \hat{y})^2] = \underbrace{\text{Irreducible Error}}_{\text{줄일 수 없는 오차}} + \underbrace{\text{Bias}^2}_{\text{편향}} + \underbrace{\text{Variance}}_{\text{분산}}
$$

- **Irreducible Error:** 데이터 자체의 잡음으로 인해 모델이 아무리 좋아도 줄일 수 없는 부분
- **Bias:** 모델이 너무 단순해서 발생하는 오차
- **Variance:** 모델이 너무 복잡해서 데이터에 과도하게 민감해지는 현상

모델링을 통해 우리가 조절할 수 있는 것은 **Bias**와 **Variance**이다.

---

## 3. Bias–Variance Trade-off
Bias와 Variance는 서로 상충(trade-off) 관계에 있다.

- Bias를 줄이면 → Variance가 커지는 경향
- Variance를 줄이면 → Bias가 커지는 경향

따라서 현실적으로 둘을 동시에 최소화할 수는 없다. 중요한 것은 상황에 맞는 균형점을 찾는 것이다.



---

## 4. Bias와 Variance의 직관적 이해 (과녁 그림)
<img width="1332" height="940" alt="image" src="https://github.com/user-attachments/assets/4fb438ca-7b62-4cc9-947a-21bfe51b2547" />

- **Case 1: 중앙에 정확히 모여 있음**
  → Bias ↓, Variance ↓ (가장 이상적인 경우)
- **Case 2: 중앙 근처지만 퍼져 있음**
  → Bias ↓, Variance ↑
- **Case 3: 한쪽으로 치우쳐 있으나 모여 있음**
  → Bias ↑, Variance ↓
- **Case 4: 중앙에서도 멀고 흩어져 있음**
  → Bias ↑, Variance ↑ (가장 나쁜 경우)

항상 1번이 최선, 4번이 최악이며 2번과 3번 중 어느 쪽이 더 좋은지는 문제 상황에 따라 달라질 수 있다.



---

## 5. 최소제곱법(Least Squares Estimation)
선형 회귀 모델에서 최소제곱법은 다음 문제를 푸는 것이다.

$$
\min_{\beta} \sum_{i=1}^{n} (y_i - x_i^\top \beta)^2
$$

이 문제는 Convex optimization 문제이므로 미분 후 0으로 두면 해를 구할 수 있다. 해는 다음과 같다.

$$
\hat{\beta}_{OLS} = (X^\top X)^{-1} X^\top y
$$

이 추정량의 성질:
- Unbiased estimator (불편추정량)
- 모든 unbiased estimator 중 분산이 최소 (**BLUE**: Best Linear Unbiased Estimator) → 가우스–마르코프 정리

하지만 문제점도 있다: **“Bias를 조금 희생하더라도 Variance를 크게 줄일 수 있다면 오히려 미래 예측 성능은 더 좋아지지 않을까?”** 이 질문에서 **정규화(Regularization)**가 등장한다.

---

## 6. 모델 복잡도와 과적합 / 과소적합
<img width="1496" height="1119" alt="image" src="https://github.com/user-attachments/assets/04a2e3a6-92a1-435a-9f8e-552038916c5e" />

다항 회귀 모델을 예로 들어 보자.

- **차수가 낮은 모델:** Bias 큼 (**Underfitting**)
- **차수가 너무 높은 모델:** Variance 큼 (**Overfitting**)

훈련 오차는 차수가 커질수록 감소하지만, 테스트 오차는 어느 지점 이후 다시 증가한다. 이 균형 지점이 우리가 찾고자 하는 모델이다.



---

## 7. 정규화(Regularization)의 핵심 아이디어
<img width="1472" height="1026" alt="image" src="https://github.com/user-attachments/assets/a6ffa004-e92f-4630-8873-2200228db7ad" />

정규화란 다음을 의미한다.
> **손실 함수에 파라미터에 대한 제약(penalty)을 추가하여 모델의 복잡도를 인위적으로 줄이는 방법**

일반적인 형태:

$$
\min_{\beta} \Big( \underbrace{\text{MSE}}_{\text{데이터 적합}} + \lambda \underbrace{\text{Penalty}(\beta)}_{\text{복잡도 제약}} \Big)
$$
- $\lambda$: 정규화 강도를 조절하는 하이퍼파라미터
- 크면 → 단순한 모델 (가중치 억제)
- 작으면 → 복잡한 모델 (OLS에 가까워짐)

---

## 8. Ridge Regression (L2 Regularization)
<img width="1491" height="1094" alt="image" src="https://github.com/user-attachments/assets/ca3a84cc-28cd-4e83-acdb-bd9b19d46bd9" />

Ridge Regression은 $L2$ 노름을 패널티로 사용한다.

$$
\min_{\beta} \sum_{i=1}^{n} (y_i - x_i^\top \beta)^2 + \lambda \sum_{j=1}^{p} \beta_j^2
$$

즉, $\text{Penalty} = \|\beta\|_2^2$ 이다.

**해의 형태:**
<img width="1486" height="1118" alt="image" src="https://github.com/user-attachments/assets/fec470a8-8431-4e31-8269-9e376b83ad50" />

$$
\hat{\beta}_{ridge} = (X^\top X + \lambda I)^{-1} X^\top y
$$

OLS와 비교하면 $(X^\top X)$에 $\lambda I$가 추가되어 수치적으로 더 안정적이며 분산을 감소시킨다.

---

## 9. $\lambda$ 값에 따른 모델 변화

- $\lambda \to 0$: Ridge ≈ OLS (제약 거의 없음)
- $\lambda \to \infty$: 모든 $\beta \to 0$ (매우 단순한 모델, Underfitting)

따라서 적절한 $\lambda$를 선택하는 것이 핵심이다.

---

## 10. 기하학적 해석 (Contour & Constraint)
- **MSE의 등고선:** 타원 형태
- **L2 제약:** 원 형태

최적해는 **타원이 제약 원에 처음 접하는 지점**에서 결정된다. 이 지점에서 Bias는 약간 증가하지만 Variance를 크게 줄여 전체 예측 오차를 최소화할 수 있다.



---

## 11. 정리
- **최소제곱법(OLS):** Bias ↓, Variance ↑
- **Ridge Regression:** Bias ↑, Variance ↓
- **목표:** 미래 데이터 예측 성능 최적화

다음 강의에서는 Lasso (L1), Elastic Net 등 다른 정규화 기법을 다룬다.
