# One-Class Support Vector Machine (One-Class SVM)
이상치 탐지 (Anomaly / Outlier Detection)

---

## 1. 개요
<img width="1305" height="993" alt="image" src="https://github.com/user-attachments/assets/03296abc-378e-40b9-989a-3a6d139a6d35" />

One-Class SVM(OC-SVM)은 **정상 데이터만을 사용**하여
- 정상 영역을 학습하고
- 그 영역에서 벗어난 데이터를 이상치로 판단하는 알고리즘이다.

주요 활용:
- 불량 탐지
- 이상 행동 탐지
- 품질 관리
- 보안 침입 탐지

대표 논문:
> Schölkopf, Smola, Williamson, Bartlett (2001)  
> *Estimating the Support of a High-Dimensional Distribution*

---

## 2. 기본 아이디어
<img width="1314" height="924" alt="image" src="https://github.com/user-attachments/assets/e11aefb7-cef5-47dc-915d-b1e2bd17021e" />

### 2.1 기존 SVM과의 차이
- **기존 SVM**: 정상 vs 불량 (라벨 필요)
- **One-Class SVM**: 정상 데이터만 사용

### 2.2 직관적 설명
1. 원본 공간(original space)에서는 정상 데이터가 비선형 형태로 분포
2. 커널 함수 $\phi(x)$를 이용해 고차원 feature space로 변환
3. feature space에서:
   - 정상 데이터들을 **원점으로부터 최대한 멀리 밀어내는**
   - 하나의 **선형 hyperplane**을 학습
4. hyperplane 바깥 → 정상  
   hyperplane 안쪽 → 이상치



---

## 3. 결정 함수 (Decision Function)
<img width="1307" height="907" alt="image" src="https://github.com/user-attachments/assets/38f01139-b324-4f41-9925-d34f9574c5c9" />

$$f(x) = \text{sign}(w^T \phi(x) - \rho)$$

- $w^T \phi(x)$: 분리 hyperplane
- $\rho$: 원점과 hyperplane 사이의 거리
- 결과:
  - $f(x) \ge 0$: 정상
  - $f(x) < 0$: 이상치

---

## 4. 최적화 문제 (Primal Form)
<img width="1310" height="910" alt="image" src="https://github.com/user-attachments/assets/6d8a11de-bc01-4a38-a36f-8be11343a9a3" />

### 목적식
$$\min_{w,\xi,\rho} \quad \frac{1}{2}\|w\|^2 - \rho + \frac{1}{\nu n}\sum_{i=1}^{n} \xi_i$$

### 제약식
$$w^T \phi(x_i) \ge \rho - \xi_i$$
$$\xi_i \ge 0$$

### 변수 의미
- $w$: 모델 파라미터 (hyperplane)
- $\rho$: 원점과 hyperplane 거리
- $\xi_i$: 에러(slack variable)
- $n$: 정상 데이터 개수
- $\nu \in (0,1]$: 사용자 파라미터

---

## 5. 목적식 해석

### 5.1 $\frac{1}{2}\|w\|^2$
- 파라미터 정규화 (Regularization)
- 모델을 **robust**하게 만듦
- 입력 변화에 민감하지 않도록 함

### 5.2 $-\rho$
- 원점으로부터 hyperplane까지의 거리 **최대화**
- 정상 영역을 크게 확보

### 5.3 $\frac{1}{\nu n}\sum \xi_i$
- 잘못 분류된 정상 데이터(에러) 최소화
- $\nu$가 에러 허용 정도를 조절

---

## 6. 라그랑지안과 Dual Problem
<img width="1302" height="907" alt="image" src="https://github.com/user-attachments/assets/c24a568a-acc7-4c71-a29e-940d57c9a433" />
<img width="1306" height="892" alt="image" src="https://github.com/user-attachments/assets/aa53c772-31ef-4c88-a788-d1c5df7d2b6c" />

### 6.1 라그랑지안 (Primal)
라그랑지 승수:
- $\alpha_i \ge 0$
- $\beta_i \ge 0$

### 미분 결과 (KKT 조건)

$$w = \sum_{i=1}^{n} \alpha_i \phi(x_i)$$

$$0 \le \alpha_i \le \frac{1}{\nu n}$$

$$\sum_{i=1}^{n} \alpha_i = 1$$

---

## 7. Dual Problem (Kernel 적용 가능)
<img width="1332" height="937" alt="image" src="https://github.com/user-attachments/assets/cb44b5a0-0a60-4793-b98f-692a34f215a7" />
<img width="1315" height="940" alt="image" src="https://github.com/user-attachments/assets/0a99d527-c4e6-4512-a723-fbe571224c26" />

$$\max_{\alpha} \quad - \frac{1}{2} \sum_{i,j} \alpha_i \alpha_j K(x_i, x_j)$$

subject to:
- $0 \le \alpha_i \le \frac{1}{\nu n}$
- $\sum \alpha_i = 1$

➡ 커널 함수 $K(x_i, x_j)$ 사용 가능  
➡ 비선형 이상치 탐지 가능

---

## 8. 알파($\alpha_i$)의 역할

### 경우 1: $\alpha_i = 0$
- hyperplane에 전혀 영향 없음
- 정상 영역 내부 점 (회색 점)

### 경우 2: $0 < \alpha_i < \frac{1}{\nu n}$
- hyperplane 위
- **Support Vector**
- 에러 없음 (파란 점)

### 경우 3: $\alpha_i = \frac{1}{\nu n}$
- 에러 발생
- 정상인데 이상치로 판정될 수 있음
- **Penalty가 부여된 Support Vector** (빨간 점)

➡ **Support Vector만이 $w$를 결정**

[Image illustrating support vectors in One-Class SVM with different alpha values and their positions relative to the boundary]
<img width="1292" height="904" alt="image" src="https://github.com/user-attachments/assets/596ed951-c0a1-4e94-8b2d-15a92e6e454d" />

---

## 9. $\nu$ 파라미터의 의미 (핵심)
<img width="1314" height="920" alt="image" src="https://github.com/user-attachments/assets/2ec1ea40-2d3e-44a4-a824-b0a61e6aa6e3" />

$$\nu \in (0,1]$$

### 의미 1
- **Support Vector의 최소 비율**
$$\text{SV} \ge \nu n$$

### 의미 2
- **에러가 발생하는 데이터의 최대 비율**
$$\text{Error SV} \le \nu n$$

### 예시
- 정상 데이터: 5,000개
- $\nu = 0.1$

결과:
- 최소 500개의 Support Vector 존재
- 최대 500개까지 에러 허용

---

## 10. Control Chart 관점 해석

- 모니터링 통계량:
  $$w^T \phi(x)$$
  (원점으로부터의 거리)

- $\nu$:  
  - Type I error (오탐) 조절 가능
  - Control limit 역할

➡ One-Class SVM은 **비모수적 Control Chart**로 해석 가능

---

## 11. 커널 파라미터 영향

- $\nu$: 이상치 허용 비율
- 커널 파라미터 (예: RBF의 $\gamma$):
  - 결정 경계의 복잡도 조절

$\nu \downarrow$: 전체 정상 데이터를 감싸는 부드러운 경계  
$\nu \uparrow$: 더 타이트하고 민감한 경계

[Image showing the effect of different nu and gamma parameters on the One-Class SVM decision boundary]
<img width="1314" height="757" alt="image" src="https://github.com/user-attachments/assets/15b48fb6-795f-4cd0-948e-e77e5ae462c5" />

---

## 12. 요약

- One-Class SVM은 정상 데이터만으로 이상치 탐지
- 핵심 개념:
  - 원점으로부터 최대한 멀리 정상 데이터를 밀어내는 hyperplane
- $\alpha_i \neq 0$ → Support Vector
- $\nu$:
  - Support Vector 비율 하한
  - 에러 비율 상한
- 커널 사용 가능 → 비선형 이상치 탐지

---
# Support Vector Data Description (SVDD)
이상치 탐지 (Anomaly Detection)

---

## 1. 개요
<img width="1283" height="910" alt="image" src="https://github.com/user-attachments/assets/d00c0aa1-0167-432a-868b-372d4b616ec5" />
<img width="1316" height="925" alt="image" src="https://github.com/user-attachments/assets/528cd95a-cb2f-4f4b-acd6-5b7e8983809a" />
<img width="1309" height="917" alt="image" src="https://github.com/user-attachments/assets/e5f6fc3c-f4d1-45a2-9771-25b2358755ed" />

SVDD(Support Vector Data Description)는  
**정상 데이터만을 이용하여 정상 영역을 하나의 구(Hypersphere)로 감싸는** 이상치 탐지 알고리즘이다.

- One-Class SVM보다 **기하학적으로 더 직관적**
- Feature Space에서 **Hypersphere(초구)**를 학습
- 구 안 → 정상  
- 구 밖 → 비정상



---

## 2. 기본 아이디어

### 2.1 Hypersphere 개념
- 2차원: 원 (Circle)
- 3차원: 구 (Sphere)
- 4차원 이상: 초구 (Hypersphere)

Hypersphere의 파라미터:
- 중심: $a$
- 반지름: $R$

---

### 2.2 정상 / 비정상 판별 기준
<img width="1320" height="948" alt="image" src="https://github.com/user-attachments/assets/ba7d4154-2007-473f-9b0d-2d3a6138c558" />

$$\|\phi(x) - a\|^2 \le R^2 \quad \Rightarrow \text{정상}$$

$$\|\phi(x) - a\|^2 > R^2 \quad \Rightarrow \text{비정상}$$

---

### 2.3 Overfitting 방지 아이디어

- 모든 정상 데이터를 포함하는 **큰 구** → 과적합
- 미래 데이터 대부분이 구 안으로 들어옴 → 이상치 탐지 실패

➡ 해결:
- 일부 정상 데이터는 구 밖으로 나가도록 허용
- Slack variable $\xi_i$ 도입

---

## 3. Primal Optimization Problem

### 목적식
$$\min_{R, a, \xi} \quad R^2 + C \sum_{i=1}^{n} \xi_i$$

### 제약식
$$\|\phi(x_i) - a\|^2 \le R^2 + \xi_i$$

$$\xi_i \ge 0$$

### 변수 의미
- $a$: hypersphere 중심
- $R$: 반지름
- $\xi_i$: 에러 허용 변수
- $C$: 에러 패널티 파라미터

---

## 4. Decision Function

$$f(x) = R^2 - \|\phi(x) - a\|^2$$

- $f(x) \ge 0$: 정상 (hypersphere 내부)
- $f(x) < 0$: 비정상 (hypersphere 외부)

➡ 거리 기반 결정 함수

---

## 5. 라그랑지안 (Primal)
<img width="1314" height="906" alt="image" src="https://github.com/user-attachments/assets/65cdb1dc-21b1-4e2a-a871-a4bb2864ac33" />

라그랑지 승수:
- $\alpha_i \ge 0$
- $\beta_i \ge 0$

### 라그랑지안
$$\mathcal{L} = R^2 + C \sum \xi_i - \sum \alpha_i (R^2 + \xi_i - \|\phi(x_i) - a\|^2) - \sum \beta_i \xi_i$$

---

## 6. KKT 조건
<img width="1300" height="886" alt="image" src="https://github.com/user-attachments/assets/984ab665-7776-416a-bcb5-a00282bcb1e7" />

### (1) $a$에 대한 미분
$$a = \sum_{i=1}^{n} \alpha_i \phi(x_i)$$

➡ 중심은 **Support Vector들의 가중합**

---

### (2) $R$에 대한 미분
$$\sum_{i=1}^{n} \alpha_i = 1$$

---

### (3) $\xi_i$에 대한 미분
$$\alpha_i + \beta_i = C$$

$$0 \le \alpha_i \le C$$

---

## 7. Dual Problem
<img width="1317" height="909" alt="image" src="https://github.com/user-attachments/assets/a2cb2f55-3895-4b2c-96f9-e3a336254574" />
<img width="1301" height="891" alt="image" src="https://github.com/user-attachments/assets/4c664ef2-ff9f-4bbb-ad04-9f271a35ac97" />
<img width="1302" height="868" alt="image" src="https://github.com/user-attachments/assets/1a09c59b-8798-4012-9ca6-f8106d361f2e" />

$$\min_{\alpha} \quad \sum_{i,j} \alpha_i \alpha_j K(x_i, x_j) - \sum_i \alpha_i K(x_i, x_i)$$

subject to:
- $\sum \alpha_i = 1$
- $0 \le \alpha_i \le C$

➡ Kernel trick 사용 가능  
➡ 비선형 hypersphere 학습 가능

---

## 8. 알파($\alpha_i$)의 의미 (Support Vector 해석)
<img width="1303" height="913" alt="image" src="https://github.com/user-attachments/assets/ed463df7-fb07-45d2-bd07-45521f67a242" />

### Case 1: $\alpha_i = 0$
- Hypersphere 내부
- 중심 계산에 기여 ❌

---

### Case 2: $0 < \alpha_i < C$
- Hypersphere 위 (on the boundary)
- Support Vector (정상)

---

### Case 3: $\alpha_i = C$
- Hypersphere 밖
- 에러 허용된 정상 데이터
- Outlier Support Vector

➡ 중심 $a$는 **Support Vector들만으로 결정**

[Image illustrating the different types of data points in SVDD: interior points, support vectors on the boundary, and outliers]
<img width="1312" height="906" alt="image" src="https://github.com/user-attachments/assets/5c88e666-f958-48d4-a58e-94f199ec02f8" />

---

## 9. 하이퍼파라미터 영향

### 9.1 $C$의 역할
- $C \uparrow$: 에러 거의 허용 안 함 → 큰 구
- $C \downarrow$: 에러 허용 → 작은 구

---

### 9.2 RBF Kernel의 $\sigma$

- $\sigma \downarrow$: 데이터 형태를 세밀하게 반영 → 과적합
- $\sigma \uparrow$: 부드러운 구 → 단순 모델

---

## 10. One-Class SVM vs SVDD

| 항목 | One-Class SVM | SVDD |
| :--- | :--- | :--- |
| **결정 경계** | Hyperplane | Hypersphere |
| **기준** | 원점과의 거리 | 중심과의 거리 |
| **직관성** | 낮음 | 높음 |
| **본질** | Margin 기반 | Compactness 기반 |

⚠️ 특정 조건에서는 **두 방법이 수학적으로 동일**

---

## 11. Anomaly Score 해석

- SVDD:
$$\text{score}(x) = \|\phi(x) - a\|^2$$

- One-Class SVM:
$$\text{score}(x) = \text{distance to hyperplane}$$

➡ 둘 다 거리 기반 Anomaly Score 제공 가능

---

## 12. 요약

- SVDD는 정상 데이터를 감싸는 hypersphere를 학습
- 일부 정상 데이터의 이탈을 허용하여 overfitting 방지
- 중심 $a$는 Support Vector의 선형 결합
- $C$와 Kernel 파라미터가 모델 복잡도 결정
- One-Class SVM과 구조적으로 매우 유사

---
