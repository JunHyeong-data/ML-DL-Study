# 📘 부분 최소제곱법 (Partial Least Squares, PLS)

---

## 1. 고차원 데이터와 차원 축소

고차원 데이터란 **설명 변수 $X$** 의 개수가 매우 많은 데이터를 의미한다.

- 변수 수가 많으면 정보가 많을 수 있음
- 동시에 **불필요한 변수 존재 가능성도 증가**
- 시각화 어려움
- 계산 복잡도 증가
- 모델 과적합 위험 증가

따라서 머신러닝에서는 **중요한 변수만 사용하여 모델링**하는 과정이 필요하다.

이를 **차원 축소(Dimensionality Reduction)** 라고 한다.

---

## 2. 차원 축소 방법의 분류

차원 축소는 크게 두 가지로 나뉜다.

### (1) 변수 선택 (Variable Selection)

- 기존 변수 중 일부만 선택
- 새로운 변수 생성 ❌

### (2) 변수 추출 (Variable Extraction)

- 기존 변수들의 **선형 결합 또는 변환**
- 새로운 변수 생성 ⭕

---

## 3. 지도 / 비지도에 따른 분류

출력 변수 $y$ 사용 여부에 따라 다음과 같이 나뉜다.

| 구분 | 변수 선택 | 변수 추출 |
|---|---|---|
| **지도학습** | 지도 변수 선택 | **지도 변수 추출 (PLS)** |
| **비지도학습** | 비지도 변수 선택 | **비지도 변수 추출 (PCA)** |

- **PCA**: 비지도 변수 추출
- **PLS**: 지도 변수 추출

---

## 4. PCA vs PLS 비교
<img width="1494" height="977" alt="image" src="https://github.com/user-attachments/assets/287d6979-2688-4608-a808-ec2a16b63a50" />

### 🔹 PCA (Principal Component Analysis)

- 입력 변수 $X$ 만 사용
- 분산(Variance)을 최대화하는 방향으로 주성분 추출

$$t = Xw$$

- 목적:
$$\text{Var}(t) \quad \text{최대화}$$

---

### 🔹 PLS (Partial Least Squares)
<img width="1457" height="879" alt="image" src="https://github.com/user-attachments/assets/0116ca57-664b-4fb2-b67b-a793f5a1447c" />

- 입력 변수 $X$ 와 출력 변수 $y$ 모두 사용
- **분산 + 출력과의 공분산을 동시에 고려**

$$t = Xw$$

- 목적:
$$\text{Cov}(t, y) \quad \text{최대화}$$

---

## 5. PLS의 핵심 개념
<img width="1481" height="965" alt="image" src="https://github.com/user-attachments/assets/8d6dbdfa-4ed5-4ffa-9cbd-bd87865fe7f6" />

PLS에서 추출되는 컴포넌트 $t$ 는 다음 성질을 가진다.

> **입력 변수들의 선형 결합으로 만들어진 변수 중에서**
>
> - 분산이 크고
> - 출력 변수 $y$ 와의 공분산이 큰 방향

---

## 6. 공분산 최대화 해석
<img width="1491" height="1013" alt="image" src="https://github.com/user-attachments/assets/20bf1af9-18ec-4ac0-9d22-7ef3a2ac7162" />

PLS는 다음 값을 최대화한다.

$$\text{Cov}(t, y)$$

단,
$$t = Xw$$
이므로,
$$\text{Cov}(Xw, y) = w^T X^T y$$

즉, 두 벡터의 내적 형태가 된다.

---

### 🔹 벡터 내적 해석
<img width="1443" height="1089" alt="image" src="https://github.com/user-attachments/assets/668d9e14-a920-4b96-a078-a322c3a44332" />

$$w^T (X^T y) = \|w\| \|X^T y\| \cos\theta$$

이를 최대화하려면:
- $\cos\theta = 1$
- 두 벡터의 방향이 동일

---

### ✅ 결론

$$\boxed{ w \propto X^T y }$$

즉,
$$w = \frac{X^T y}{\|X^T y\|}$$

---

## 7. 첫 번째 PLS 컴포넌트 추출
<img width="1450" height="1044" alt="image" src="https://github.com/user-attachments/assets/c092cd82-6cc9-4d54-a11b-f9d882cf9e4e" />

### Step 1. 데이터 전처리
- 평균 제거 (Mean-centering)
- 필요 시 정규화

---

### Step 2. 초기 설정
$$X_1 = X, \quad y_1 = y$$

---

### Step 3. 가중치 계산
$$w_1 = \frac{X_1^T y_1}{\|X_1^T y_1\|}$$

---

### Step 4. 첫 번째 PLS 컴포넌트
$$t_1 = X_1 w_1$$

---

## 8. 설명되지 않은 부분 (잔차)

PLS의 핵심 아이디어는 다음과 같다.

> **이미 설명한 부분을 제거하고 설명되지 않은 부분에 다시 최소제곱을 적용한다.**

---

### (1) y에 대한 잔차

회귀:
$$y_1 = t_1 b_1 + f_1$$

- $b_1$: 최소제곱법으로 추정
- $f_1$: 설명되지 않은 부분 (잔차)

$$y_2 = f_1$$

---

### (2) X에 대한 잔차

회귀:
$$X_1 = t_1 p_1^T + E_1$$

- $p_1$: 최소제곱 추정
- $E_1$: 잔차

$$X_2 = E_1$$

---

## 9. 두 번째 PLS 컴포넌트
<img width="1494" height="1093" alt="image" src="https://github.com/user-attachments/assets/8314340f-d06f-45fe-adc5-5891e2eb9b7c" />
<img width="1297" height="802" alt="image" src="https://github.com/user-attachments/assets/a452c91e-59c5-46e5-85a7-e63c249ea99f" />

이제 새로운 데이터는 다음과 같다.
$$X_2,\quad y_2$$

동일한 절차 반복:
$$w_2 = \frac{X_2^T y_2}{\|X_2^T y_2\|}$$

$$t_2 = X_2 w_2$$

---

## 10. 일반화 (k번째 컴포넌트)
<img width="1441" height="989" alt="image" src="https://github.com/user-attachments/assets/a0ce903d-0e8b-4a0a-803d-feda802824ba" />

k번째 PLS 컴포넌트는 다음 순서로 구한다.

1. 이전 잔차 데이터 사용
   $$X_k, \ y_k$$

2. 가중치 계산
   $$w_k = \frac{X_k^T y_k}{\|X_k^T y_k\|}$$

3. 컴포넌트 추출
   $$t_k = X_k w_k$$

4. 회귀 후 잔차 계산

$$y_{k+1} = y_k - t_k b_k$$
$$X_{k+1} = X_k - t_k p_k^T$$

---

## 11. 왜 ‘부분 최소제곱법’인가?

전체 데이터를 한 번에 최소제곱하는 것이 아니라,

> **설명되지 않은 부분(Partial)에 대해 반복적으로 최소제곱을 적용**

하기 때문이다. 그래서 이름이 **Partial Least Squares** 이다.

---

## 12. 최종 회귀 모델
<img width="1273" height="648" alt="image" src="https://github.com/user-attachments/assets/7b5379b3-f47b-4280-a4ce-5ab9708c65a9" />

PLS 모델은 다음 형태를 가진다.

$$\hat{y} = \beta_1 t_1 + \beta_2 t_2 + \dots + \beta_k t_k$$

여기서:
- $t_i$: PLS 컴포넌트
- 각 $t_i$ 는 원래 변수들의 선형 결합
- 매우 많은 정보를 압축 포함

---

## 13. 차원 축소

원래 변수 개수:
$$p \gg k$$
- $p$: 원래 변수 수
- $k$: 선택한 PLS 컴포넌트 수

$$k \ll p$$
👉 이것이 **PLS 기반 차원 축소**이다.

---

## ✅ 핵심 요약

- **PCA** → 분산 최대화 (y 미사용)
- **PLS** → 분산 + 출력 변수와의 공분산 최대화
- **PLS 컴포넌트는:**
  - 입력 변수의 선형 결합
  - 출력 변수 예측에 최적화됨
- **알고리즘 핵심:**
  - 설명된 부분 제거
  - 남은 부분에 최소제곱 반복 적용

---

# Partial Least Squares (PLS) — 부분 최소제곱법

---

## 1. 예제를 통한 PLS 개념 설명
<img width="1295" height="911" alt="image" src="https://github.com/user-attachments/assets/c7cf88ec-fbf2-4bba-81ab-7160f719b933" />
<img width="1496" height="1108" alt="image" src="https://github.com/user-attachments/assets/c720be0e-6c39-4429-b5b6-083ce5d02524" />
<img width="1494" height="1107" alt="image" src="https://github.com/user-attachments/assets/67748c03-8077-4f83-8f7b-7d927955c949" />
<img width="1506" height="1077" alt="image" src="https://github.com/user-attachments/assets/62ea5bfa-fde8-4644-912f-6228da602703" />
<img width="1476" height="1067" alt="image" src="https://github.com/user-attachments/assets/eeed254b-c451-4ca6-8055-f09a312bf84c" />
<img width="1502" height="1070" alt="image" src="https://github.com/user-attachments/assets/94d05041-fa81-4549-a1ee-64dfb481f151" />
<img width="1500" height="1069" alt="image" src="https://github.com/user-attachments/assets/ce9f8f20-d7f4-441c-9847-30c73d4c67ad" />

지금까지는 기호 위주의 설명을 진행했으므로, 이번에는 **간단한 수치 예제**를 통해 PLS 과정을 직관적으로 설명한다.

---

## 2. 데이터 구조

- 관측치 개수: **5개**
- 입력 변수 $X$: **3개**
- 출력 변수 $y$: **1개**

PLS는 **출력 변수 $y$** 를 이용하는 차원 축소 기법이므로 $y$의 정보가 매우 중요하다. 모든 데이터는 다음과 같이 **mean-centering(평균 0)** 되어 있다고 가정한다.

---

## 3. PLS 컴포넌트 개수

원래 입력 변수 개수가 3개이므로,
$$t_1,\; t_2,\; t_3$$
총 **3개의 PLS 컴포넌트**를 이론적으로 구할 수 있다.

---

## 4. 첫 번째 PLS 컴포넌트 $t_1$

### 4.1 정의
$$t_1 = X_1 w_1$$
- $X_1$: 원래 입력 데이터 $X$
- $w_1$: 첫 번째 가중치 벡터

### 4.2 가중치 벡터 $w_1$
PLS에서는 다음 조건을 만족하도록 $w_1$을 설정한다.
> **$t_1$과 $y$의 공분산을 최대화**

따라서,
$$w_1 = \frac{X_1^T y_1}{\|X_1^T y_1\|}$$
- 분모는 벡터 크기를 1로 만들기 위한 정규화(normalization)

### 4.3 첫 번째 컴포넌트 계산
$$t_1 = X_1 w_1$$
이때 $t_1$은 다음 성질을 가진다.
- 원래 변수들의 **선형결합**
- $y$와의 **공분산을 최대화**
- PCA와 달리 **출력 변수 정보를 반영**

---

## 5. 두 번째 PLS 컴포넌트 $t_2$

두 번째 컴포넌트는 **첫 번째 컴포넌트가 설명하지 못한 부분**만을 이용하여 계산한다.

### 5.1 $y$의 잔차 계산
$t_1$과 $y_1$ 사이에 회귀모형을 적용한다.
$$y_1 = t_1 b_1 + f_1$$
- $b_1$: 최소제곱법으로 추정
- $f_1$: 잔차(residual)

$$y_2 = f_1 = y_1 - t_1 b_1$$

### 5.2 $X$의 잔차 계산
마찬가지로 $X_1$과 $t_1$ 사이에 회귀를 수행한다.
$$X_1 = t_1 p_1^T + E_1$$
- $p_1$: loading 벡터
- $E_1$: 설명되지 않은 잔차

$$X_2 = E_1 = X_1 - t_1 p_1^T$$



---

### 5.3 두 번째 가중치 벡터
$$w_2 = \frac{X_2^T y_2}{\|X_2^T y_2\|}$$

### 5.4 두 번째 컴포넌트
$$t_2 = X_2 w_2$$

---

## 6. 세 번째 PLS 컴포넌트 $t_3$

같은 과정을 반복한다.
- $t_1, t_2$가 설명하지 못한 부분만 사용
- 새로운 잔차 $X_3, y_3$ 생성
- 가중치 $w_3$ 계산
- 컴포넌트 $t_3 = X_3 w_3$

---

## 7. 최종 PLS 모델

PLS 회귀 모델은 다음과 같이 표현된다.
$$\hat{y} = b_1 t_1 + b_2 t_2 + \cdots + b_k t_k$$
여기서:
- $t_k$: PLS 컴포넌트
- $b_k$: 최소제곱법으로 추정된 회귀계수

---

## 8. PLS와 선형회귀의 차이

### 일반 선형회귀
$$\hat{y} = X \beta$$

### PLS 회귀
$$\hat{y} = T b$$
- $T = [t_1, t_2, ..., t_k]$
- 각 $t_k$는 **모든 X 변수 정보를 포함한 선형결합 변수**

---

## 9. 차원 축소 방법

원래 입력 변수 $p = 15$개일 때, PLS 컴포넌트는 $t_1, ..., t_{15}$까지 생성 가능하지만 **모두 사용할 필요는 없다.**

### 컴포넌트 선택 방법
- 컴포넌트 개수 $k$를 증가시키며 모델 학습
- 각 경우에 대해 **Test Error(MSE)** 계산



### 최적 컴포넌트 수 선택
<img width="1464" height="1087" alt="image" src="https://github.com/user-attachments/assets/82c7a484-abd3-44cc-be64-f1608cba061d" />

예제 결과:
- Test error 최소 지점: **k = 5**
- 결과: $$15 \text{개 변수} \rightarrow 5 \text{개 PLS 컴포넌트}$$
👉 **차원 축소 달성**

---
<img width="1521" height="573" alt="image" src="https://github.com/user-attachments/assets/3162be5e-fabe-46d4-91d5-db72c8a586f2" />

## 10. 출력 변수가 여러 개인 경우 (Multi-response PLS)
<img width="1501" height="1028" alt="image" src="https://github.com/user-attachments/assets/cb4723d1-b0be-49ab-a0d5-725b160a7fe0" />
<img width="1478" height="1113" alt="image" src="https://github.com/user-attachments/assets/219afd91-6b94-4dc1-adfc-00d9ba692f7e" />
<img width="1516" height="1121" alt="image" src="https://github.com/user-attachments/assets/20ed8dd0-0dcd-4e6e-9e80-a0183d36110e" />

출력 변수가 여러 개인 경우:
$$Y = [y_1, y_2, ..., y_q]$$
이때는 $X$와 $Y$ 각각에서 선형결합 변수를 추출하여 두 잠재변수 간 공분산을 최대화한다.
$$t = X w,\quad u = Y q$$

---

## 11. 출력 변수가 범주형일 때
<img width="1497" height="972" alt="image" src="https://github.com/user-attachments/assets/42133488-9c4c-4271-a7bb-0f5bfbc89c1d" />

### 11.1 이진 분류 (2 classes)
$$y \in \{1, -1\}$$
- 그대로 PLS 적용 가능하며, 이를 **PLS-DA (Discriminant Analysis)** 라고 한다.

### 11.2 다중 클래스 (3개 이상)
One-hot encoding을 수행하여 출력 변수가 여러 개인 PLS를 적용한다.

---

## 12. R 패키지 예시

```r
library(pls)

# ncomp 매개변수를 통해 최적의 컴포넌트 수를 지정
model <- plsr(y ~ X, ncomp = 2, data = dataset)

summary(model)

y_pred <- predict(model, ncomp = 2, newdata = test_data)
---

📌 **PLS는 차원 축소 + 회귀를 동시에 수행하는 대표적인 지도 차원 축소 기법이다.**
