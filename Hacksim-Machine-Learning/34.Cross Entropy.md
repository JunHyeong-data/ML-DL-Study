# 크로스 엔트로피(Cross Entropy) 비용함수 정리

---

## 1. 비용함수(Cost Function)란?
<img width="1495" height="1038" alt="image" src="https://github.com/user-attachments/assets/5a9b6a03-7034-406e-bcd1-43f847514141" />

비용함수란 **모델이 예측한 값과 실제 정답 값 사이의 차이를 수치로 나타내는 함수**이다.

- 모델의 출력값: $$\hat{y}$$
- 실제 정답값: $$y$$

비용함수는 이 두 값의 차이를 측정하며, **비용함수를 최소로 만드는 파라미터(가중치)를 찾는 것**이 머신러닝과 딥러닝 학습의 핵심 목적이다.

---

## 2. 모델 학습의 기본 구조

뉴럴 네트워크 모델은 다음 과정을 반복한다.
1. 입력 데이터 $x$가 모델에 들어감
2. 모델이 예측값 $\hat{y}$ 출력
3. 실제값 $y$와 비교하여 비용함수 계산
4. 비용함수를 최소화하도록 파라미터 $w$ 업데이트

$$\text{목표} = \arg\min_{w} \; \text{Cost}(y, \hat{y})$$

---

## 3. 문제 유형에 따른 비용함수 차이

### ✔ 회귀 문제 (연속형 y)
<img width="1501" height="942" alt="image" src="https://github.com/user-attachments/assets/26442fbb-8bd2-4dc9-92ee-2bcd59beab4f" />

- y가 실수값 (예: 키, 몸무게, 가격 등)
- **평균제곱오차 (MSE):**
  $$MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$$

### ✔ 분류 문제 (범주형 y)
<img width="1489" height="1113" alt="image" src="https://github.com/user-attachments/assets/e72beb59-6c81-4ec9-8112-942f6df9f8b1" />

- y는 범주값 (예: 이진 분류 $y \in \{0, 1\}$)
- 숫자의 크기 자체에 의미가 없으므로 단순히 빼고 제곱하는 MSE는 적절하지 않다.
- 이 경우 사용하는 것이 **크로스 엔트로피(Cross Entropy)** 이다.

---

## 4. 크로스 엔트로피(Cross Entropy)

### 이진 분류에서의 비용함수
$$L(y, \hat{y}) = - \left[ y \log(\hat{y}) + (1 - y)\log(1 - \hat{y}) \right]$$

---

## 5. 경우별 해석

### ✅ 실제 $y = 1$ 인 경우
비용함수는 다음과 같이 단순화된다.
$$L = -\log(\hat{y})$$
- $\hat{y}$이 1에 가까울수록(정답) 비용은 0에 수렴한다.
- $\hat{y}$이 0에 가까울수록(오답) 비용은 무한대로 발산한다.



### ✅ 실제 $y = 0$ 인 경우
이때는 다음 식을 사용한다.
$$L = -\log(1 - \hat{y})$$
- $\hat{y}$이 0에 가까울수록(정답) 비용은 0에 수렴한다.
- $\hat{y}$이 1에 가까울수록(오답) 비용은 무한대로 발산한다.

---

## 6. 크로스 엔트로피의 핵심 특징

- 정답을 **확률적으로 얼마나 확신했는지** 평가한다.
- 잘 맞추면 비용 $\approx 0$, 틀리면 비용이 급격히 증가한다.
- 완전히 틀린 예측($y=1$인데 $\hat{y}=0$ 등)에 대해 **무한대에 가까운 강한 패널티**를 부여한다.

[Image comparing Mean Squared Error and Cross Entropy loss surfaces to show how Cross Entropy penalizes wrong classifications more severely]

---

## 7. 실제 예제
<img width="1465" height="1077" alt="image" src="https://github.com/user-attachments/assets/4fd71953-4568-4d29-9118-9eb388e4b27c" />

- **실제값 (y):** $[1, 0, 1, 1, 0]$
- **예측값 ($\hat{y}$):** $[0.99, 0.11, 0.50, 0.05, 0.75]$

| 실제 $y$ | 예측 $\hat{y}$ | 비용값(Loss) | 비고 |
| :--- | :--- | :--- | :--- |
| 1 | 0.99 | 0.01 | 매우 잘 맞춤 |
| 0 | 0.11 | 0.12 | 잘 맞춤 |
| 1 | 0.50 | 0.69 | 애매함 |
| 1 | 0.05 | 2.99 | **크게 틀림 (강한 패널티)** |
| 0 | 0.75 | 1.39 | 틀림 |

---

## 8. 정리

- **비용함수:** 실제값과 예측값의 차이를 수치화한 것.
- **크로스 엔트로피:** 분류 문제의 표준 비용함수로, 정답 확률에 근거하여 패널티를 부여함.
- **학습의 목표:** $$\boxed{\text{Cross Entropy Loss 최소화}}$$

이를 통해 모델은 정답일 확률을 더 높게 출력하도록 최적화된다.

---

## ✅ 한 줄 요약
> **크로스 엔트로피는 “모델이 정답을 얼마나 확신하며 맞췄는가”를 측정하는 비용함수이다.**
