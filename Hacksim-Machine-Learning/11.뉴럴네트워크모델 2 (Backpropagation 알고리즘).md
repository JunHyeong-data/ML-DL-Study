# 인공신경망 (Neural Network) – 2강
## 모델 학습: Gradient Descent와 Backpropagation

### 1. 강의 목표
이번 강의에서는 다음 내용을 다룬다.

- 인공신경망 모델의 파라미터(가중치) 추정
- 비용 함수(Cost Function)의 정의
- 경사하강법(Gradient Descent)을 이용한 최적화
- **오차 역전파 알고리즘(Backpropagation)**의 유도와 의미
- 실제 분류 문제와 회귀 문제 적용 예시

즉,
👉 인공신경망이 어떻게 학습되는지를 수식과 알고리즘 관점에서 설명한다.

---

### 2. 인공신경망의 파라미터
인공신경망에서 학습 대상이 되는 파라미터는 **가중치(weight)**이다.

- 출력층–은닉층 사이 가중치: $w_{kj}$
- 은닉층–입력층 사이 가중치: $w_{ji}$

이 가중치들은 알고리즘에 의해 자동으로 결정되는 파라미터이며, 은닉층 개수, 노드 개수, 활성화 함수 종류 등은 하이퍼파라미터이다.

---

### 3. 비용 함수 (Cost Function)
모델 학습의 목표는 비용 함수를 최소화하는 가중치 값을 찾는 것이다.

#### 3.1 회귀 문제 (연속형 출력)
출력이 연속형일 경우, 비용 함수는 보통 제곱 오차 함수를 사용한다.
$$
L = \frac{1}{2} \sum_{k=1}^{n} (y_k - o_k)^2
$$
- $y_k$: 실제 값
- $o_k$: 모델의 예측값

#### 3.2 분류 문제 (범주형 출력)
분류 문제에서는 로지스틱 회귀와 동일하게 크로스 엔트로피(Cross-Entropy) 기반 비용 함수를 사용한다.

---

### 4. Gradient Descent (경사하강법)
경사하강법은 비용 함수의 **기울기(gradient)**를 이용하여 가중치를 반복적으로 업데이트하는 방법이다.

#### 4.1 기본 업데이트 식
$$
w^{(t+1)} = w^{(t)} - \alpha \frac{\partial L}{\partial w}
$$
- $\alpha$: 학습률(learning rate)
- 기울기 방향의 반대 방향으로 이동

#### 4.2 학습률의 의미
- 학습률이 크면: 빠르지만 불안정
- 학습률이 작으면: 안정적이지만 느림

---

### 5. 신경망 구조와 표기
<img width="1436" height="1077" alt="image" src="https://github.com/user-attachments/assets/755b9844-658b-435c-b71f-91775d15157a" />
<img width="1425" height="1077" alt="image" src="https://github.com/user-attachments/assets/0f675e5a-6cba-4504-b974-a376c7bd0c3c" />

- 입력: $x_i$
- 은닉층 노드 출력: $h_j$
- 출력층 노드 출력: $o_k$

**은닉층 노드**
$$
h_j = \sigma\left(\sum_i w_{ji} x_i\right)
$$

**출력층 노드**
$$
o_k = \sigma\left(\sum_j w_{kj} h_j\right)
$$
여기서 $\sigma(\cdot)$는 시그모이드 함수이다.

---

### 6. 출력층–은닉층 가중치 업데이트
<img width="1463" height="916" alt="image" src="https://github.com/user-attachments/assets/22472509-c1b3-4261-a622-9108d9640a76" />

<img width="1453" height="965" alt="image" src="https://github.com/user-attachments/assets/0903bd60-987f-4631-ac1e-6ce6a5cc8398" />

#### $w_{kj}$에 대한 미분
목표는 다음 값을 구하는 것이다.
$$
\frac{\partial L}{\partial w_{kj}}
$$
연쇄 법칙(chain rule)을 적용하면,
$$
\frac{\partial L}{\partial w_{kj}} = \frac{\partial L}{\partial o_k} \cdot \frac{\partial o_k}{\partial net_k} \cdot \frac{\partial net_k}{\partial w_{kj}}
$$

각 항은 다음과 같다.
- $\frac{\partial L}{\partial o_k} = -(y_k - o_k)$
- $\frac{\partial o_k}{\partial net_k} = o_k (1 - o_k)$
- $\frac{\partial net_k}{\partial w_{kj}} = h_j$

따라서,
$$
\frac{\partial L}{\partial w_{kj}} = -(y_k - o_k), o_k (1 - o_k), h_j
$$

#### 출력층 가중치 업데이트
$$
w_{kj}^{(t+1)} = w_{kj}^{(t)} \alpha (y_k - o_k), o_k (1 - o_k), h_j
$$

---

### 7. 은닉층–입력층 가중치 업데이트
<img width="1499" height="878" alt="image" src="https://github.com/user-attachments/assets/f2c05b0c-0061-46ce-ab13-68de18d0027c" />

<img width="1502" height="993" alt="image" src="https://github.com/user-attachments/assets/6ed772b6-646b-4e88-a67a-c25e52107ef8" />

#### $w_{ji}$에 대한 미분
은닉층 가중치는 출력층을 거쳐서 간접적으로 영향을 받는다. 연쇄 법칙을 적용하면,
$$
\frac{\partial L}{\partial w_{ji}} = \sum_k \frac{\partial L}{\partial o_k} \cdot \frac{\partial o_k}{\partial net_k} \cdot \frac{\partial net_k}{\partial h_j} \cdot \frac{\partial h_j}{\partial net_j} \cdot \frac{\partial net_j}{\partial w_{ji}}
$$

정리하면,
$$
\frac{\partial L}{\partial w_{ji}} = \left( \sum_k (y_k - o_k), o_k (1 - o_k), w_{kj} \right) h_j (1 - h_j), x_i
$$

#### 은닉층 가중치 업데이트
$$
w_{ji}^{(t+1)} = w_{ji}^{(t)} \alpha \left( \sum_k (y_k - o_k), o_k (1 - o_k), w_{kj} \right) h_j (1 - h_j), x_i
$$

---

### 8. Backpropagation 알고리즘
지금까지 유도한 가중치 업데이트 과정을 **오차 역전파(Backpropagation)**라고 한다.
<img width="1441" height="880" alt="image" src="https://github.com/user-attachments/assets/f2e57708-3fb1-4844-9b0f-768c9a01f3dc" />

**알고리즘 요약**
1. 가중치를 작은 랜덤 값으로 초기화
2. **순전파(Forward Propagation)**: 입력 → 은닉층 → 출력층
3. 비용 함수 계산
4. **역전파(Backward Propagation)**: 출력층 → 은닉층 → 입력층
5. 가중치 업데이트
6. 오차가 충분히 줄어들 때까지 반복

---

### 9. 학습 방식
- **온라인 학습**: 관측치 1개마다 가중치 업데이트
- **배치 학습**: 여러 관측치를 모아서 한 번에 업데이트

---

### 10. 적용 예시
#### 10.1 XOR 분류 문제
<img width="1438" height="692" alt="image" src="https://github.com/user-attachments/assets/10a5df7e-32b8-4dd8-9aa9-a15cc75d1dc9" />

- 단층 퍼셉트론: 해결 불가
- 다층 퍼셉트론: 해결 가능
- 은닉층 1개, 노드 2개인 신경망에서 반복 학습(iteration)을 증가시키면 정확도가 향상됨.

#### 10.2 회귀 문제 예시
- 시그모이드 곡선 형태의 데이터
- 신경망은 비선형 구조를 잘 학습
- 단순 선형 회귀나 다항 회귀보다 높은 성능 달성

---

### 11. 강의 정리
- 인공신경망 학습 = 비용 함수 최소화 문제
- Gradient Descent + Backpropagation 사용
- 출력층 → 은닉층 → 입력층 순으로 오차 전파
- 분류 문제와 회귀 문제 모두 적용 가능

다음 강의에서는
👉 **신경망 구조 설계와 실전 구현**을 다룬다.
