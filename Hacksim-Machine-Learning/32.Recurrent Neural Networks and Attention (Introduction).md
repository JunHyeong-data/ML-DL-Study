# Recurrent Neural Network(RNN) & Attention 핵심 정리

---

## 1. 왜 RNN과 Attention이 필요한가?
<img width="1435" height="851" alt="image" src="https://github.com/user-attachments/assets/10ca8c69-caf2-4dcf-9a19-b5449cbf1230" />
<img width="1468" height="1060" alt="image" src="https://github.com/user-attachments/assets/a94deab9-ad3d-4eca-84d0-10ec096025f7" />
<img width="1502" height="991" alt="image" src="https://github.com/user-attachments/assets/32a3ca58-8fd8-4e3f-8a98-6f206e8136ca" />

기존의 신경망(Feedforward Neural Network)은  
**각 입력을 독립적인 데이터로 처리**한다.

하지만 다음과 같은 데이터는 이 방식으로 잘 다룰 수 없다.

- 문장 (자연어)
- 시계열 데이터
- 음성
- 비디오 프레임

이 데이터들의 공통점은 다음과 같다.

> **현재 정보는 과거 정보에 의존한다**

이를 해결하기 위해 등장한 것이 **RNN**이고,  
RNN의 한계를 극복하기 위해 등장한 것이 **Attention**이다.

---

## 2. Recurrent Neural Network (RNN)
<img width="1451" height="1027" alt="image" src="https://github.com/user-attachments/assets/aaf79306-0601-4f63-90ec-f3a5fe1ac18f" />

### 2.1 RNN의 핵심 아이디어

RNN은 **과거 정보를 hidden state에 저장**하여  
현재 예측에 함께 사용하는 신경망이다.

- 현재 입력: $x_t$
- 과거 정보: $h_{t-1}$
- 현재 hidden state: $h_t$

즉,

> **현재 = 현재 입력 + 과거 기억**



---
<img width="1463" height="1041" alt="image" src="https://github.com/user-attachments/assets/17409fcd-3f42-43ce-99e1-66e17cb1cb06" />
<img width="1461" height="1051" alt="image" src="https://github.com/user-attachments/assets/cc40f2d3-ab18-4e64-a8e0-75b01549535c" />
<img width="1463" height="1050" alt="image" src="https://github.com/user-attachments/assets/b3326c37-44af-40d4-bad5-000b0046d904" />

### 2.2 RNN의 수식
<img width="1466" height="1043" alt="image" src="https://github.com/user-attachments/assets/faeb72f5-1a58-4310-b9ab-674ce693af3b" />
<img width="1493" height="1068" alt="image" src="https://github.com/user-attachments/assets/65229411-a825-4756-aad7-72f22a32749c" />
<img width="1499" height="1044" alt="image" src="https://github.com/user-attachments/assets/ef1151cf-3a53-4ad0-89b2-810d27f5547a" />
<img width="1474" height="1050" alt="image" src="https://github.com/user-attachments/assets/c568452a-3ce3-4d63-b8f5-e09af79c9e92" />

$$h_t = \tanh(W_{xh}x_t + W_{hh}h_{t-1} + b)$$

- $W_{xh}$: 입력 → hidden
- $W_{hh}$: 이전 hidden → 현재 hidden
- 모든 시점에서 **같은 파라미터 공유**

---

### 2.3 RNN의 구조적 특징

- 시간 축 방향으로 반복되는 구조
- 이를 **Unrolling**이라고 표현
- 시계열 길이가 달라도 처리 가능



---

### 2.4 RNN의 출력 형태
<img width="1479" height="1006" alt="image" src="https://github.com/user-attachments/assets/b055b657-e306-44bb-b43d-fed8b15ed015" />
<img width="1491" height="1064" alt="image" src="https://github.com/user-attachments/assets/a596146c-f813-4b48-9cd4-6a9186f18ddb" />
<img width="1473" height="1039" alt="image" src="https://github.com/user-attachments/assets/90a23c4f-9686-4c6b-a50c-674fd831b2ac" />
<img width="1469" height="1013" alt="image" src="https://github.com/user-attachments/assets/3c7459d1-3050-4b18-afbe-899b9fdb6d68" />
<img width="1489" height="1023" alt="image" src="https://github.com/user-attachments/assets/f75f2ba7-5f25-4139-a18e-b2bbd16eb330" />

- **Many-to-One** - 여러 시점 → 하나의 출력  
  - 예: 주가 상승/하락 예측

- **Many-to-Many** - 입력 시퀀스 → 출력 시퀀스  
  - 예: 문장 번역

---

### 2.5 RNN의 근본적인 한계

#### 1️⃣ Long-Term Dependency 문제
- 시계열이 길어질수록
- 과거 정보가 점점 사라짐

#### 2️⃣ Gradient Vanishing / Exploding
- Backpropagation Through Time(BPTT) 과정에서
- gradient가 0에 수렴하거나 무한히 커짐

➡️ LSTM, GRU가 등장했지만  
➡️ **구조적 한계는 여전히 존재**

---

## 3. Sequence-to-Sequence와 Context Vector

RNN 기반 번역 모델에서는  
**Encoder–Decoder 구조**를 사용한다.

- Encoder: 입력 문장 요약
- Decoder: 요약 정보를 이용해 출력 생성

여기서 Encoder의 마지막 hidden state를  
**Context Vector**라고 부른다.

---

### 3.1 Context Vector의 문제점
<img width="1488" height="1038" alt="image" src="https://github.com/user-attachments/assets/596ff064-7162-4028-8bd2-d80bf656c079" />
<img width="1459" height="1011" alt="image" src="https://github.com/user-attachments/assets/8ca6b1e5-719a-4a84-a046-c7038ee9201f" />

- 모든 입력 정보를 **하나의 벡터에 압축**
- 문장이 길어질수록 정보 손실
- 출력 시점마다 다른 중요 단어 반영 불가

이 문제를 해결하기 위해 **Attention**이 등장했다.

---

## 4. Attention 메커니즘
<img width="1465" height="1032" alt="image" src="https://github.com/user-attachments/assets/d96ce052-7693-43ae-9de6-02972e323a0f" />
<img width="1491" height="1026" alt="image" src="https://github.com/user-attachments/assets/3c264c18-29e0-4ee5-992c-bc5813f012e7" />
<img width="1476" height="1020" alt="image" src="https://github.com/user-attachments/assets/0eee7f0f-3fc6-498d-bf46-e035d8d944dd" />
<img width="1461" height="1000" alt="image" src="https://github.com/user-attachments/assets/973029d0-e7d4-4858-ab73-e8137db43c45" />
<img width="1499" height="1029" alt="image" src="https://github.com/user-attachments/assets/820ad5e1-c19d-48df-9989-261e90a5fdf1" />

### 4.1 Attention의 핵심 질문

> “이 출력 단어를 만들 때,  
> 입력 문장의 **어떤 부분이 중요한가?**”

---

### 4.2 Attention의 핵심 아이디어

- Encoder의 **모든 hidden state를 저장**
- Decoder의 각 시점마다
  - 입력 전체를 다시 참고
  - 중요도에 따라 가중합

➡️ **출력 시점마다 다른 Context Vector 생성**



---

### 4.3 Attention 계산 과정 (Step-by-Step)

#### Step 1️⃣ 유사도(score) 계산

Decoder의 현재 hidden state $h_t$와  
Encoder의 hidden state $h_s$ 간 유사도:

$$\text{score}(h_t, h_s) = h_t \cdot h_s$$

---

#### Step 2️⃣ Attention Weight (Softmax)

$$\alpha_{ts} = \frac{\exp(\text{score}_{ts})}{\sum_{s'} \exp(\text{score}_{ts'})}$$

- 각 입력 시점의 중요도
- 합은 항상 1
- **해석 가능**

---

#### Step 3️⃣ Context Vector 생성

$$c_t = \sum_s \alpha_{ts} h_s$$

- 출력 시점 $t$에 특화된 정보 요약

---

### 4.4 Attention의 직관적 의미

- RNN:  
  > “기억 하나로 모든 걸 해결”

- Attention:  
  > “필요할 때마다 과거를 다시 찾아본다”

---

## 5. RNN vs RNN + Attention

| 항목 | RNN | RNN + Attention |
|---|---|---|
| 과거 정보 사용 | 제한적 | 전체 사용 |
| 긴 시퀀스 처리 | 약함 | 강함 |
| 출력 시점별 중요도 | 불가능 | 가능 |
| 해석 가능성 | 낮음 | 높음 |

---

## 6. Attention의 Explainable AI 관점

Attention weight $\alpha$는 다음을 의미한다.

> “이 출력은  
> 이 입력을 **얼마나 참고했는가**”

따라서 Attention은:
- 시점별 중요도 제공
- 관측치 단위 해석 가능
- XAI의 중요한 도구

⚠️ 단,  
> Attention ≠ 완전한 인과 설명  
(해석 가능하지만 인과는 아님)

---

## 7. Attention 이후의 발전

- Self-Attention
- Multi-Head Attention
- Transformer
- BERT / GPT

👉 **RNN의 시대를 끝낸 핵심 기술이 Attention**

---

## 8. 핵심 요약 (시험 대비)

- RNN: 과거 정보를 hidden state로 전달
- 문제점: Long-term dependency
- Attention: 출력 시점마다 중요한 입력 선택
- Context vector를 동적으로 생성
- Transformer의 기반 개념

---
