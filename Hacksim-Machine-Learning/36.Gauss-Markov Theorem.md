<img width="1521" height="1063" alt="image" src="https://github.com/user-attachments/assets/f30003a9-e10a-4252-bbb2-19e1885546fe" />
<img width="1498" height="1106" alt="image" src="https://github.com/user-attachments/assets/897f3ff3-e967-4999-b90f-4f9b0ec6fe37" />
<img width="1488" height="1100" alt="image" src="https://github.com/user-attachments/assets/d9413e37-a1a3-4113-a6ea-b1fe0275f5f7" />

# 가우스-마르코프 정리(Gauss-Markov Theorem)의 증명

## 1. 개요 (Objective)
가우스-마르코프 정리는 **"오차항의 기댓값이 0이고, 분산이 일정하며, 서로 독립인 선형 회귀 모델에서, 최소제곱추정량(OLS)은 모든 선형 불편 추정량 중에서 가장 작은 분산을 가진다(BLUE)"**는 것을 증명한다.

---

## 2. 기본 설정 (Assumptions)
선형 회귀 모델을 다음과 같이 정의한다.
$$\mathbf{y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\varepsilon}$$
여기서 OLS 추정량은 다음과 같다.
$$\hat{\boldsymbol{\beta}}_{OLS} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y} = \mathbf{M}\mathbf{y} \quad (\text{단, } \mathbf{M} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T)$$

---

## 3. 증명 단계 (Step-by-Step Proof)

### Step 1. 임의의 선형 불편 추정량 가정
또 다른 임의의 선형 추정량 $\tilde{\boldsymbol{\beta}}$를 정의한다. 선형성(Linearity)에 의해 다음과 같이 쓸 수 있다.
$$\tilde{\boldsymbol{\beta}} = \mathbf{C}\mathbf{y}$$

### Step 2. 불편성(Unbiasedness) 조건 도출
$\tilde{\boldsymbol{\beta}}$가 불편 추정량이 되기 위해서는 $\mathbb{E}[\tilde{\boldsymbol{\beta}}] = \boldsymbol{\beta}$를 만족해야 한다.
$$\mathbb{E}[\tilde{\boldsymbol{\beta}}] = \mathbb{E}[\mathbf{C}(\mathbf{X}\boldsymbol{\beta} + \boldsymbol{\varepsilon})] = \mathbf{C}\mathbf{X}\boldsymbol{\beta} + \mathbf{C}\mathbb{E}[\boldsymbol{\varepsilon}] = \mathbf{C}\mathbf{X}\boldsymbol{\beta}$$
모든 $\boldsymbol{\beta}$에 대해 이 값이 $\boldsymbol{\beta}$와 같으려면 다음의 **불편성 제약 조건**이 성립해야 한다.
$$\boxed{\mathbf{C}\mathbf{X} = \mathbf{I}}$$

### Step 3. 행렬 C의 분해
임의의 행렬 $\mathbf{C}$를 OLS 추정량의 행렬 $\mathbf{M}$과 편차 행렬 $\mathbf{D}$의 합으로 표현한다.
$$\mathbf{C} = \mathbf{M} + \mathbf{D} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T + \mathbf{D}$$
위 식을 Step 2의 조건($\mathbf{C}\mathbf{X} = \mathbf{I}$)에 대입하면:
$$(\mathbf{M} + \mathbf{D})\mathbf{X} = \mathbf{M}\mathbf{X} + \mathbf{D}\mathbf{X} = \mathbf{I}$$
여기서 $\mathbf{M}\mathbf{X} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{X} = \mathbf{I}$이므로, 다음 조건을 얻는다.
$$\boxed{\mathbf{D}\mathbf{X} = \mathbf{0}}$$

### Step 4. 분산(Variance) 계산 및 비교
$\tilde{\boldsymbol{\beta}}$의 분산 행렬을 전개한다. (단, $\text{Var}(\mathbf{y}) = \sigma^2 \mathbf{I}$)
$$\text{Var}(\tilde{\boldsymbol{\beta}}) = \mathbf{C} \text{Var}(\mathbf{y}) \mathbf{C}^T = \sigma^2 \mathbf{C}\mathbf{C}^T$$
$$\text{Var}(\tilde{\boldsymbol{\beta}}) = \sigma^2 (\mathbf{M} + \mathbf{D})(\mathbf{M} + \mathbf{D})^T = \sigma^2 (\mathbf{M}\mathbf{M}^T + \mathbf{M}\mathbf{D}^T + \mathbf{D}\mathbf{M}^T + \mathbf{D}\mathbf{D}^T)$$

여기서 교차항 $\mathbf{M}\mathbf{D}^T$를 확인하면:
$$\mathbf{M}\mathbf{D}^T = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{D}^T = (\mathbf{X}^T\mathbf{X})^{-1}(\mathbf{D}\mathbf{X})^T$$
Step 3에서 $\mathbf{D}\mathbf{X} = \mathbf{0}$이었으므로 **교차항은 0**이 된다.



따라서 최종 분산식은 다음과 같다.
$$\text{Var}(\tilde{\boldsymbol{\beta}}) = \sigma^2 \mathbf{M}\mathbf{M}^T + \sigma^2 \mathbf{D}\mathbf{D}^T$$
여기서 $\sigma^2 \mathbf{M}\mathbf{M}^T$는 OLS의 분산인 $\text{Var}(\hat{\boldsymbol{\beta}}_{OLS})$와 같다.

### Step 5. 최소 분산성(Efficiency) 확인
$$\text{Var}(\tilde{\boldsymbol{\beta}}) = \text{Var}(\hat{\boldsymbol{\beta}}_{OLS}) + \sigma^2 \mathbf{D}\mathbf{D}^T$$
$\mathbf{D}\mathbf{D}^T$는 반양정치 행렬(Positive Semi-definite matrix)이므로 항상 0보다 크거나 같다($\succeq 0$).
$$\therefore \text{Var}(\tilde{\boldsymbol{\beta}}) \succeq \text{Var}(\hat{\boldsymbol{\beta}}_{OLS})$$

---

## 4. 결론 (Conclusion)
임의의 선형 불편 추정량 $\tilde{\boldsymbol{\beta}}$의 분산은 OLS 추정량의 분산에 항상 양의 값($\sigma^2 \mathbf{D}\mathbf{D}^T$)을 더한 형태가 된다. 

즉, **분산이 최소가 되는 조건은 $\mathbf{D} = \mathbf{0}$일 때뿐이며, 이는 곧 OLS 추정량 자신을 의미한다.** 따라서 OLS 추정량은 BLUE이다.

> **교수님의 한 줄 평:**
> "OLS에서 벗어나려는 모든 시도는 반드시 분산을 추가로 발생시킨다."
