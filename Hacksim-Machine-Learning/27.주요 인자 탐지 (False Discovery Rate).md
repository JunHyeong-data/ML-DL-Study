# 주요 인자 탐지 (Key Factor Detection)

---

## 1. 주요 인자 탐지란?

**주요 인자 탐지(Key Factor Detection)**란,

> 불량(이상)이 발생했을 때  
> 그 원인이 되는 **가장 중요한 변수(인자, 센서, 파라미터)**를 찾는 과정이다.

---

### ✔ 왜 중요한가?

현실 세계의 대부분 문제는 다음과 같다.
- LCD / LED 패널 불량
- 제조 공정 품질 이상
- 금융 사기 탐지
- 네트워크 침입 탐지
- DDoS 공격 탐지

특히 제조업에서는 **“불량이 발생했는가?”** 보다 **“왜 발생했는가?”** 가 훨씬 중요하다.

---

## 2. 문제 정의

### 데이터 구조
- 관측치 수: $n$
- 변수(인자) 수: $p$

각 관측치는 다음 두 그룹 중 하나에 속한다.
- 정상 (Normal)
- 불량 (Abnormal)

### 🎯 목표
> $p$개의 변수 중에서 **정상과 불량을 가장 잘 구분하는 주요 인자들을 찾는 것**

---

## 3. 핵심 아이디어

### 중요한 인자란?
> 정상과 불량 사이에서 **분포 차이가 크게 나타나는 변수**



❌ 단순히 값이 다르다고 중요한 것이 아니다.  
⭕ **얼마나 다르고, 얼마나 일관되게 다른가**가 핵심이다.

---

## 4. 직관적 예시

- **예제 1 — 인자 $X_1$:** 정상(20), 불량(20) $\rightarrow$ **중요 인자 아님**
- **예제 2 — 인자 $X_2$:** 정상(10), 불량(200) $\rightarrow$ **매우 중요한 인자**
- **예제 3 — 인자 $X_3$:** 정상(12), 불량(11, 12) $\rightarrow$ **중요도가 낮거나 애매한 인자**

---

## 5. 평균 차이 기반 접근

각 변수 $X_j$에 대해 다음 값을 계산한다.

### 정상 평균
$$\mu_j^{(N)} = \frac{1}{n_N} \sum_{i \in Normal} x_{ij}$$

### 불량 평균
$$\mu_j^{(A)} = \frac{1}{n_A} \sum_{i \in Abnormal} x_{ij}$$

### 평균 차이
$$D_j = |\mu_j^{(N)} - \mu_j^{(A)}|$$

### 해석
- $D_j$ 가 클수록 정상과 불량의 차이가 크며, **중요 인자일 가능성이 증가**한다.

---

## 6. 평균 차이 방법의 한계

### 문제 1. 분산을 고려하지 못함
- **인자 A:** 정상(10 ± 0.1), 불량(20 ± 0.1) $\rightarrow$ 매우 명확한 차이
- **인자 B:** 정상(10 ± 50), 불량(20 ± 50) $\rightarrow$ 평균은 같지만 분포가 완전히 겹침

📌 평균 차이는 동일하지만 **실제 구분력은 완전히 다르다.**

---

## 7. 분산까지 고려한 개념

따라서 다음이 필요하다: **평균 차이 $\div$ 변동성**

### 대표적인 형태
$$Score_j = \frac{|\mu_j^{(N)} - \mu_j^{(A)}|}{\sigma_j} \quad \text{또는} \quad Score_j = \frac{|\mu_j^{(N)} - \mu_j^{(A)}|}{\sqrt{\sigma_N^2 + \sigma_A^2}}$$

### 의미
- 평균 차이 $\uparrow \implies$ 중요도 $\uparrow$
- 분산 $\uparrow \implies$ 중요도 $\downarrow$

---

## 8. 통계적 관점
이 개념은 $t$-test, SNR(Signal-to-Noise Ratio), Fisher score와 매우 유사하다.

### Fisher Score
$$F_j = \frac{(\mu_j^{(N)} - \mu_j^{(A)})^2}{\sigma_N^2 + \sigma_A^2}$$

- 분류 모델 학습 전 변수 중요도를 평가할 수 있으며 계산량이 매우 작다.



---

## 9. 주요 인자 탐지 절차
1. 정상 / 불량 데이터 분리
2. 각 변수별 평균 및 분산 계산
3. 중요도 점수 산출
4. 점수 기준 정렬 (Ranking)
5. 상위 변수 선택

---

## 10. 주요 인자 탐지의 활용
- 공정 불량 원인 분석 및 센서 이상 감지
- 변수 선택 (Feature Selection)을 통한 모델 단순화
- 모델의 해석 가능성(Explainability) 향상

---

## 11. 요약

### ✔ 주요 인자 탐지란?
- 정상과 불량을 가장 잘 구분하는 변수를 찾는 것

### ✔ 핵심 기준
- 평균 차이, 분산, 분포 겹침 정도

### ✔ 대표 기법
- Mean difference, Fisher score, $t$-statistic, Mutual Information, Model-based importance

✅ **주요 인자 탐지는 분류 이전 단계에서 반드시 수행해야 하는 핵심 분석 과정이다.**

---
# 다중 가설 검정 (Multiple Hypothesis Testing)

---

## 1. 문제 상황
<img width="1401" height="1092" alt="image" src="https://github.com/user-attachments/assets/53af9aff-c96c-4e54-bed6-3e34cce4181c" />
<img width="1508" height="1056" alt="image" src="https://github.com/user-attachments/assets/6d3985eb-1e14-445f-9a4a-624bd92c10b0" />

변수(센서)가 하나일 때는 문제가 없다.
- 가설 검정 1회
- 유의수준 $\alpha = 0.01$
- Type I error 확률 = 1%

현실에서는 센서가 100개, 변수가 1000개에 달하며 각 변수마다 가설 검정을 수행해야 한다.
> **가설 검정을 변수마다 각각 수행해야 한다.**

---

## 2. 다중 가설 검정이란?

변수마다 가설 검정을 수행하면 가설 개수 $p$만큼의 가설이 존재한다.
$$H_1, H_2, \dots, H_p$$
이를 **동시에 검정**하는 문제를 **다중 가설 검정 (Multiple Hypothesis Testing)**이라고 한다.

---

## 3. 단순 반복 검정의 문제점

❌ “가설이 여러 개면 그냥 각각 하면 되는 거 아닌가요?”라고 생각하기 쉽지만, 이는 틀린 생각이다.

---

## 4. 핵심 문제 — 오류 확률 폭증

- **단일 가설:** $\alpha = 0.01 \implies \text{Type I error} = 1\%$
- **가설이 2개일 때:**
  $$P(\text{error 없음}) = (1 - \alpha)^2 = 0.99^2 = 0.9801$$
  $$\alpha_{total} = 1 - 0.9801 \approx 0.02$$
- **가설이 10개일 때:** $1 - 0.99^{10} \approx 0.096$ (약 10% 오류)
- **가설이 100개일 때:** $1 - 0.99^{100} \approx 0.63$ (63% 확률로 잘못된 기각 발생)

📌 분명히 1%만 틀리고 싶다고 설정했음에도, 실제로는 **63% 확률로 틀리게 되는 것**이다.



---

## 5. 이것이 바로 문제
이를 통계학에서는 **Multiple Testing Problem (다중 검정 문제)**이라고 부른다.

---
<img width="1513" height="1070" alt="image" src="https://github.com/user-attachments/assets/104a8e72-5b36-4e7d-bbe0-24d94b1912c1" />

## 6. Family-Wise Error Rate (FWER)
1970~90년대까지 주로 사용된 개념이다.

### 정의
$$\text{FWER} = P(\text{잘못 기각된 가설이 하나 이상 존재})$$
> **단 하나라도 틀리면 실패**라는 매우 강한 기준.

### 대표 방법: Bonferroni
$$\alpha' = \frac{\alpha}{m}$$
- $m$: 가설 개수
- 유의수준을 극단적으로 줄여 보수적으로 검정한다. 
- **문제점:** 너무 보수적이어서 중요한 변수까지 놓치는 **Type II error**가 증가한다.

---

## 7. 새로운 관점의 필요성
- **개별 검정:** 너무 많이 뽑힘 (오류 과다)
- **FWER:** 너무 적게 뽑힘 (발견 누락)
👉 “적당히 틀리면서도 유용하게 선택할 수는 없을까?”

---

## 8. FDR의 등장 (1995)
### 제안자: Yoav Benjamini, Yosef Hochberg
> **기각된 가설들 중에서 잘못 기각된 가설의 비율을 제어하자**

---

## 9. FDR 정의
<img width="1480" height="1107" alt="image" src="https://github.com/user-attachments/assets/f89ae748-5313-4a22-8f01-676e47c8c95e" />
<img width="1450" height="1074" alt="image" src="https://github.com/user-attachments/assets/987e9ec0-99f7-487c-8669-09df8f1b34e5" />
<img width="1532" height="1104" alt="image" src="https://github.com/user-attachments/assets/a7151b44-10c3-4b4c-a202-eb21928aba3c" />
<img width="1478" height="1080" alt="image" src="https://github.com/user-attachments/assets/1a510dbb-28cc-4ec0-a1d3-91678e84f016" />
<img width="1468" height="1109" alt="image" src="https://github.com/user-attachments/assets/7997ae14-5852-4dc6-a6f9-062a6ce01df6" />
<img width="1482" height="1075" alt="image" src="https://github.com/user-attachments/assets/c03bf8ab-7038-4c71-b8fe-a2de9d2ccdf0" />
<img width="1460" height="933" alt="image" src="https://github.com/user-attachments/assets/caf46e44-e52d-47f3-b91d-731a51fa20ff" />

$$\text{FDR} = E\left[ \frac{\text{False Rejections}}{\text{Total Rejections}} \right]$$
- **의미:** 기각한 것들 중에서 평균적으로 얼마나 틀렸는가.
- **예시:** $\text{FDR} = 0.01$이면 선택한 변수 중 평균적으로 1%만 틀리도록 보장한다.

---

## 10. FWER vs FDR 비교

| 구분 | FWER | FDR |
| :--- | :--- | :--- |
| **기준** | 하나라도 틀리면 실패 | 잘못 기각된 비율 허용 |
| **보수성** | 매우 큼 | 적절함 |
| **적합성** | 소규모 검정 | 대규모 데이터 (현대 통계) |

---

## 11. Benjamini–Hochberg (BH) Procedure

1. **Step 1:** 모든 변수에 대해 p-value 계산 ($p_1, p_2, \dots, p_m$)
2. **Step 2:** p-value 오름차순 정렬 ($p_{(1)} \le p_{(2)} \le \dots \le p_{(m)}$)
3. **Step 3:** FDR 수준 $\alpha$ 선택 (예: 0.05)
4. **Step 4:** 다음 임계값 계산: $\frac{i}{m} \alpha \quad (i = 1,2,\dots,m)$
5. **Step 5:** $p_{(i)} \le \frac{i}{m}\alpha$를 만족하는 **가장 큰 $i$**를 찾는다.
6. **Step 6:** $p_{(1)}, \dots, p_{(i)}$ 까지를 유의한 변수로 선택한다.



---

## 12. 해석
- threshold가 가설의 순서에 따라 자동으로 결정된다.
- 선택된 변수들은 평균적으로 $\alpha$ 비율만큼의 오류만 포함한다.

---

## 13. 현실 문제: 정규분포 가정
기존 p-value 계산은 정규분포, $t$-분포 등을 가정하지만 현실 데이터는 이상치가 많고 분포가 비정상적인 경우가 많다.

---

## 14. 해결책 — Permutation Test
> **분포를 가정하지 말고 데이터 자체로 p-value를 만들자**

---

## 15. Permutation 기반 p-value 절차

1. **Step 1:** 실제 관측된 통계량 계산 ($T_{obs}$)
2. **Step 2:** 라벨(정상/불량)을 무작위로 섞기(shuffle)
3. **Step 3:** 섞인 데이터에서 통계량 재계산 ($T^{(1)}, T^{(2)}, \dots, T^{(B)}$)
4. **Step 4:** p-value 계산: 
   $$p = \frac{\#\{T^{(b)} \ge T_{obs}\}}{B}$$



---

## 16. 전체 분석 프로세스
1. 각 변수별 통계량 계산
2. Permutation으로 변수별 p-value 산출
3. 수집된 모든 p-value에 BH(FDR) 적용
4. 임계치 이하의 **주요 인자 선택**

---

## 17. 정리
- **다중 가설 문제:** 검정 수 증가 시 오류 폭증 해결 필요
- **FDR:** 현대 데이터 분석에서 발견의 힘과 오류 제어 사이의 균형을 맞추는 강력한 도구
- **BH 알고리즘:** FDR을 제어하는 표준적인 방법
- **실무 조합:** **Permutation test + FDR**을 통해 분포 가정 없이 정교한 변수 선택 가능

✅ **센서 분석, 유전자 분석, 이상 탐지 등 현대 데이터 과학의 표준적인 방법론이다.**
