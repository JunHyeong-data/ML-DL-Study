# 📘 딥러닝과 신경망(Neural Network) 핵심 정리

---

## 1. 신경망과 비용 함수 개요
신경망(Neural Network)은 데이터 $(x, y)$가 주어졌을 때, 예측값과 실제값의 차이를 최소화하도록 가중치(Weight)를 학습하는 모델이다.
<img width="1481" height="1025" alt="image" src="https://github.com/user-attachments/assets/86f90747-64c4-40d0-ba55-bdd1587b678a" />

- **파라미터(Parameter):** 노드 사이 연결선의 가중치 ($w$)
- **손실 함수(Loss Function):** 하나의 데이터 포인트에서 발생하는 오차
- **비용 함수(Cost Function):** 전체 데이터셋의 손실을 합산하거나 평균낸 값
- **학습 목표:** $$\boxed{\arg\min_{w} \; \text{Cost}(w)}$$

---

## 2. 분류 문제에서의 비용 함수: 크로스 엔트로피
<img width="1369" height="1084" alt="image" src="https://github.com/user-attachments/assets/2a8bcdb5-b3bc-4f41-931f-4f0da82f82ec" />
<img width="1510" height="1061" alt="image" src="https://github.com/user-attachments/assets/45340dd3-b3c8-4561-bdf7-b24b2e2d716b" />

이진 분류 문제($y \in \{0, 1\}$)에서는 정답을 맞추는 확률에 비례해 패널티를 주는 **크로스 엔트로피(Cross Entropy)**를 사용한다.
$$L(y, \hat{y}) = - \left[ y \log(\hat{y}) + (1-y)\log(1-\hat{y}) \right]$$
- **직관적 의미:** 정답을 맞추면 손실은 0에 수렴하고, 틀린 예측을 할수록 손실은 기하급수적으로 커져 모델에 강한 패널티를 부여한다.



---

## 3. 다중 클래스 분류와 Softmax
<img width="1457" height="1018" alt="image" src="https://github.com/user-attachments/assets/a0fc4c6d-6e6d-4d00-bc0a-042bc887e2d7" />
<img width="1467" height="985" alt="image" src="https://github.com/user-attachments/assets/262b6a54-27ea-48e9-b05a-abf4e55023be" />

클래스가 3개 이상일 경우, 출력층에서 각 클래스에 속할 확률을 계산하기 위해 **Softmax** 함수를 사용한다.
$$\text{Softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}$$
- 출력값의 특징: 모든 클래스 확률의 합은 1이며, 가장 큰 확률을 가진 클래스를 최종 예측값으로 선택한다.

---

## 4. 신경망 학습 과정 (Backpropagation)
<img width="1529" height="1084" alt="image" src="https://github.com/user-attachments/assets/b23314db-dd53-4f59-8011-a195d101e0bf" />

신경망은 다음의 반복적인 과정을 통해 학습된다.
1. **가중치 초기화:** 랜덤한 값으로 가중치 설정
2. **순전파(Forward Propagation):** 데이터를 입력하여 예측값 계산
3. **비용 함수 계산:** 예측값과 정답의 차이 측정
4. **역전파(Backpropagation):** 출력층에서 입력층 방향으로 기울기(Gradient) 전달
5. **경사하강법 업데이트:** 가중치를 조정한다.
   $$w \leftarrow w - \alpha \frac{\partial L}{\partial w}$$
   ($\alpha$: 학습률, Learning Rate)



---

## 5. 배치 사이즈와 에포크
<img width="1509" height="1040" alt="image" src="https://github.com/user-attachments/assets/b7e9dd88-15de-4494-a804-2fadfc9626c6" />

- **배치 사이즈(Batch Size):** 가중치를 한 번 업데이트할 때 사용하는 데이터의 묶음 단위
- **에포크(Epoch):** 전체 학습 데이터를 한 번 모두 훑은 횟수
- **배치 사이즈 조절:** 작으면 학습은 세밀하지만 느리고, 크면 학습은 빠르지만 메모리 부담과 과적합 위험이 증가할 수 있다.

---

## 6. Early Stopping과 데이터 분할
<img width="1477" height="1031" alt="image" src="https://github.com/user-attachments/assets/bd6998c6-958a-4977-a762-813f9a4559ab" />

과적합을 방지하기 위해 데이터를 다음과 같이 나누어 관리한다.

| 구분 | 역할 |
| :--- | :--- |
| **Training** | 가중치(파라미터) 업데이트에 직접 사용 |
| **Validation** | 하이퍼파라미터 결정 및 과적합 감지 (조기 종료 판단) |
| **Test** | 학습에 전혀 관여하지 않은 최종 성능 평가용 |

- **Early Stopping:** Validation error가 줄어들다 다시 증가하기 시작하는 시점에 학습을 중단한다.

---

## 7. 딥러닝의 정의와 특징
<img width="1516" height="1061" alt="image" src="https://github.com/user-attachments/assets/3fe3bde2-812d-4ad5-be90-71717a1d9133" />
<img width="1443" height="1069" alt="image" src="https://github.com/user-attachments/assets/eac71228-1ffa-425e-a686-5c2b301f4a3e" />
<img width="1478" height="907" alt="image" src="https://github.com/user-attachments/assets/084416c6-81dc-4338-b059-167cf437a190" />
<img width="1492" height="908" alt="image" src="https://github.com/user-attachments/assets/b9e1376e-d1d4-4787-b5d3-85916ef88419" />
<img width="1470" height="1088" alt="image" src="https://github.com/user-attachments/assets/c16725aa-7450-47cc-9d68-afdeab16b5e3" />
<img width="1506" height="959" alt="image" src="https://github.com/user-attachments/assets/202269ce-6edf-43ef-bb5b-906b27565c3c" />

딥러닝은 은닉층(Hidden Layer)이 여러 개인 **심층 신경망(Deep Neural Network)**을 의미한다.
- **자동 특징 추출:** 사람이 직접 특징(Feature)을 정의하지 않아도, 모델이 초기 레이어(단순 패턴)부터 마지막 레이어(의미 있는 객체)까지 스스로 학습한다.

---

## 8. 기울기 소실 문제(Gradient Vanishing)
<img width="1513" height="1070" alt="image" src="https://github.com/user-attachments/assets/8f0872d7-a173-4ad8-8122-c7892047e147" />
과거 Sigmoid 활성화 함수를 여러 층 사용했을 때, 층이 깊어질수록 기울기가 0에 수렴하여 초기 층의 가중치가 학습되지 않는 현상이 발생했다.

### 해결책: ReLU(Rectified Linear Unit)
<img width="1493" height="1066" alt="image" src="https://github.com/user-attachments/assets/97dd747f-bd33-44a7-a56d-5e36a83e4494" />
<img width="1490" height="1078" alt="image" src="https://github.com/user-attachments/assets/ecc0e811-75d5-4bd9-b317-e650fd36c3ba" />

$$f(x) = \max(0, x)$$
- 양수 영역에서 기울기를 1로 유지하여 깊은 층에서도 학습 안정성을 확보한다.



---

## 9. 과적합(Overfitting) 해결 방법
<img width="1487" height="1059" alt="image" src="https://github.com/user-attachments/assets/5c7889fd-e3df-4d8a-b259-147ab4874072" />
<img width="1543" height="952" alt="image" src="https://github.com/user-attachments/assets/34c9c106-82ab-4d12-b2fa-8de2a137b0b9" />
<img width="1497" height="1013" alt="image" src="https://github.com/user-attachments/assets/c1397670-2472-4294-a4ae-779866973607" />
<img width="1519" height="997" alt="image" src="https://github.com/user-attachments/assets/3663c84c-b404-4a03-9426-d7ba867246db" />
<img width="1472" height="956" alt="image" src="https://github.com/user-attachments/assets/b53f5ed4-d9c7-4292-b3b7-5b90a242eb4f" />
<img width="1467" height="964" alt="image" src="https://github.com/user-attachments/assets/dcbc56b4-9927-4594-bc9e-c4676f691bcc" />
<img width="1491" height="789" alt="image" src="https://github.com/user-attachments/assets/9d520333-b7ca-4460-9ac4-6a8c306766fa" />
<img width="1493" height="966" alt="image" src="https://github.com/user-attachments/assets/c9a8cb96-2fc5-45a4-9fef-f5e77f14c10e" />

1. **Dropout:** 학습 중 일부 노드를 랜덤하게 비활성화하여 특정 노드에 대한 의존도를 낮추고 일반화 성능을 높인다.
2. **Data Augmentation:** 기존 데이터를 회전, 반전, 왜곡하여 인위적으로 데이터의 양을 늘려 다양한 패턴을 학습시킨다.

---
# CNN (Convolutional Neural Network) 완전 정리

※ 이미지 데이터 관점 → 왜 CNN이 필요한지 → 각 구성요소 → 수학적 의미

---

## 1️⃣ 이미지 데이터는 무엇인가 (가장 중요)
<img width="1301" height="1031" alt="image" src="https://github.com/user-attachments/assets/823bd39b-c5d2-4b5d-bb01-2ced200fdc96" />
<img width="1517" height="1034" alt="image" src="https://github.com/user-attachments/assets/04bd9c6f-768e-4607-a80e-1f9ad67f1679" />

### 1. 이미지 = 숫자의 집합
이미지는 픽셀(pixel)들의 집합이며, 각 픽셀은 숫자 값을 가진다.

- **(1) 흑백 이미지:** 픽셀 1개 = 1개의 값
  - 8bit → $0 \sim 255$
  - 16bit → $0 \sim 65,535$
- **(2) 컬러 이미지:** RGB 3개 채널 존재. 한 픽셀 = $(R, G, B)$
  - 이미지 크기: $H \times W \times 3$

📌 **예시:** $400 \times 700$ 컬러 이미지 $\rightarrow$ 변수 개수 = $400 \times 700 \times 3 = 840,000$개

---

## 2️⃣ 기존 신경망(Dense NN)의 치명적 문제
<img width="1466" height="1065" alt="image" src="https://github.com/user-attachments/assets/e830a15e-2a76-4f40-92c6-012ef7060baa" />

- **(1) 픽셀 하나 = 변수 하나:** 입력 노드만 84만 개. 은닉층 노드가 100개만 있어도 가중치 개수는 $840,000 \times 100 = 84,000,000$개에 달한다. 레이어 하나만 있어도 사실상 학습이 불가능하다.
- **(2) 이미지의 구조를 전혀 사용하지 못함:** Dense NN은 픽셀 위치 개념, 인접 픽셀 관계, 공간 정보가 없다. 이미지에는 매우 비효율적이다.

---

## 3️⃣ 기존 해결 방식: Feature Engineering
<img width="1462" height="975" alt="image" src="https://github.com/user-attachments/assets/2877af26-1509-4c41-b9d6-618a1a098c1d" />

- **대표 기법:** PCA, Feature Selection, 도메인 지식 기반 변수 축소
- **❌ 문제점:** 사람이 직접 설계해야 하며, 이미지에서는 의미 있는 feature 정의가 매우 어렵다.

---

## 4️⃣ CNN이 등장한 이유 (핵심 철학)
CNN은 다음 질문에서 출발함: 
> **“이미지의 구조적 특성을 모델이 자동으로 배우게 할 수 없을까?”**

---

## 5️⃣ 이미지 데이터의 핵심 특성 2가지
<img width="1488" height="1027" alt="image" src="https://github.com/user-attachments/assets/a7acc791-6591-422e-afb8-784f94925a4b" />
<img width="1495" height="1042" alt="image" src="https://github.com/user-attachments/assets/39434015-20b8-4950-a546-2e9485acc2cc" />

- **(1) Local Correlation (지역적 상관성):** 인접한 픽셀끼리는 강하게 연관됨. (예: 눈, 귀, 윤곽선, 텍스처)
  - 📌 멀리 떨어진 픽셀보다 가까운 픽셀이 더 중요함.
- **(2) Spatial Invariance (위치 불변성):** 특정 패턴(예: 고양이 귀)은 왼쪽 위에 있어도 귀고 오른쪽 아래에 있어도 귀다. 특정 위치에 의존하지 않고 패턴 자체가 중요하다.

---

## 6️⃣ Convolution (합성곱)
<img width="1484" height="1076" alt="image" src="https://github.com/user-attachments/assets/af4d4b1a-f263-4fdc-9f92-292836be1f01" />
<img width="1485" height="1016" alt="image" src="https://github.com/user-attachments/assets/61e33456-0471-4f7d-8ac5-8d0afc020a8b" />
<img width="1510" height="1097" alt="image" src="https://github.com/user-attachments/assets/b953a864-780a-4937-99d2-3b5e54fdcb86" />
<img width="1517" height="1049" alt="image" src="https://github.com/user-attachments/assets/8dcd0a36-f632-4608-a794-a7ba782b7193" />
<img width="1511" height="998" alt="image" src="https://github.com/user-attachments/assets/f7dcd3f1-e81e-4296-a1ff-ea8fa5b13e60" />
<img width="1510" height="1000" alt="image" src="https://github.com/user-attachments/assets/cbbdb89d-9d85-48ef-95a9-9773769b84c8" />
<img width="1524" height="1011" alt="image" src="https://github.com/user-attachments/assets/6a1c0f11-ffb7-4e72-a35b-548a9f695df1" />
<img width="1527" height="1086" alt="image" src="https://github.com/user-attachments/assets/5997d6df-aff6-45d7-87b6-43d3a5e04618" />
<img width="1504" height="1075" alt="image" src="https://github.com/user-attachments/assets/7ee2d2d7-ac6f-4d6d-8408-6ae684c545b8" />
<img width="1507" height="1127" alt="image" src="https://github.com/user-attachments/assets/ea6c3b4f-2d37-481b-b7cf-0636c5ee0336" />
<img width="1507" height="1111" alt="image" src="https://github.com/user-attachments/assets/e9450481-e9e4-463d-b795-8f53519f0675" />
<img width="1484" height="1044" alt="image" src="https://github.com/user-attachments/assets/42c4c2a8-a996-44f8-9bdf-e0f226565213" />
<img width="1478" height="1093" alt="image" src="https://github.com/user-attachments/assets/31eac88f-b50e-4595-ad31-50c5f6dab969" />

### 6-1. 필터(Filter) / 커널(Kernel)
이미지 위를 훑으며 연산하는 작은 행렬 (예: $3 \times 3, 5 \times 5$)

### 6-2. 합성곱 연산 방식 (정확한 정의)
필터와 이미지의 같은 위치 원소끼리 곱한 후 모두 더하여 하나의 숫자를 생성한다.
- **수식:** $$y = \sum_{i,j} (x_{i,j} \times w_{i,j})$$
👉 이 값이 **Feature Map**의 한 칸이 된다.



### 6-3. Feature Map
합성곱 결과로 나온 행렬이다.
- 하나의 필터 $\rightarrow$ 하나의 Feature Map
- 여러 필터 $\rightarrow$ 여러 Feature Map
📌 **Feature Map = 새롭게 생성된 특징들**

---

## 7️⃣ Weight Sharing (CNN의 핵심)

하나의 필터를 전체 이미지에 동일하게 사용한다.
- **장점:** 파라미터 수 급감, 위치 불변성 확보
- **Dense NN ❌ vs CNN ⭕:** CNN은 같은 패턴을 어디서든 인식 가능하다.

---

## 8️⃣ Stride

필터가 이동하는 간격이다.
- **Stride $\uparrow$:** 출력 크기 $\downarrow$, 연산량 $\downarrow$, 정보 손실 $\uparrow$
- **Stride $\downarrow$:** 출력 크자 $\uparrow$, 연산량 $\uparrow$, 정보 보존 $\uparrow$

---

## 9️⃣ Padding

왜 필요한가?
- **문제 1 (출력 크기 감소):** Convolution 할수록 이미지 크기가 감소한다.
- **문제 2 (가장자리 정보 손실):** 중앙 픽셀보다 가장자리 픽셀이 덜 사용된다.
- **해결 (Zero Padding):** 가장자리를 0으로 감싼다.
  - **효과:** 출력 크기 유지, 가장자리 정보 보존



---

## 🔟 Activation (ReLU)

- **정의:** $$ReLU(x) = \max(0, x)$$
- **왜 쓰는가?:** 비선형성 추가, Gradient Vanishing 완화, 계산 속도가 빠름
📌 **CNN에서는 ReLU가 사실상 표준이다.**

---

## 1️⃣1️⃣ Pooling

- **목적:** Feature Map 크기 축소, 계산량 감소, 과적합 방지
- **종류:**
  - **(1) Max Pooling:** 영역 내 최대값 선택 (가장 많이 사용됨)
  - **(2) Average Pooling:** 평균값 사용
📌 **Pooling에는 학습 파라미터가 없다.**



---

## 1️⃣2️⃣ CNN 전체 구조 (완전 흐름)

$$\text{Input Image} \rightarrow \text{Convolution} \rightarrow \text{ReLU} \rightarrow (\text{Padding / Stride}) \rightarrow \text{Pooling} \rightarrow (\text{반복}) \rightarrow \text{Flatten} \rightarrow \text{Fully Connected Layer} \rightarrow \text{Output}$$



---

## 1️⃣3️⃣ Flatten

2D Feature Map을 1D 벡터로 변환하여 Fully Connected Layer의 입력으로 사용할 수 있게 한다.

---

## 1️⃣4️⃣ Fully Connected Layer

우리가 알고 있는 일반 신경망으로, 최종 분류나 회귀를 수행한다.

---

## 1️⃣5️⃣ CNN 하이퍼파라미터 정리

- 필터 크기
- 필터 개수
- Stride
- Padding
- Pooling 크기
- Activation 함수

---

### ✅ 한 문장 요약 (진짜 중요)
**CNN은 이미지의 공간 구조를 유지하면서, 지역적 특징을 자동으로 추출하고, 위치에 상관없이 패턴을 인식하도록 설계된 신경망이다.**

## ✅ 최종 요약
딥러닝은 **Deep Layer**를 통해 데이터의 고차원 특징을 스스로 학습하며, **ReLU, Dropout, Early Stopping** 등의 기법을 통해 학습의 효율성과 일반화 성능을 극대화하는 모델이다.
<img width="1543" height="945" alt="image" src="https://github.com/user-attachments/assets/0d21d698-be5f-4559-948f-36e0a3b15cb4" />
<img width="1481" height="1074" alt="image" src="https://github.com/user-attachments/assets/1defb711-165c-4b59-b459-685716a2f94f" />
<img width="1509" height="1071" alt="image" src="https://github.com/user-attachments/assets/37ac70f1-3bb5-4c1b-b79a-a190a24af670" />
<img width="1516" height="1093" alt="image" src="https://github.com/user-attachments/assets/38da33d5-f994-4b67-b992-98180fcc1c17" />
<img width="1472" height="1032" alt="image" src="https://github.com/user-attachments/assets/e89b19d1-aff2-4df4-bc93-e7c3206013c1" />
<img width="1515" height="996" alt="image" src="https://github.com/user-attachments/assets/4f0ffffe-b4c2-417b-98e4-0a30f9b142ac" />
<img width="1472" height="925" alt="image" src="https://github.com/user-attachments/assets/c8fc245f-3d69-47fb-99ce-be63f5b51560" />
<img width="1281" height="757" alt="image" src="https://github.com/user-attachments/assets/203e1815-bcf4-4b8d-8a01-98dbac98621b" />
<img width="1496" height="1017" alt="image" src="https://github.com/user-attachments/assets/6ad02e0c-5ad8-42ea-970d-4a1a0041eeb4" />
<img width="1455" height="1114" alt="image" src="https://github.com/user-attachments/assets/c67f5a8f-d976-4705-8b86-7e7115b6d4f8" />
<img width="1486" height="1005" alt="image" src="https://github.com/user-attachments/assets/7e48800a-5a26-4596-a307-d1849f10c0a6" />
<img width="1520" height="1051" alt="image" src="https://github.com/user-attachments/assets/46566d3e-a79e-4291-b516-3dd935842e36" />
