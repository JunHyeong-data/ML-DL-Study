# 인공신경망(Neural Network) 모델

## 1. 강의 개요
인공신경망(Neural Network) 강의는 크게 두 부분으로 구성된다.

### 모델의 배경과 구조
- 선형 회귀
- 로지스틱 회귀
- 퍼셉트론
- 다층 퍼셉트론과 인공신경망

### 모델의 파라미터와 학습 방법
- 파라미터의 종류
- 비용 함수(Cost Function)
- 경사하강법(Gradient Descent)

이번 강의에서는 인공신경망 모델의 구조적 배경과 핵심 개념을 중심으로 설명한다.

---

## 2. 선형 회귀 모델 복습
선형 회귀 모델은 입력 변수 $x$들의 선형 결합으로 출력 변수 $y$를 표현한다.

$$
y = \beta_0 + \beta_1 x_1 + \cdots + \beta_p x_p
$$

**특징:**
- 출력 변수 $y$는 연속형
- 입력과 출력의 관계는 직선(혹은 초평면)
- 입력 변수가 하나일 경우, $x$와 $y$의 관계는 직선으로 표현된다.

---

## 3. 로지스틱 회귀 모델
로지스틱 회귀는 출력 변수 $y$가 **범주형(0 또는 1)**일 때 사용하는 모델이다.

### 3.1 모델 구조
1. 입력 변수들의 선형 결합
2. 로지스틱 함수(sigmoid)를 통한 비선형 변환

$$
\pi(x) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 x)}}
$$

**특징:**
- 출력값은 확률 $(0, 1)$
- S자 형태의 곡선
- 입력 변수가 2개 이상이면 S자 형태의 **면(surface)**이 형성됨

---

## 4. 퍼셉트론(Perceptron)

### 4.1 단층 퍼셉트론
<img width="1317" height="1051" alt="image" src="https://github.com/user-attachments/assets/940b24ef-5bad-4916-8d37-e999bc9dac79" />

퍼셉트론은 인공신경망의 가장 초기 형태로, 로지스틱 회귀보다 더 단순한 구조를 가진다.

**구조:**
- 입력 변수의 선형 결합
- 임계값(threshold)을 기준으로 이진 분류

$$
y =
\begin{cases} 
1 & \text{if } w_0 + w_1 x_1 + w_2 x_2 > 0 \\
0 & \text{otherwise}
\end{cases}
$$

- **활성화 함수:** 계단 함수(step function)
- **출력:** 0 또는 1

### 4.2 퍼셉트론의 한계
<img width="1505" height="1081" alt="image" src="https://github.com/user-attachments/assets/f8549c1d-9a87-4d47-93df-cde3e606d764" />

단층 퍼셉트론은 AND, OR와 같은 논리 연산은 표현할 수 있으나, **XOR 문제**는 해결할 수 없다.

**이유:**
- 단층 퍼셉트론은 **선형 분리(linear separable)**만 가능
- XOR 데이터는 어떤 직선으로도 완벽히 분리 불가능

---

## 5. 다층 퍼셉트론(Multi-Layer Perceptron)
<img width="1484" height="1091" alt="image" src="https://github.com/user-attachments/assets/2f1473e8-4d1c-45b9-8eac-f1cb2fd4bedf" />

XOR 문제를 해결하기 위해 등장한 모델이 다층 퍼셉트론이다.



### 5.1 구조
<img width="1491" height="1020" alt="image" src="https://github.com/user-attachments/assets/bb58e605-31fb-48a8-8803-fae60ed0f8e1" />

- **입력층(Input Layer)**
- **은닉층(Hidden Layer)**
- **출력층(Output Layer)**
은닉층이 1개 이상 존재하면 이를 다층 퍼셉트론이라 한다.

### 5.2 은닉층의 역할
은닉층에서는 입력 변수의 선형 결합 후 활성화 함수를 통한 비선형 변환이 수행된다.

$$
h_j = \sigma\left(\sum_i w_{ij} x_i\right)
$$

이렇게 생성된 $h_1, h_2, \dots$가 다시 출력층의 입력으로 사용되어 복잡한 패턴 학습이 가능해진다.

---

## 6. 인공신경망(Neural Network)
은닉층이 하나 이상인 다층 퍼셉트론을 일반적으로 **인공신경망(Neural Network)**이라고 부른다.

### 6.1 용어 정리
- **Input Layer:** 입력 변수 개수 = 입력 노드 개수
- **Hidden Layer:** 사용자가 직접 설계
- **Output Layer:** - 분류 문제: 클래스 수만큼 노드
    - 회귀 문제: 출력 변수 개수만큼 노드

---

## 7. 활성화 함수 (Activation Function)
활성화 함수는 선형 결합 결과를 비선형으로 변환한다.

### 7.1 시그모이드 함수
$$
\sigma(z) = \frac{1}{1 + e^{-z}}
$$

**특징:**
- 출력 범위: $(0, 1)$
- 단조 증가 함수
- 미분 형태가 단순함: $\sigma'(z) = \sigma(z)(1 - \sigma(z))$

### 7.2 기타 활성화 함수
- Tanh, ReLU, Leaky ReLU, ELU 등 (이번 강의는 시그모이드 중심)

---

## 8. 인공신경망의 파라미터
- **가중치(weight):** $w$
- **편향(bias)**
이들은 노드 간 연결의 강도를 의미한다.

---

## 9. 비용 함수 (Cost Function)

<img width="1482" height="992" alt="image" src="https://github.com/user-attachments/assets/a3b0637e-6004-4095-8aad-134779ed483b" />

### 9.1 회귀 문제
$$
\text{MSE} = \frac{1}{n} \sum (y - \hat{y})^2
$$

### 9.2 분류 문제
$$
\text{Cross-Entropy} = -\sum y \log(\hat{y})
$$
(로그우도 최대화 ↔ 크로스 엔트로피 최소화)

---

## 10. 경사하강법 (Gradient Descent)
비용 함수를 최소화하기 위한 최적화 알고리즘이다.

<img width="1490" height="1096" alt="image" src="https://github.com/user-attachments/assets/8d01e203-308b-4809-8ffc-3870d356339c" />

### 10.1 기본 아이디어
<img width="1300" height="1074" alt="image" src="https://github.com/user-attachments/assets/3cd4484c-4fb4-4eff-83cf-ad5930e4c60c" />

<img width="1333" height="1006" alt="image" src="https://github.com/user-attachments/assets/83b62094-4e8c-4f6d-a982-eb9a8f2de801" />

기울기(gradient)를 계산하여 반대 방향으로 파라미터를 업데이트한다.

$$
w^{(t+1)} = w^{(t)} - \alpha \frac{\partial L}{\partial w}
$$

- $\alpha$: 학습률(learning rate)

### 10.2 학습률의 역할
- $\alpha$가 크면: 빠르지만 불안정
- $\alpha$가 작으면: 안정적이지만 느림

---

## 11. 정리
- 단층 퍼셉트론은 XOR 문제 해결 불가 $\rightarrow$ 은닉층 도입으로 해결
- 인공신경망은 로지스틱 회귀의 확장
- 파라미터 학습의 핵심은 비용 함수와 경사하강법
- 다음 강의: **역전파 알고리즘(Backpropagation)**
