# 주성분 분석 (Principal Component Analysis, PCA)

## 1. 차원 축소의 개요
<img width="1351" height="913" alt="image" src="https://github.com/user-attachments/assets/9353ee98-92f3-4b45-b8f8-8b928940c290" />

고차원 데이터란 변수($x$)의 개수가 굉장히 많은 데이터를 의미한다. 최근 제조업, 의료 분야 등에서 고성능 데이터 수집 기기의 등장으로 고차원 데이터가 도처에서 생성되고 있다.

### 1.1 고차원 데이터의 문제점
- 수학적으로 표현하고 시각화하기 어려움.
- 계산 복잡도가 증가하여 모델링 효율성이 저하됨.
- 분석 목적과 무관한 불필요한 변수가 포함될 가능성이 큼.

### 1.2 차원 축소(Dimension Reduction)의 두 가지 방법
<img width="1476" height="1092" alt="image" src="https://github.com/user-attachments/assets/4c4e232c-5c77-475e-9a56-5a3be9a2cf89" />

| 구분 | 변수 선택 (Selection) | 변수 추출 (Extraction) |
| :--- | :--- | :--- |
| **정의** | 원래 변수 중 중요한 $x$들만 골라냄 | 기존 변수들의 결합/변환을 통해 새로운 변수 생성 |
| **장점** | 원래 변수를 유지하므로 해석이 용이함 | 변수 간 상관관계를 잘 고려하며 압축률이 높음 |
| **단점** | 변수 간 상관관계를 직접 고려하지 못함 | 추출된 변수의 의미를 직관적으로 해석하기 어려움 |

---

## 2. 차원 축소 방법론의 분류
타깃 변수($y$) 유무에 따라 4가지 경우로 나뉜다.
<img width="1491" height="633" alt="image" src="https://github.com/user-attachments/assets/a0e9570d-88a0-413e-9fef-f28cdd81b810" />

1. **지도 변수 선택 (Supervised Feature Selection):** $y$를 이용해 중요한 $x$ 선택 (예: Information Gain, Lasso 등)
2. **지도 변수 추출 (Supervised Feature Extraction):** $y$를 이용해 새로운 변수 추출 (예: PLS - Partial Least Squares)
3. **비지도 변수 선택 (Unsupervised Feature Selection):** $y$ 없이 $x$ 간의 상관관계만으로 변수 선택 (예: PC Loading 이용)
4. **비지도 변수 추출 (Unsupervised Feature Extraction):** $y$ 없이 $x$의 결합으로 변수 추출 (**대표 예: PCA**)

---

## 3. 주성분 분석(PCA) 정의 및 목적
<img width="1373" height="347" alt="image" src="https://github.com/user-attachments/assets/30b9b890-41a2-4ce8-8eee-b65310e1f8af" />

### 3.1 PCA의 정의
$n$개의 관측치와 $p$개의 변수로 구성된 원래 데이터를, 상관관계가 없는 $k$($k < p$)개의 변수로 요약하는 분석 기법이다. 여기서 요약된 변수는 기존 변수들의 **선형 결합(Linear Combination)**으로 생성된다.
<img width="1465" height="950" alt="image" src="https://github.com/user-attachments/assets/782b20f1-e59e-4823-b2d0-3360c63aab80" />

### 3.2 주요 목적
- **데이터 정보 보존:** 원래 데이터의 분산(Variance)을 최대한 보존하는 새로운 축을 찾는다.
- **차원 축소:** 변수 개수를 줄여 데이터 시각화 및 해석 효율을 높인다.
- **전처리 단계:** 모델링(분류, 회귀 등) 전 초기 단계에서 데이터의 패턴을 파악하기 위해 사용된다.

---

## 4. PCA의 수리적 개념

### 4.1 주성분($Z$)의 생성
<img width="1422" height="1020" alt="image" src="https://github.com/user-attachments/assets/141ed833-a52a-4069-8f61-43ad6175a269" />
<img width="1509" height="907" alt="image" src="https://github.com/user-attachments/assets/df572dd6-048c-453e-80db-3ad792029f89" />
<img width="1424" height="894" alt="image" src="https://github.com/user-attachments/assets/578f9ea4-cb2a-457d-b24a-712c70259e3e" />

새로운 변수 $Z$는 원래 변수 $x$들의 선형 결합이다.
$$Z_i = \alpha_{i1}X_1 + \alpha_{i2}X_2 + \dots + \alpha_{ip}X_p$$
- **로딩(Loading) / 기저(Basis):** 가중치 계수인 $\alpha$를 의미한다.
- 모든 $X$ 변수가 주성분 생성에 참여한다는 점이 중요하다.

### 4.2 분산 최대화 (Variance Maximization)
<img width="728" height="732" alt="image" src="https://github.com/user-attachments/assets/826ce818-e662-4ef9-82bd-0d306beff40f" />

PCA가 찾는 '좋은 축'이란, 데이터를 그 축으로 **사영(Projection)**시켰을 때 사영된 데이터들의 분산이 가장 큰 축을 의미한다. 분산이 크다는 것은 원래 데이터가 가진 정보(흩어짐)를 가장 잘 보존하고 있다는 뜻이다.



---

## 5. 수리적 배경 지식
<img width="1466" height="1079" alt="image" src="https://github.com/user-attachments/assets/0455dee5-e3d5-4a54-b0a5-41319cfaf235" />

### 5.1 공분산 행렬 (Covariance Matrix, $\Sigma$)
<img width="1513" height="976" alt="image" src="https://github.com/user-attachments/assets/11093852-ce23-4ddd-8968-7f4de0332f48" />
<img width="1442" height="933" alt="image" src="https://github.com/user-attachments/assets/ba13c926-4ed3-4fbd-9c02-1797c5c0bded" />

변수들 간의 상관관계 정보를 요약한 $p \times p$ 행렬이다. 대각 원소는 각 변수의 분산을, 비대각 원소는 변수 간의 공분산을 담고 있다.

### 5.2 고유값과 고유벡터 (Eigenvalue & Eigenvector)
<img width="1495" height="1055" alt="image" src="https://github.com/user-attachments/assets/06d01d98-491c-4605-93ec-d2885f8c5ce5" />
행렬의 특징을 나타내는 고유한 값과 벡터이다. 공분산 행렬 $\Sigma$에 대해 다음을 만족한다.
$$\Sigma \mathbf{v} = \lambda \mathbf{v}$$
- **$\lambda$ (고유값):** 주성분이 설명하는 분산의 크기.
- **$\mathbf{v}$ (고유벡터):** 주성분의 방향(로딩 계수).

---

## 6. PCA 알고리즘 및 최적화 과정
<img width="1505" height="1028" alt="image" src="https://github.com/user-attachments/assets/14ec7691-a4b6-49a5-bb0e-88886fe54360" />
<img width="1484" height="1049" alt="image" src="https://github.com/user-attachments/assets/0847a348-2760-4606-8690-ebb5e43d70b3" />

### 6.1 데이터 전처리
- **Centering:** 각 변수의 평균을 구해 모든 데이터에서 빼준다. (평균을 0으로 맞춤)

### 6.2 최적화 문제 설정
주성분 $Z = \alpha^T \mathbf{x}$의 분산을 최대화하는 $\alpha$를 찾는다.

**목적 함수:**
$$\max_{\alpha} \text{Var}(Z) = \max_{\alpha} \alpha^T \Sigma \alpha$$

**제약 조건:**
$$\|\alpha\|^2 = \alpha^T \alpha = 1 \text{ (계수 벡터의 길이를 1로 제한)}$$

### 6.3 문제 풀이 결과
라그랑주 승수법을 통해 위 문제를 풀면, 공분산 행렬의 **고유값 문제**로 귀결된다.
1. **첫 번째 주성분($Z_1$):** 가장 큰 고유값 $\lambda_1$에 대응하는 고유벡터 $\alpha_1$을 가중치로 사용한다.
2. **두 번째 주성분($Z_2$):** 두 번째로 큰 고유값 $\lambda_2$에 대응하는 고유벡터 $\alpha_2$를 사용한다. (이때 $Z_2$는 $Z_1$과 직교한다.)



---

## 7. 정리
- **비지도 학습 기법:** 타깃 변수 $y$ 없이 오직 $X$ 간의 상관관계만 이용한다.
- **정보 보존:** 분산을 최대화하는 축을 찾아 데이터 손실을 최소화한다.
- **독립성:** 추출된 각 주성분($Z_1, Z_2, \dots, Z_k$)은 서로 상관관계가 0이다.

---

# 주성분 분석 (Principal Component Analysis, PCA) – 실전 데이터 적용 및 해석

## 1. 데이터 전처리: Centering
PCA를 수행하기 전, 각 변수의 평균을 0으로 맞추는 **Centering** 과정이 필요하다. 
<img width="1508" height="1048" alt="image" src="https://github.com/user-attachments/assets/926cc59c-6bbf-408c-8e85-079f12512229" />
<img width="1427" height="885" alt="image" src="https://github.com/user-attachments/assets/30bcaeb5-db7b-4cab-9895-f0cab49da334" />

- **방법:** 각 변수($X_1, X_2, X_3$)의 평균을 구해 모든 데이터에서 빼준다.
- **결과:** 모든 변수의 평균은 0이 된다.
- **의미:** 데이터의 원래 형태(분포)는 그대로 유지하면서 축의 위치만 원점으로 이동시키는 것이며, 데이터의 오리지널 성격은 바뀌지 않는다.



---

## 2. 주성분($Z$)의 도출 과정 (3변수 예시)
<img width="1459" height="758" alt="image" src="https://github.com/user-attachments/assets/08a53c9c-6440-4dc4-acc4-e16dcca8069c" />
<img width="1512" height="1087" alt="image" src="https://github.com/user-attachments/assets/2d4bd120-c611-4a2f-981e-0ccab15906e6" />
<img width="1413" height="926" alt="image" src="https://github.com/user-attachments/assets/e010156b-912b-474e-af46-c16f12669e9e" />

변수가 3개인 데이터의 상관관계 행렬(Correlation Matrix)은 $3 \times 3$ 행렬이 된다.

### 2.1 고유값과 고유벡터 추출
$3 \times 3$ 행렬로부터 3개의 **고유값($\lambda$, Eigenvalue)**과 각각에 대응하는 **고유벡터(Eigenvector)**를 얻는다.
- 고유값($\lambda$): 주성분 축이 담고 있는 분산의 크기
- 고유벡터: 주성분을 만들기 위한 선형 결합의 계수($\alpha$)

### 2.2 주성분($Z$) 수식
- **제1주성분($Z_1$):** 가장 큰 고유값($\lambda_1$)에 대응하는 고유벡터 값을 계수로 사용
- **제2주성분($Z_2$):** 두 번째로 큰 고유값($\lambda_2$)에 대응하는 고유벡터 값을 계수로 사용
- **제3주성분($Z_3$):** 세 번째로 큰 고유값($\lambda_3$)에 대응하는 고유벡터 값을 계수로 사용

$$Z_1 = 0.669X_1 + 0.589X_2 + 0.453X_3 \quad (\text{예시 계수})$$

---

## 3. 추출된 주성분의 특성: 독립성
새롭게 추출된 변수들($Z_1, Z_2, Z_3$)의 공분산을 구해보면 대각 원소(분산)를 제외한 **비대각 원소는 모두 0**이 된다.
- **의미:** 새롭게 추출된 주성분들은 서로 **통계적으로 독립(Orthogonal)**이다.
- 이는 원래 변수들($X$) 사이의 상관관계를 제거하고 정보를 요약했음을 뜻한다.

---

## 4. 차원 축소의 기준: 얼마나 많은 주성분을 쓸 것인가?
<img width="1382" height="824" alt="image" src="https://github.com/user-attachments/assets/9c851258-911f-45e7-9548-26cefe4ff755" />
<img width="1483" height="865" alt="image" src="https://github.com/user-attachments/assets/124e6554-7109-4a41-8799-a06dcb5f0033" />
이론적으로는 원래 변수 개수만큼 주성분을 만들 수 있지만, 주성분 분석의 목적은 차원 축소에 있다.

### 4.1 고유값($\lambda$)과 분산의 관계
- 각 주성분($Z_i$)의 분산은 해당 축의 고유값($\lambda_i$)과 동일하다.
- **$\lambda$가 클수록** 해당 주성분이 전체 데이터의 정보를 많이 담고 있다는 뜻이다.

### 4.2 주성분 선택 방법 (Scree Plot)
<img width="1507" height="1011" alt="image" src="https://github.com/user-attachments/assets/27e1bbfb-3965-4c78-a9a0-90865668808a" />

1. **분산 비율 확인:** 첫 번째 주성분이 전체 분산의 몇 %를 차지하는지 확인한다. (예: 92%를 설명한다면 하나만 써도 충분함)
2. **엘보우 포인트 (Elbow Point):** 고유값 감소율이 급격히 낮아지는 '팔꿈치' 지점을 선택한다.
3. **누적 분산:** 누적 설명력이 일정 수준(예: 70~80%) 이상이 될 때까지의 주성분을 선택한다.



---

## 5. 로딩 플롯 (Loading Plot) 해석
주성분을 구성하는 계수($\alpha$) 값들을 시각화한 것이다.
<img width="1527" height="1025" alt="image" src="https://github.com/user-attachments/assets/fe85066d-452a-4a0f-8f68-3553e04ddf88" />

- **해석:** 특정 변수가 특정 주성분(PC1 등)을 구성하는 데 얼마나 큰 역할을 했는지 알 수 있다.
- **방향:** 변수 값이 특정 축 방향으로 크게 치우쳐 있다면, 그 주성분은 해당 변수의 성격을 강하게 반영하고 있는 것이다.



---

## 6. PCA의 한계점
<img width="1469" height="1025" alt="image" src="https://github.com/user-attachments/assets/b10bd5d9-18b5-4acd-88fa-309532190623" />
<img width="1485" height="1098" alt="image" src="https://github.com/user-attachments/assets/6d143f08-53b3-45c1-9de3-067a3dc53b0a" />

1. **분포의 제약:** 데이터가 가우시안(정규분포) 형태가 아닐 경우 적용이 어렵다. (해결책: Kernel PCA 등)
2. **비지도 학습의 한계:** 오직 분산이 최대화되는 방향으로만 축을 찾으므로, 분류(Classification)나 예측(Regression) 성능을 직접적으로 보장하지는 않는다.

---

## 7. 알고리즘 요약
<img width="1324" height="1078" alt="image" src="https://github.com/user-attachments/assets/3f1785e6-e156-4db3-9b91-a5d6060d9779" />

1. 데이터 Centering 및 정규화 수행
2. 상관관계 또는 공분산 행렬 계산
3. 고유값($\lambda$) 및 고유벡터 계산
4. 고유값이 큰 순서대로 주성분 축($Z$) 생성
5. 선택된 주성분 축으로 데이터를 투영(Projection)하여 차원 축소 완료

<img width="1479" height="1107" alt="image" src="https://github.com/user-attachments/assets/a3888161-7174-4c25-aa3e-7d13a85345fc" />
<img width="1374" height="1033" alt="image" src="https://github.com/user-attachments/assets/34036efd-5401-4c54-83fb-1ef52ec70dd9" />
<img width="1267" height="664" alt="image" src="https://github.com/user-attachments/assets/9a3e95ac-0d45-4200-b046-745fa857ff75" />
<img width="1318" height="957" alt="image" src="https://github.com/user-attachments/assets/1f4abb2c-634b-4229-9b3a-50af880b0874" />
<img width="1324" height="1102" alt="image" src="https://github.com/user-attachments/assets/888f6340-e362-41a6-8aac-df4ffc7c14d4" />
<img width="1430" height="1103" alt="image" src="https://github.com/user-attachments/assets/fdfbf5ff-91c5-4b6f-b042-2d9dec7bcd5b" />


