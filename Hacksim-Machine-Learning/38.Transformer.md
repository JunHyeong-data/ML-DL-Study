# Transformer 완전 정리 (Self-Attention 중심)

이 문서는 2017년 논문 **"Attention Is All You Need"**를 기반으로, 트랜스포머(Transformer)의 구조와 핵심 개념을 **자연어 처리 예제(“나는 학생이다”)**를 통해 단계별로 정리한 Markdown 문서입니다.

---

## 1. Transformer 개요
<img width="1484" height="1038" alt="image" src="https://github.com/user-attachments/assets/339d6687-23ca-4581-a231-2904de98ebd7" />

- **제안 논문:** Attention Is All You Need (Vaswani et al., 2017)
- **핵심 아이디어:** RNN/CNN 없이 Attention만으로 시퀀스 모델링 수행
- **기본 구조:** Encoder – Decoder
- **현재 활용 분야:** NLP, Vision Transformer(ViT), Multimodal, LLM 전반
- **핵심 연산:** Self-Attention

---

## 2. Attention vs Self-Attention
<img width="1495" height="989" alt="image" src="https://github.com/user-attachments/assets/245a50ea-9763-4431-b27d-8cc6d85fc202" />

### 2.1 Attention (전통적 Attention)
- Encoder–Decoder 사이의 관계를 계산한다.
- 디코더의 현재 단어가 인코더의 어떤 단어를 참고해야 하는지 결정한다.

### 2.2 Self-Attention
- 같은 문장(Encoder 내부 또는 Decoder 내부) 내에서 단어 간 관계를 계산한다.
- 예: “나는” ↔ “학생”, “학생” ↔ “이다”
- 문장 내부의 모든 단어 쌍 간 관계를 동시에 계산한다.

---

## 3. Transformer 전체 구조

<img width="1476" height="1052" alt="image" src="https://github.com/user-attachments/assets/cd1188ae-9b78-421f-869c-49165d51e305" />
<img width="1449" height="892" alt="image" src="https://github.com/user-attachments/assets/455e162c-5d81-4fc9-b1a6-0deebf4742a8" />
<img width="1512" height="916" alt="image" src="https://github.com/user-attachments/assets/d36a27ed-c439-474c-b2bc-cd0913890ceb" />
<img width="1497" height="1023" alt="image" src="https://github.com/user-attachments/assets/6d469dac-2569-4d83-a5c9-f7d9f6049765" />

### 3.1 Encoder
- 동일한 블록을 여러 번 반복한다 (원 논문: 6개).
- **각 Encoder 블록 구성:**
  - Multi-Head Self-Attention
  - Feed Forward Network (FFN)

### 3.2 Decoder
- **각 Decoder 블록 구성:**
  - Masked Multi-Head Self-Attention
  - Encoder–Decoder Attention
  - Feed Forward Network

### 3.3 Transformer에 존재하는 3가지 Attention
1. Encoder Self-Attention
2. Decoder Masked Self-Attention
3. Encoder–Decoder Attention

---

## 4. 입력(Input) 구성 과정
<img width="1508" height="994" alt="image" src="https://github.com/user-attachments/assets/c59f8975-dd84-4d6e-8be7-30b62ef86e3a" />
<img width="1503" height="1034" alt="image" src="https://github.com/user-attachments/assets/befbfd98-65e8-4ca5-8e96-f9938d262bd7" />
<img width="1527" height="1080" alt="image" src="https://github.com/user-attachments/assets/e65787b0-340d-458c-b9d0-02d20f963d96" />
<img width="1516" height="1057" alt="image" src="https://github.com/user-attachments/assets/f1c3d740-1f4a-4065-a1c5-a9bfcae1d95d" />
<img width="1522" height="1093" alt="image" src="https://github.com/user-attachments/assets/2914e384-21cd-4366-98bf-9c192bea9131" />

### 4.1 자연어 → 토큰(Token)
- 예시 문장: “나는 학생이다”
- 토큰화 결과: `[나는, 학생, 이다]`

### 4.2 Word Embedding
- 각 토큰을 고정 차원 벡터로 변환한다. ($d_{model} = 4$ 가정 시)

| 토큰 | 임베딩 벡터 |
| :--- | :--- |
| **나는** | $(x_1, x_2, x_3, x_4)$ |
| **학생** | $(x_1, x_2, x_3, x_4)$ |
| **이다** | $(x_1, x_2, x_3, x_4)$ |

### 4.3 Positional Encoding (위치 정보)
Transformer는 병렬 구조로 인해 순서 개념이 없으므로 위치 정보를 반드시 추가해야 한다. 사인·코사인 함수를 기반으로 생성한다.

- **공식:**
  $$PE(pos, 2i) = \sin\left(\frac{pos}{10000^{2i/d_{model}}}\right)$$
  $$PE(pos, 2i+1) = \cos\left(\frac{pos}{10000^{2i/d_{model}}}\right)$$
- **특징:** 값 범위 $[-1, 1]$, 가까운 위치일수록 높은 유사도를 가진다.



### 4.4 최종 입력 벡터
$$\text{Input Embedding} = \text{Word Embedding} + \text{Positional Encoding}$$
이 결과값이 Encoder의 첫 번째 입력이 된다.

---

## 5. Self-Attention 메커니즘 (핵심)
<img width="1538" height="840" alt="image" src="https://github.com/user-attachments/assets/12d5dfce-fb8d-4ab9-9038-4f93c69c5c98" />
<img width="1528" height="1032" alt="image" src="https://github.com/user-attachments/assets/a8e7af22-4364-4e66-a839-b26470c7f256" />
<img width="1527" height="1097" alt="image" src="https://github.com/user-attachments/assets/f07397e1-689c-48da-8cda-0dddc5b4b75d" />
<img width="1519" height="1049" alt="image" src="https://github.com/user-attachments/assets/c2d53518-5df8-4e33-a90e-11b4562c5aea" />
<img width="1521" height="1061" alt="image" src="https://github.com/user-attachments/assets/46665bcd-48f8-4472-bdee-f8acb5ee42a8" />

### 5.1 Query, Key, Value
각 단어 임베딩 벡터 $X$에 대해 가중치 행렬 $W$를 곱해 생성한다.
- **Query:** $Q = X \cdot W_Q$ (질문하는 주체)
- **Key:** $K = X \cdot W_K$ (대조되는 대상)
- **Value:** $V = X \cdot W_V$ (실제 정보 값)

### 5.2 Attention Score 계산
현재 단어($Q$)와 모든 단어($K$)의 유사도를 계산한다.
$$\text{Score} = \frac{Q \cdot K^T}{\sqrt{d_k}}$$
이후 Softmax를 적용하여 가중치를 얻는다.
$$\text{Attention Weight} = \text{softmax}(\text{Score})$$

### 5.3 Value 가중합
$$Z = \sum (\text{Attention Weight} \times V)$$
$Z$는 문맥 정보가 반영된 해당 단어의 새로운 특징 벡터가 된다.



---

## 6. 예제: “나는 학생이다”

- **"나는" 기준 Self-Attention 결과 (가정):**
  - 나는–나는: 0.952
  - 나는–학생: 0.047
  - 나는–이다: 0.001
- **결과:** 자기 자신 및 의미적으로 밀접한 단어에 집중하게 된다.

---

## 7. Multi-Head Attention

### 7.1 개념
여러 개의 $W_Q, W_K, W_V$ 세트를 사용하여 여러 관점에서 문장을 동시에 바라본다.

### 7.2 CNN과의 유사성 비교
| 항목 | CNN | Transformer |
| :--- | :--- | :--- |
| **구성 요소** | 여러 필터 | 여러 Head |
| **목적** | 다양한 공간 특징 추출 | 다양한 문맥 관계 학습 |

### 7.3 처리 흐름
1. 각 Head별 Self-Attention 계산
2. 결과값 Concatenate (병합)
3. Linear Projection을 통해 최종 차원 조정



---

## 8. Encoder 블록 정리
1. Multi-Head Self-Attention
2. Add & Layer Normalization (잔차 연결 및 정규화)
3. Feed Forward Network
4. Add & Layer Normalization

---

## 9. 핵심 요약
- **Transformer**는 RNN/CNN 없이 **Attention만으로 시퀀스를 모델링**한다.
- 순서 정보는 **Positional Encoding**으로 보완한다.
- **Self-Attention**은 문장 내 단어 간 관계를 병렬로 계산한다.

---
# Transformer 강의 정리 (Encoder–Decoder · Multi-Head · Masked Attention)

---

## 1. 왜 Multi-Head Attention이 필요한가?
<img width="1522" height="1100" alt="image" src="https://github.com/user-attachments/assets/2e295afe-2bee-4a36-98b0-bc3469be4805" />
<img width="1494" height="1034" alt="image" src="https://github.com/user-attachments/assets/590d3775-2ad0-4f64-bbfb-e249c0a88dd0" />

### 1.1 코끼리 비유 (관점의 다양성)
큰 코끼리 그림을 한 번에 보여주면 사람마다 코, 귀, 상아, 다리 등 집중하는 곳이 다르다. 
👉 **하나의 대상도 관점에 따라 다르게 해석될 수 있다.**

### 1.2 딥러닝 관점에서의 의미
입력 정보(이미지, 텍스트)를 하나의 관점으로만 해석하면 표현의 한계가 존재한다. 여러 관점으로 동시에 분해해서 보면 더 풍부한 특징 추출이 가능하다. 💡 **이것이 바로 Multi-Head Attention의 본질이다.**

### 1.3 CNN과의 대응 관계
| 항목 | CNN | Transformer |
| :--- | :--- | :--- |
| **구성 요소** | 여러 개의 필터 | 여러 개의 Attention Head |
| **역할** | 각 필터가 다른 공간 특징 추출 | 각 Head가 다른 문맥 관계 학습 |

---

## 2. Multi-Head Attention의 수학적 구조

### 2.1 입력 차원 예시
- 입력 시퀀스 길이: 3 / 임베딩 차원($d_{model}$): 4
- 👉 입력 행렬 크기: $3 \times 4$

### 2.2 Head 분할
각 Head는 독립적인 가중치 행렬을 사용하여 $Z_0, Z_1, \dots, Z_7$ 등의 출력을 생성한다. 각 Head의 차원은 $d_{model} / h$로 줄어들어 연산 효율을 높인다.

### 2.3 Output Projection ($W_O$)
모든 Head의 결과를 Concat(병합)한 뒤, 원래 입력 차원으로 복원하기 위해 선형 변환을 수행한다.
$$(\text{Concat Result}) \times W_O = \text{Final Output}$$
이때 사용하는 가중치 $W_O$ 역시 학습을 통해 결정된다.



---

## 3. Encoder 전체 흐름 요약
<img width="1524" height="992" alt="image" src="https://github.com/user-attachments/assets/f610aa22-0ee5-4202-ac8c-566752f5221a" />
<img width="1530" height="1053" alt="image" src="https://github.com/user-attachments/assets/aee9d255-6f88-4024-9b4b-de405de07ee9" />
<img width="1504" height="1121" alt="image" src="https://github.com/user-attachments/assets/442b2e00-1492-4156-b57a-0325217f044a" />

### 3.1 입력 단계
$$\text{Input} = \text{Word Embedding} + \text{Positional Encoding}$$

### 3.2 Self-Attention 핵심 연산
각 입력 벡터 $X$에 대해 Query($Q$), Key($K$), Value($V$)를 생성한다.
$$Q = XW_Q, \quad K = XW_K, \quad V = XW_V$$
$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

### 3.3 Residual & Normalization
Encoder 블록 내부에서는 정보 손실을 막기 위해 **Residual Connection(잔차 연결)**과 **Layer Normalization**을 수행한다.
$$\text{LayerNorm}(X + \text{Sublayer}(X))$$

### 3.4 Feed Forward Network (FFN) 의미
Attention으로 관계를 학습한 후, 비선형 변환을 통해 특징을 재조합한다.
$$\text{FFN}(x) = \max(0, xW_1 + b_1)W_2 + b_2$$

---

## 4. Self-Attention 직관 예시
<img width="1474" height="1080" alt="image" src="https://github.com/user-attachments/assets/f19f44ac-5fd1-43cf-9ce2-d24f540e8421" />

- **문장:** "The animal was too tired to cross the street"
- **결과:** 모델은 "it"이라는 단어가 나왔을 때, 이 단어가 "animal"이나 "tired"와 강하게 연결되어 있음을 스스로 찾아낸다. 즉, 대명사가 무엇을 가리키는지 문맥적으로 파악한다.

---

## 5. Decoder 구조 개요
<img width="1477" height="999" alt="image" src="https://github.com/user-attachments/assets/96178983-fdec-4d03-bdcc-4db143e0bb43" />

Decoder의 핵심은 **미래 정보를 차단**하고 **Encoder의 정보를 참조**하는 것이다.
- **키워드:** Masked Self-Attention, Encoder–Decoder Attention

---

## 6. Masked Self-Attention
<img width="1517" height="969" alt="image" src="https://github.com/user-attachments/assets/1021c921-409b-4e25-b9df-5930b3a124d7" />
<img width="1530" height="1020" alt="image" src="https://github.com/user-attachments/assets/105fd630-c329-4e7e-89e5-17f4945d0493" />
<img width="1526" height="1071" alt="image" src="https://github.com/user-attachments/assets/f30aebb5-94c7-4d80-8098-87032e2fe5d0" />
<img width="1522" height="982" alt="image" src="https://github.com/user-attachments/assets/6c3b030f-c054-4b66-9e05-9cb0a4a52bfa" />
<img width="1530" height="977" alt="image" src="https://github.com/user-attachments/assets/356723ee-dcc9-4e86-adaf-deb89c30dbe8" />

### 6.1 왜 Mask가 필요한가?
번역 시 "I am a student"를 생성할 때, "I"를 예측하는 시점에서 뒤에 나올 "am, a, student"는 아직 알 수 없는 미래 정보이다. 이를 학습 시 미리 보게 되면 컨닝(Cheating)이 되어 제대로 된 학습이 불가능하다.

### 6.2 Masking 방식
Attention Score 행렬에서 미래 위치의 값을 $-\infty$로 설정하여 Softmax 통과 후 가중치를 0으로 만든다.



---

## 7. Encoder–Decoder Attention

### 7.1 핵심 아이디어
- **Query:** Decoder에서 생성 (현재 번역 중인 단어 정보)
- **Key, Value:** Encoder 출력 사용 (원본 문장 정보)
👉 **의미 정렬(Alignment)을 학습**하는 단계이다.

---

## 8. Decoder 전체 흐름
1. **Masked Self-Attention** (자신이 생성한 이전 단어들 참조)
2. **Encoder–Decoder Attention** (원본 문장 참조)
3. **Feed Forward Network** (특징 재조합)
4. (각 단계 사이에는 Add & LayerNorm 포함)



---

## 9. 최종 예측 단계
$$\text{Linear Projection} \rightarrow \text{Softmax} \rightarrow \text{Next Token Probability}$$
예를 들어, "I am"까지 생성되었다면 다음 단어로 "a"가 나올 확률을 계산한다.

---

## 10. Transformer vs RNN

| 항목 | RNN | Transformer |
| :--- | :--- | :--- |
| **처리 방식** | 순차적 (Sequential) | 병렬적 (Parallel) |
| **장기 의존성** | 거리에 따라 정보 소실 | Attention으로 직접 연결 |
| **학습 속도** | 느림 | 매우 빠름 (GPU 최적화) |

---

## 11. 최종 요약
<img width="1521" height="979" alt="image" src="https://github.com/user-attachments/assets/37445995-519f-4ac7-a613-23cbcfab34da" />
<img width="1523" height="1007" alt="image" src="https://github.com/user-attachments/assets/72e589b3-b129-4bde-82ed-1e54f0cfc3bc" />
<img width="1491" height="1065" alt="image" src="https://github.com/user-attachments/assets/72bd8473-c21a-4818-ac56-9a3152936f94" />

- **Attention만으로** 시퀀스 모델링을 완성한 혁신적인 구조이다.
- **Positional Encoding**으로 데이터의 순서 정보를 복원한다.
- 엄청난 수의 파라미터를 가진 **Heavy Model**이지만, 그만큼 높은 표현력을 가진다.

✅ **NLP를 넘어 비전, 시계열, 이상 탐지 등 전 분야의 표준 모델로 자리 잡았다.**
