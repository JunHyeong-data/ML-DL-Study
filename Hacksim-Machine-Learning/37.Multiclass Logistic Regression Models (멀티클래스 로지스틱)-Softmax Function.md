# 📘 Multiclass Logistic Regression (Softmax) 아주 자세한 정리

---

## 1. 오늘 강의의 목표

- **이진 로지스틱 회귀(Binary Logistic Regression)** 복습
- 클래스가 **3개 이상(K개)** 일 때 각 클래스에 속할 **확률을 어떻게 구하는지**
- **Softmax 함수가 어떻게 유도되는지** 이해하기

> ⚠️ 오늘은 학습 방법이나 손실함수는 다루지 않고 **확률식 유도에만 집중**한다.

---

## 2. 이진 로지스틱 회귀 복습
<img width="1461" height="918" alt="image" src="https://github.com/user-attachments/assets/0f2d6acd-e579-4ad2-a91b-0acb8fd28ed8" />

### (1) 클래스가 2개인 경우
- 클래스: $Y \in \{0, 1\}$
- 확률의 관계: $P(Y=0|x) = 1 - P(Y=1|x)$

### (2) 로지스틱 함수
클래스 1에 속할 확률을 다음과 같이 정의한다.
$$P(Y=1|x) = \frac{1}{1 + e^{-x^T\theta}}$$
- 출력값은 항상 **0과 1 사이**이므로 **확률로 해석 가능**하다.



---

## 3. Odds (오즈) 개념 복습
<img width="1469" height="883" alt="image" src="https://github.com/user-attachments/assets/e9cd7a6b-6915-42cb-a44d-d8067642ee67" />
<img width="1407" height="1036" alt="image" src="https://github.com/user-attachments/assets/b2f88d9e-ca78-4668-89f3-21eac10d800e" />

### (1) Odds란?
성공 확률을 $p$, 실패 확률을 $1-p$라고 할 때:
$$\text{Odds} = \frac{p}{1-p}$$

### (2) 로지스틱 회귀에서 Odds
$$\text{Odds} = \frac{P(Y=1|x)}{P(Y=0|x)}$$
이를 로지스틱 함수에 대입하면:
$$\frac{P(Y=1|x)}{1-P(Y=1|x)} = e^{x^T\theta}$$

📌 **중요한 결과:** Odds의 로그는 선형 함수이며, 이것이 로지스틱 회귀의 핵심 성질이다.

---

## 4. 이제 멀티클래스로 확장해보자
<img width="1500" height="961" alt="image" src="https://github.com/user-attachments/assets/406c8bad-55eb-4c2d-87ab-6c7146c2a5a6" />
<img width="1488" height="981" alt="image" src="https://github.com/user-attachments/assets/dba48585-1dfc-4511-bd60-a407aeefc73a" />

### (1) 문제 설정
- 클래스 개수: $K$
- 클래스 집합: $Y \in \{1, 2, ..., K\}$

### (2) 기준 클래스 설정
식을 간단하게 만들기 위해 **클래스 $K$를 기준(Reference Class)**으로 설정한다.

---

## 5. 멀티클래스 Odds 정의

### (1) 클래스 $j$와 기준 클래스 $K$의 Odds
$$\frac{P(Y=j|x)}{P(Y=K|x)} = e^{x^T\theta_j} \quad (j = 1, 2, \dots, K-1)$$

### (2) 기준 클래스의 경우
$$\frac{P(Y=K|x)}{P(Y=K|x)} = 1 \implies e^{x^T\theta_K} = 1 \implies \theta_K = 0$$

---

## 6. 모든 Odds를 더해보자

위의 식들을 모두 더하면 다음과 같다.
$$\sum_{j=1}^{K-1} \frac{P(Y=j|x)}{P(Y=K|x)} + 1 = \sum_{j=1}^{K-1} e^{x^T\theta_j} + 1$$

### (1) 좌변 정리
$$\frac{P(Y=1|x) + \dots + P(Y=K|x)}{P(Y=K|x)}$$
확률의 합은 항상 1이므로:
$$\frac{1}{P(Y=K|x)}$$

### (2) 결과 정리
$$\frac{1}{P(Y=K|x)} = 1 + \sum_{j=1}^{K-1} e^{x^T\theta_j}$$

---

## 7. 기준 클래스($K$)의 확률

위 식을 뒤집으면 **기준 클래스의 확률**을 얻는다.
$$P(Y=K|x) = \frac{1}{1 + \sum_{j=1}^{K-1} e^{x^T\theta_j}}$$

---

## 8. 일반 클래스 $j$의 확률 구하기

Odds 정의를 다시 사용하여 대입하면:
$$P(Y=j|x) = P(Y=K|x) \cdot e^{x^T\theta_j}$$
$$P(Y=j|x) = \frac{e^{x^T\theta_j}}{1 + \sum_{l=1}^{K-1} e^{x^T\theta_l}} \quad (j=1, \dots, K-1)$$

---

## 9. 모든 클래스를 포함한 최종 형태

기준 클래스($e^{x^T\theta_K} = 1$)를 포함하여 정리하면 최종 확률식이 완성된다.
$$P(Y=j|x) = \frac{e^{x^T\theta_j}}{\sum_{l=1}^{K} e^{x^T\theta_l}} \quad (j=1, \dots, K)$$

---

## 10. 이것이 바로 Softmax 함수

### Softmax 정의
$$\text{Softmax}(z_j) = \frac{e^{z_j}}{\sum_{l=1}^{K} e^{z_l}}$$
여기서 $z_j = x^T\theta_j$는 각 클래스에 대한 선형 결합 점수(Logit)이다.



---

## 11. Softmax의 성질

- 모든 출력값은 **0~1 사이**이다.
- 모든 클래스 확률의 합은 항상 **1**이다.
- 가장 큰 점수($x^T\theta_j$)를 가진 클래스가 예측 클래스가 된다.

---

## 12. 핵심 요약 (시험 직전용)

1. 이진 로지스틱의 **Odds** 개념을 확장한다.
2. **기준 클래스**를 설정하여 상대적 확률 비를 정의한다.
3. 확률의 총합이 1이라는 성질을 이용해 식을 정리한다.
4. 모든 클래스에 대해 일반화하면 **Softmax 함수**가 유도된다.

---

## ✨ 한 줄 요약
> **멀티클래스 로지스틱 회귀에서 각 클래스 확률은 선형 결합 점수의 지수 승 비중인 Softmax 함수로 계산된다.**
