# 랜덤 포레스트(Random Forest) 모델 정리

## 1. 단일 의사결정나무 모델의 한계
<img width="1425" height="1018" alt="image" src="https://github.com/user-attachments/assets/05c50284-08b6-45e5-86bb-1d5ab79f5a12" />

앞서 우리는 단일 의사결정나무(Decision Tree) 모델에 대해 살펴보았다. 의사결정나무는 계층적 구조를 가지는 모델로, 상위 노드에서의 판단이 하위 노드로 그대로 전달된다. 이로 인해 중간 단계에서 발생한 오류가 이후 단계로 계속 전파되는 문제가 존재한다.

또한 의사결정나무는 다음과 같은 단점을 가진다.
- **민감성:** 학습 데이터의 작은 변동이나 소수의 노이즈 데이터에도 최종 결과가 크게 달라질 수 있다.
- **과적합(Overfitting):** 트리의 깊이를 깊게 하거나 리프 노드 수를 늘리면 학습 데이터에 대한 오류는 거의 0이 되지만, 새로운 테스트 데이터에 대해서는 예측 성능을 보장할 수 없다.
- **상태:** 이를 **Bias는 낮고 Variance는 높은 상태**라고 하며, 일반화 성능이 떨어지는 위험한 모델 상태라고 볼 수 있다.

이 문제를 해결하기 위한 대표적인 방법이 바로 **랜덤 포레스트(Random Forest)** 모델이다.

---

## 2. 랜덤 포레스트의 배경: 앙상블(Ensemble) 학습
<img width="1459" height="1024" alt="image" src="https://github.com/user-attachments/assets/73350b54-5248-49ff-b1d0-849427fda117" />
<img width="1490" height="1070" alt="image" src="https://github.com/user-attachments/assets/8ef48478-6580-4f8a-a515-cc2d63ce5d6c" />

랜덤 포레스트는 앙상블(Ensemble) 모델의 한 종류이다. 앙상블이란 여러 개의 모델(**베이스 모델**)을 결합하여 하나의 더 강력한 모델을 만드는 방법이다.

- **핵심 아이디어:** 하나의 모델에 의존하지 않고, 여러 모델의 예측 결과를 다수결 또는 평균을 통해 통합함으로써 예측 성능을 향상시킨다.
- **효과:** 단일 모델의 불안정한 예측을 안정적이고 강건(Robust)하게 만든다.

---

## 3. 앙상블 모델이 잘 동작하기 위한 조건
앙상블 모델이 효과를 가지기 위해서는 다음 두 가지 조건이 중요하다.

### (1) 베이스 모델 간의 독립성 (Diversity)
- 개별 모델들이 서로 최대한 다른 오류를 내야 한다. 모두 비슷한 판단을 한다면 앙상블 효과는 거의 없다.

### (2) 베이스 모델의 최소 성능 보장
- 베이스 모델은 무작위 예측(Random Guess)보다는 반드시 좋아야 한다. 
- 이진 분류 문제에서 무작위 예측의 오류율은 $0.5$이다. 따라서 각 모델의 오류율은 **$0.5$보다 작아야** 앙상블 효과가 나타난다.

---

## 4. 왜 의사결정나무를 베이스 모델로 쓰는가?
<img width="1478" height="988" alt="image" src="https://github.com/user-attachments/assets/4b05548f-ee84-4242-9839-203a1fb0105f" />

의사결정나무는 앙상블 모델의 베이스 모델로 매우 적합하다.
- 모델 구축 속도가 빠르다.
- 데이터 분포에 대한 확률적 가정이 필요 없는 비모수(Non-parametric) 모델이다.
- 다양한 형태의 데이터를 잘 처리할 수 있다.

이러한 이유로 의사결정나무를 베이스 모델로 사용하는 앙상블 모델을 **랜덤 포레스트(Random Forest)**라고 부른다.

---

## 5. 랜덤 포레스트의 전체 구조 개요
<img width="1480" height="1061" alt="image" src="https://github.com/user-attachments/assets/86cbd01e-63ef-4921-83c5-36b418eec26c" />

랜덤 포레스트의 전체적인 흐름은 다음과 같다.

1. **부트스트랩(Bootstrap)** 샘플링을 통해 여러 개의 서로 다른 학습 데이터 생성
2. 각 학습 데이터로부터 개별 **의사결정나무**를 학습
3. 여러 개의 트리로부터 나온 예측 결과를 **결합(Aggregation)**
4. 최종 예측 결과 도출



---

## 6. 랜덤 포레스트의 핵심 아이디어 2가지
<img width="1427" height="877" alt="image" src="https://github.com/user-attachments/assets/9e248040-9614-4e99-9e07-baacff678ed1" />

랜덤 포레스트의 핵심은 다음 두 가지 키워드로 요약된다.

### (1) Bagging (Bootstrap Aggregating)
- 서로 다른 학습 데이터를 만들어 여러 모델을 학습시킴으로써 모델의 **분산(Variance)을 감소**시키는 효과가 있다.

### (2) Random Subspace
- 각 의사결정나무를 학습할 때 모든 변수를 사용하지 않고 **일부 변수만 무작위로 선택**하여 베이스 모델 간의 **독립성(Diversity)**을 높인다.

👉 **반드시 기억해야 할 키워드: Bagging + Random Subspace**

---

## 7. 부트스트랩(Bootstrap) 샘플링
<img width="1501" height="1096" alt="image" src="https://github.com/user-attachments/assets/49183a40-44a6-4f49-892a-00c54e8596d7" />

부트스트랩은 복원 추출(with replacement)을 사용하는 샘플링 기법이다.

- 원래 데이터 개수가 $N$개라면 항상 $N$개를 다시 샘플링한다.
- 같은 데이터가 여러 번 선택될 수 있고, 어떤 데이터는 한 번도 선택되지 않을 수 있다.

**중요한 성질:**
<img width="1286" height="585" alt="image" src="https://github.com/user-attachments/assets/0101a51f-d02c-452c-867e-741316400912" />

이론적으로 하나의 데이터가 한 번도 선택되지 않을 확률은 다음과 같다.
$$\lim_{N \to \infty} \left( 1 - \frac{1}{N} \right)^N = \frac{1}{e} \approx 0.368$$
즉, 하나의 부트스트랩 샘플에는 원본 데이터의 **약 63.2%**만 서로 다른 데이터로 포함된다. 이때 선택되지 않은 36.8%의 데이터를 **OOB(Out-Of-Bag)** 데이터라고 부르며, 이를 통해 모델의 성능을 검증할 수 있다.



---

## 8. 예측 결과를 결합하는 방법 (Aggregation)
### (1) 다수결 투표 (Majority Voting)
<img width="1422" height="963" alt="image" src="https://github.com/user-attachments/assets/9b52ffb4-34f7-47af-a312-16ed633b6bfc" />

- 분류 문제에서 가장 일반적인 방법으로, 가장 많은 모델이 선택한 클래스를 최종 결과로 채택한다.

### (2) 가중 다수결 (Weighted Voting)
<img width="1528" height="966" alt="image" src="https://github.com/user-attachments/assets/084f94b8-e158-4fc2-9cdc-3f0e406f96a1" />

- 각 모델의 성능에 따라 가중치를 부여하여 성능이 좋은 모델의 의견을 더 크게 반영한다.

### (3) 확률 평균 (Probability Averaging)
<img width="1483" height="1084" alt="image" src="https://github.com/user-attachments/assets/f7504d77-a573-4085-8f1f-634fd48e5bc3" />

- 각 모델이 예측한 클래스 확률을 평균 내어 평균 확률이 가장 큰 클래스를 선택한다.

---

## 9. 정리
- 단일 의사결정나무의 과적합 문제를 해결한다.
- **분산 감소**를 통해 일반화 성능을 향상시킨다.
- 노이즈에 강건하며 실무 활용도가 매우 높다.

결론적으로, 랜덤 포레스트는 **“여러 개의 랜덤한 의사결정나무를 결합한 강력한 앙상블 모델”**이다.

# 랜덤 포레스트(Random Forest) – Random Subspace & 변수 중요도

## 1. Random Subspace 개념 (변수 무작위 선택)
랜덤 포레스트에서 개별 의사결정나무를 생성할 때, 모든 변수를 사용하여 분할을 수행하지 않는다. 일반적인 의사결정나무는 각 분기 노드에서 모든 변수와 가능한 분할점을 탐색하여 비용 함수(예: Gini, Entropy)를 최소화하는 최적의 분할을 선택하지만, 랜덤 포레스트에서는 이 과정을 수정한다.

---

## 2. 분기 노드에서의 변수 선택 방식
각 분기 노드(split node)에서 다음 과정을 수행한다.

1. 전체 변수 집합이 $p$개라고 할 때, 그중 $m \ll p$ 인 일부 변수만 무작위로 선택한다.
   <img width="1475" height="1052" alt="image" src="https://github.com/user-attachments/assets/0f25b0df-cd31-4ee1-8c3b-e5d5b05e0fc5" />

2. 선택된 변수들에 대해서만 최적의 분할 변수와 분할점을 탐색한다.
  <img width="1460" height="1043" alt="image" src="https://github.com/user-attachments/assets/18085594-dfb5-47d2-930b-2eaf25088753" />
 
3. 그중 가장 성능이 좋은 분할을 선택한다.
  <img width="1510" height="1052" alt="image" src="https://github.com/user-attachments/assets/a557d4de-03f8-4d77-8b61-b54c74204182" />

**예시:**
전체 변수가 16개라면 각 노드마다 4개 변수만 무작위로 선택하여 최적의 분할을 수행한다. 이 과정은 모든 분기 노드에서 독립적으로 반복되므로, 부모 노드와 자식 노드에서 선택되는 변수 집합은 서로 다를 수 있다.



---

## 3. Random Subspace의 핵심 의미
<img width="1494" height="1049" alt="image" src="https://github.com/user-attachments/assets/3d1573c2-d1bc-46a1-a3dd-e4bd342bbe05" />

- 모든 변수를 사용하지 않고 일부 변수만 무작위로 선택하여 분할함으로써 각 트리가 서로 다른 구조를 가지도록 유도한다.
- 이를 **Random Subspace Method**라고 부르며, 랜덤 포레스트에서 **트리 간 다양성(Diversity)**을 확보하는 핵심 메커니즘이다.

---

## 4. Bagging + Random Subspace = Random Forest
랜덤 포레스트는 다음 두 가지를 결합한 모델이다.

1. **Bagging (Bootstrap Aggregating):** 복원 추출을 통해 여러 개의 학습 데이터를 생성하고, 각 데이터로부터 독립적인 의사결정나무를 학습한다.
2. **Random Subspace:** 각 분기 노드에서 일부 변수만 무작위로 선택하여 트리 간 상관관계를 감소시킨다.

이 두 과정을 통해 랜덤 포레스트는 **Variance는 줄이고, 일반화 성능은 높이는** 모델이 된다.

---

## 5. 랜덤 포레스트의 일반화 오차 (Generalization Error)
<img width="1494" height="1020" alt="image" src="https://github.com/user-attachments/assets/701bc5ed-04a5-4f09-be47-7c64746dd321" />

이론적으로 랜덤 포레스트의 일반화 오차는 다음 두 요소에 의해 상한(upper bound)이 결정된다.
1. **트리 간 평균 상관계수 ↓ $\rightarrow$ 오차 ↓**
2. **개별 트리의 정확도 ↑ $\rightarrow$ 오차 ↓**

즉, 트리들이 서로 독립적일수록, 그리고 각 트리가 최소한 무작위 예측보다 성능이 좋을수록 랜덤 포레스트는 매우 안정적인 예측 성능을 보인다.

---

## 6. 랜덤 포레스트의 또 다른 강점: 변수 중요도(Variable Importance)
<img width="1507" height="706" alt="image" src="https://github.com/user-attachments/assets/eda3493b-6c21-43ad-a4dc-7d179f4df99c" />

랜덤 포레스트는 데이터 기반의 실용적인 변수 중요도 측정 방법을 제공한다. 이는 특정 변수가 모델의 예측 성능에 얼마나 기여하는지 정량적으로 평가할 수 있게 해준다.

---

## 7. OOB(Out-of-Bag) 데이터
부트스트랩 샘플링 과정에서 하나의 트리를 학습할 때 사용되지 않은 데이터가 반드시 존재하는데, 이를 **OOB(Out-of-Bag) 데이터**라고 한다.
- OOB 데이터는 해당 트리에 대해 **테스트 데이터** 역할을 수행한다.
- 별도의 검증 데이터셋 없이도 모델의 성능 평가를 가능하게 한다.

---

## 8. Permutation 기반 변수 중요도 계산
<img width="1506" height="1092" alt="image" src="https://github.com/user-attachments/assets/9d5e6b11-8dd8-43b1-b6dc-f7e5970712fc" />

특정 변수 $X_i$의 중요도를 계산하는 과정은 다음과 같다.

1. **Step 1:** 각 트리에 대해 OOB 데이터로 기준 오류율 $R^{(1)}$ 계산
2. **Step 2 (Permutation):** 변수 $X_i$의 값만 무작위로 섞음 (다른 변수는 유지)
3. **Step 3:** 섞인 데이터로 다시 OOB 오류율 $R^{(2)}$ 계산
4. **Step 4 (오류 증가량):** $D_i = | R^{(2)} - R^{(1)} |$
5. **Step 5 (평균):** $D_i$를 모든 트리에 대해 평균낸다. 값이 클수록 해당 변수는 중요하다.



---

## 9. 변수 중요도의 해석
- **오류가 크게 증가:** 모델이 해당 변수에 크게 의존하고 있음을 의미 (중요 변수)
- **오류 증가가 거의 없음:** 모델 예측에 기여도가 낮음을 의미 (중요하지 않은 변수)

---

## 10. 랜덤 포레스트의 하이퍼파라미터
<img width="1459" height="1105" alt="image" src="https://github.com/user-attachments/assets/09deae15-a2d1-4726-9eb2-29ef2efb0512" />

1. **트리 개수 (Number of Trees):** 보통 200개 이상 사용하며, 많을수록 안정적이지만 계산 비용이 증가한다.
2. **분기 시 선택할 변수 개수 ($m$):**
   - 분류 문제: $m \approx \sqrt{p}$
   - 회귀 문제: $m \approx \frac{p}{3}$

---

## 11. 랜덤 포레스트 알고리즘 요약
1. 부트스트랩 샘플을 $B$개 생성한다.
2. 각 샘플로 의사결정나무를 학습하되, 각 노드에서 $m$개의 변수를 무작위 선택한다.
3. 모든 트리 학습이 완료되면 결과를 결합한다.
   - **회귀:** 평균(Averaging)
   - **분류:** 다수결(Voting) 또는 확률 평균

---

## 12. 핵심 요약
- **랜덤 포레스트 = Bagging + Random Subspace**
- **목적:** 트리 간 다양성 확보 및 상관관계 감소
- **결과:** 과적합 감소, 일반화 성능 향상, 변수 중요도 산출 가능
