# 서포트 벡터 머신(Support Vector Machine, SVM)

## 1. 강의 개요
<img width="1507" height="890" alt="image" src="https://github.com/user-attachments/assets/b1d25ac6-4002-44b5-aff2-66236de18e28" />

본 강의는 서포트 벡터 머신(Support Vector Machine, SVM) 모델을 이해하기 위한 이론 중심의 강의이다. 전체 내용은 다음 두 부분으로 구성된다.

1. 선형으로 분리 가능한 경우 (Linear SVM)
2. 선형으로 분리 불가능한 경우 (Non-linear SVM)

이번 문서에서는 첫 번째 주제인 선형 SVM을 중심으로 다음 내용을 체계적으로 정리한다.

- 분류 문제의 기본 개념
- 일반화 성능(Generalization)
- 마진(Margin)의 정의
- 최적 초평면(Hyperplane)
- 최적화 문제로서의 SVM
- 라그랑주 승수법
- 듀얼 문제(Dual problem)
- 서포트 벡터(Support Vector)
- 최종 분류 함수

---

## 2. 분류 문제(Classification Problem)
SVM은 지도학습(supervised learning) 모델이며, 그중에서도 이진 분류(binary classification) 문제를 다룬다.
<img width="1142" height="796" alt="image" src="https://github.com/user-attachments/assets/a4e79e43-9191-444a-844c-c60ca3b4ca95" />

### 데이터 구성
훈련 데이터는 다음과 같이 주어진다.
$$\{(x_i, y_i)\}_{i=1}^n$$
- $x_i \in \mathbb{R}^d$ : 입력 벡터 (특징 벡터)
- $y_i \in \{+1, -1\}$ : 클래스 레이블

즉, 클래스 $+1$과 클래스 $-1$ 두 개의 범주 중 하나로 데이터를 분류하는 것이 목적이다.

---

## 3. 일반화 성능(Generalization Ability)
<img width="1037" height="725" alt="image" src="https://github.com/user-attachments/assets/1a3902b9-e4e5-42b5-a1ba-952a984773ad" />

머신러닝 모델은 다음 두 성질을 동시에 만족해야 한다.
1. 훈련 데이터에 대한 성능이 좋아야 함
2. 보지 못한 새로운 데이터에서도 성능이 좋아야 함

하지만 이 두 조건은 항상 상충(trade-off) 관계에 있다.
- 훈련 오차를 지나치게 줄이면 → 과적합(overfitting)
- 모델을 너무 단순하게 만들면 → 과소적합(underfitting)

### 일반화 능력
훈련 데이터뿐 아니라 미래 데이터에서도 좋은 성능을 보이는 능력을 **Generalization Ability**라고 한다. SVM은 다음과 같은 중요한 가설에 기반한다.
> **훈련 데이터의 마진을 최대화하면 일반화 오차가 최소화된다.**

이 이론은 통계적 학습 이론(Statistical Learning Theory)에 근거한다.

---

## 4. 초평면(Hyperplane)
<img width="1393" height="962" alt="image" src="https://github.com/user-attachments/assets/9f58fe25-8306-4797-8235-08db768bd953" />

### 정의
입력 공간에서 두 클래스를 구분하는 결정 경계(decision boundary)를 **초평면(hyperplane)**이라고 한다.
$$w^T x + b = 0$$
- $w$ : 가중치 벡터 (법선 벡터)
- $b$ : 바이어스(bias)

### 차원에 따른 의미
| 차원 | 형태 |
| :--- | :--- |
| 2차원 | 직선 |
| 3차원 | 평면 |
| 4차원 이상 | 초평면 |

---

## 5. 문제점: 초평면은 무한히 많다
두 클래스를 완벽히 분리하는 초평면은 무한히 존재한다. 따라서 다음 질문이 생긴다.
> **어떤 초평면이 가장 좋은가?**

이를 위해 SVM은 명확한 기준을 도입한다.

---

## 6. 마진(Margin)의 개념
<img width="1403" height="854" alt="image" src="https://github.com/user-attachments/assets/cdb55066-4e5f-4692-bd16-58ed050207e2" />

### 마진이란?
- 두 클래스에서 가장 가까운 데이터와 결정 경계 사이의 거리
- 보다 정확히 말하면, **클래스 $+1$에서 가장 가까운 점**과 **클래스 $-1$에서 가장 가까운 점** 사이의 거리이다.

### 마진을 정의하는 세 개의 평면
$$
\begin{aligned}
w^T x + b &= +1 \\
w^T x + b &= 0 \\
w^T x + b &= -1
\end{aligned}
$$
- 가운데: 결정 초평면
- 위·아래: 마진 경계

<img width="1509" height="1088" alt="image" src="https://github.com/user-attachments/assets/309fc9fe-45f7-4c77-8a69-8f653f080a4d" />

이때 마진의 크기는 다음과 같다.
$$\text{Margin} = \frac{2}{\|w\|}$$
즉, **$\|w\|$가 작을수록 마진은 커진다.**

---

## 7. SVM의 핵심 아이디어
<img width="1443" height="739" alt="image" src="https://github.com/user-attachments/assets/21f06d89-b58b-417c-9aea-01fa24f2a2aa" />

마진을 최대화하는 초평면을 선택하자. 이는 다음 문제와 동치이다.
> **$\|w\|$를 최소화하자.**

---

## 8. 벡터 노름(Vector Norm)

### L2 노름
$$\|w\|_2 = \sqrt{w_1^2 + w_2^2 + \cdots + w_d^2}$$
보통 SVM에서는 L2 노름을 사용한다.

---

## 9. 최적화 문제(Formulation)
<img width="1507" height="1048" alt="image" src="https://github.com/user-attachments/assets/d5e3fc76-74c8-4d10-aa4a-55bee448c14d" />

### 목적 함수
$$\min_{w,b} \quad \frac{1}{2} \|w\|^2$$

### 제약 조건
모든 데이터가 마진 바깥에 위치해야 한다.
$$y_i (w^T x_i + b) \ge 1 \quad \forall i$$

---

## 10. 최종 프라이멀 문제 (Primal Problem)
<img width="1129" height="785" alt="image" src="https://github.com/user-attachments/assets/fa8fff2c-190c-47a6-87c5-221ae8e8099c" />


$$
\begin{aligned}
\min_{w,b} \quad & \frac{1}{2} \|w\|^2 \\
\text{subject to} \quad & y_i(w^T x_i + b) \ge 1
\end{aligned}
$$

이 문제는 다음 성질을 갖는다.
- 목적 함수: 이차식(quadratic)
- 제약 조건: 선형(linear)
- → **이차 계획 문제(QP, Quadratic Programming)**

또한 Convex optimization이므로 **전역 최적해(global optimum)가 유일하게 존재한다.**

---

## 11. 라그랑주 승수법
<img width="1290" height="876" alt="image" src="https://github.com/user-attachments/assets/f9f13a45-8559-4a6c-8630-26755a566030" />

제약 조건을 포함한 문제를 하나의 함수로 만든다.
$$L(w,b,\alpha) = \frac{1}{2}\|w\|^2 - \sum_i \alpha_i [y_i(w^T x_i + b)-1]$$
- $\alpha_i \ge 0$ : 라그랑주 승수

---

## 12. 최적 조건
편미분을 0으로 두면 다음을 얻는다.

1. **$w$에 대해**
$$w = \sum_i \alpha_i y_i x_i$$
2. **$b$에 대해**
$$\sum_i \alpha_i y_i = 0$$

---

## 13. 듀얼 문제(Dual Problem)
<img width="1480" height="1059" alt="image" src="https://github.com/user-attachments/assets/fce6e8c2-c283-4603-b073-f1a216413402" />
<img width="1487" height="1079" alt="image" src="https://github.com/user-attachments/assets/fa5db1ea-bb5f-4f6a-922d-fac4d4dca7d6" />
<img width="1487" height="926" alt="image" src="https://github.com/user-attachments/assets/9c48b121-b43d-44de-b120-ff28417dc06c" />
<img width="1480" height="868" alt="image" src="https://github.com/user-attachments/assets/8dfa33ab-6af2-41e9-b799-07da24fd6aca" />

프라이멀 문제를 듀얼 형태로 변환하면 다음과 같다.


$$
\begin{aligned}
\max_{\alpha} \quad & \sum_i \alpha_i - \frac{1}{2}\sum_i\sum_j \alpha_i \alpha_j y_i y_j x_i^T x_j \\
\text{subject to} \quad & \sum_i \alpha_i y_i = 0 \\
& \alpha_i \ge 0
\end{aligned}
$$

이 문제 역시 QP 문제이며 효율적으로 풀 수 있다.

---

## 14. 서포트 벡터(Support Vector)
<img width="1505" height="972" alt="image" src="https://github.com/user-attachments/assets/8870d226-a2b4-4ed6-8a9f-14206d38553d" />

### 정의
$$\alpha_i > 0$$
을 만족하는 데이터 포인트를 **서포트 벡터**라고 한다. 이들은 다음 성질을 가진다.
- 마진 경계 위에 위치
- 결정 경계를 직접적으로 결정

<img width="1499" height="952" alt="image" src="https://github.com/user-attachments/assets/6bc3e25c-c33b-4e48-bafc-eb5ce96e4c43" />

### 중요한 사실
$$w = \sum_{i \in SV} \alpha_i y_i x_i$$
즉, **모든 데이터를 사용하지 않고 서포트 벡터만 사용한다.** 이를 **Sparse Representation**이라고 한다.

---

## 15. 바이어스 b 계산
서포트 벡터 $x_s$에 대해
$$y_s (w^T x_s + b) = 1$$
따라서
$$b = y_s - w^T x_s$$

---

## 16. 최종 분류 함수
<img width="1508" height="896" alt="image" src="https://github.com/user-attachments/assets/9771e41b-5633-4bf7-996e-602d08bdcef3" />
<img width="1258" height="939" alt="image" src="https://github.com/user-attachments/assets/3940dac5-8064-4fdd-b00f-2e990324da49" />

새로운 입력 $x$에 대해
$$f(x) = \text{sign}(w^T x + b)$$
- $f(x) = +1$ → 클래스 $+1$
- $f(x) = -1$ → 클래스 $-1$

---

## 17. 정리
선형 SVM은 다음 원리에 기반한다.
- 두 클래스를 구분하는 초평면 탐색
- 마진 최대화
- 일반화 성능 극대화
- QP 기반 최적화
- 서포트 벡터만으로 모델 결정

### 다음 강의 예고
다음 강의에서는 다음을 다룬다.
- 선형 분리가 불가능한 경우
- 소프트 마진 SVM
- 커널 트릭(Kernel Trick)
- 비선형 SVM

📘 이 문서는 실제 대학 머신러닝 이론 강의 수준으로 정리된 SVM 완전 노트이다.
