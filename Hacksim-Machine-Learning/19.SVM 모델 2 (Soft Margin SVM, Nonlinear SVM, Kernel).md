# Support Vector Machine (SVM) – Lecture 2
## Soft Margin SVM & Nonlinear SVM (Kernel Method)

---

## 0. 강의 개요
이번 강의에서는 선형으로 분리할 수 없는 데이터(non-linearly separable case) 에 대해 Support Vector Machine을 어떻게 확장하는지를 다룬다.

**주요 구성:**
- Soft Margin SVM과 슬랙 변수($\xi$)의 도입
- 하이퍼파라미터 $C$의 역할
- Dual formulation 유도 및 Support Vector 해석
- Nonlinear SVM과 커널 트릭(Kernel Trick)

---

## 1. 선형 분리가 가능한 경우 (Hard Margin SVM 복습)
훈련 데이터가 다음 조건을 만족하는 경우를 말한다.
$$y_i (\mathbf{w}^T \mathbf{x}_i + b) \ge 1$$

- 모든 데이터가 정확히 분리됨 (Training error = 0)
- 마진(margin)의 크기: $$\text{margin} = \frac{2}{\|\mathbf{w}\|}$$
- **목적함수:** $$\min_{\mathbf{w}, b} \frac{1}{2} \|\mathbf{w}\|^2$$

하지만 현실 데이터에서는 노이즈나 이상치로 인해 선형 분리가 불가능한 경우가 거의 대부분이다.

---

## 2. 선형 분리가 불가능한 경우
다음과 같은 문제가 발생한다.
- 어떤 직선을 그어도 두 클래스를 완벽히 분리할 수 없음
- Training error = 0 인 초평면이 존재하지 않음
$$\nexists (\mathbf{w}, b) \quad \text{s.t.} \quad y_i(\mathbf{w}^T \mathbf{x}_i + b) \ge 1$$



---

## 3. Soft Margin SVM 핵심 아이디어
> **“모든 데이터를 완벽하게 맞추려 하지 말고, 일부 오차를 허용하자.”**
<img width="1467" height="1076" alt="image" src="https://github.com/user-attachments/assets/c2e32b6b-188e-48e2-a9de-50acd105a580" />

이를 위해 **슬랙 변수(Slack variable, $\xi$)**를 도입한다.

---

## 4. Slack Variable ($\xi_i$)
각 데이터 포인트마다 다음 제약 조건의 위반을 허용한다.
$$y_i(\mathbf{w}^T \mathbf{x}_i + b) \ge 1 - \xi_i, \quad \xi_i \ge 0$$

### $\xi_i$의 의미
| $\xi_i$ 값 | 의미 |
| :--- | :--- |
| $\xi_i = 0$ | 마진 밖 (정확 분류) |
| $0 < \xi_i < 1$ | 마진 내부 위치 |
| $\xi_i \ge 1$ | 오분류 발생 |



---

## 5. Soft Margin SVM 목적함수
<img width="1478" height="1055" alt="image" src="https://github.com/user-attachments/assets/600eb601-ffe5-411b-bba8-9b9983baf998" />

Hard Margin과 달리 **'마진 최대화'**와 **'오차 최소화'**라는 두 목표가 공존한다.
$$\min_{\mathbf{w}, b, \xi} \frac{1}{2}\|\mathbf{w}\|^2 + C \sum_{i=1}^N \xi_i$$
$$\text{subject to } y_i(\mathbf{w}^T \mathbf{x}_i + b) \ge 1 - \xi_i, \quad \xi_i \ge 0$$

---

## 6. 하이퍼파라미터 $C$의 의미
$C$는 마진(Bias)과 훈련 오차(Variance) 사이의 트레이드오프를 조절한다.
<img width="1486" height="1084" alt="image" src="https://github.com/user-attachments/assets/646ef410-21bd-46e0-8d63-89e66d5bbc3d" />

- **$C \uparrow$ (큰 값):** 오차 허용 ❌, $\xi$에 큰 패널티 부여, 마진 작아짐 $\rightarrow$ **Overfitting** 가능성 증가
- **$C \downarrow$ (작은 값):** 오차 허용 ⭕, 마진 넓어짐, 일반화 성능 중시 $\rightarrow$ **Underfitting** 가능성 증가

---

## 7. Lagrangian Formulation
<img width="1179" height="827" alt="image" src="https://github.com/user-attachments/assets/e159fb79-35c4-4448-8289-2f2bb9160432" />

라그랑주 승수 $\alpha_i \ge 0, \gamma_i \ge 0$를 도입한 식은 다음과 같다.
$$L = \frac{1}{2}\|\mathbf{w}\|^2 + C\sum_i \xi_i - \sum_i \alpha_i[ y_i(\mathbf{w}^T\mathbf{x}_i + b) - 1 + \xi_i ] - \sum_i \gamma_i \xi_i$$

---

## 8. 미분을 통한 조건 도출
<img width="1466" height="1015" alt="image" src="https://github.com/user-attachments/assets/f3bc894c-03fa-4e4e-8755-7b189d4310fc" />

1. **$\mathbf{w}$에 대해 미분:** $\mathbf{w} = \sum_i \alpha_i y_i \mathbf{x}_i$
2. **$b$에 대해 미분:** $\sum_i \alpha_i y_i = 0$
3. **$\xi_i$에 대해 미분:** $C - \alpha_i - \gamma_i = 0 \rightarrow$ 결과: **$0 \le \alpha_i \le C$**
<img width="1519" height="1066" alt="image" src="https://github.com/user-attachments/assets/3b06c82a-c21b-4ffc-8296-23ac47d366bd" />

---

## 9. Dual Problem
<img width="1167" height="810" alt="image" src="https://github.com/user-attachments/assets/613685a4-58e9-4e08-b50d-81811850f9e3" />
<img width="1443" height="832" alt="image" src="https://github.com/user-attachments/assets/7ed6e366-1b8a-4c8a-b347-08805a5ff142" />
<img width="1450" height="917" alt="image" src="https://github.com/user-attachments/assets/08d234ea-73d8-4a76-ab75-32f7188b24e0" />

최종 Dual optimization 식은 다음과 같다.
$$\max_{\alpha} \sum_i \alpha_i - \frac{1}{2} \sum_i \sum_j \alpha_i \alpha_j y_i y_j (\mathbf{x}_i^T \mathbf{x}_j)$$
$$\text{subject to } 0 \le \alpha_i \le C, \quad \sum_i \alpha_i y_i = 0$$

---

## 10. Support Vector의 정의 (KKT 조건)
<img width="1511" height="1099" alt="image" src="https://github.com/user-attachments/assets/57203f58-3cdf-483f-abe2-511155520984" />

- **$\alpha_i = 0$:** 마진 밖 위치 (Support Vector 아님)
- **$0 < \alpha_i < C$:** 정확히 마진 경계 위 위치 (Support Vector)
- **$\alpha_i = C$:** 마진 내부 또는 오분류 (Support Vector)

👉 **$\alpha_i > 0$ 인 점만이 결정 경계에 영향을 준다.**

---

## 11. Nonlinear SVM의 필요성
<img width="1409" height="826" alt="image" src="https://github.com/user-attachments/assets/c4ba35ea-dcd8-4e1a-9edf-6145846442af" />
<img width="1505" height="1042" alt="image" src="https://github.com/user-attachments/assets/78bfb9f8-5968-4b6d-b169-df40078c2881" />

선형 결정 경계로는 XOR 구조나 원형 데이터 등을 분리할 수 없다. 이를 위해 데이터를 고차원 공간으로 변환하는 **Feature Mapping($\phi$)**을 사용한다.

---

## 12. Kernel Trick
<img width="1436" height="893" alt="image" src="https://github.com/user-attachments/assets/f0e7d719-7324-4e0e-a449-1a4a2ee9acaa" />
<img width="1381" height="940" alt="image" src="https://github.com/user-attachments/assets/d2ad3db4-32f8-48ab-bae8-e6777c1be5ec" />
<img width="1367" height="739" alt="image" src="https://github.com/user-attachments/assets/da434242-d172-41e0-bef7-52507c6137b6" />
<img width="1517" height="1087" alt="image" src="https://github.com/user-attachments/assets/b483aa48-d135-464d-bf9a-a72a2c41c8e5" />

Dual problem 식에는 오직 데이터 간의 내적($\mathbf{x}_i^T \mathbf{x}_j$)만 등장한다. 이를 커널 함수로 대체한다.
$$K(\mathbf{x}_i, \mathbf{x}_j) = \phi(\mathbf{x}_i)^T \phi(\mathbf{x}_j)$$
👉 $\phi$를 직접 계산하지 않고도 고차원에서의 내적 값을 얻는 것이 **커널 트릭**의 핵심이다.



---

## 13. 대표적인 Kernel 함수
<img width="1327" height="1047" alt="image" src="https://github.com/user-attachments/assets/a0cfd0b2-34c7-4386-ae1a-0b43f8e423a2" />
<img width="1516" height="1114" alt="image" src="https://github.com/user-attachments/assets/632939d2-21c0-48d0-a7b4-78832d910324" />

- **Linear:** $K(x,z)=x^Tz$
- **Polynomial:** $K(x,z)=(x^Tz + c)^d$
- **RBF (Gaussian):** $K(x,z)=\exp\left(-\frac{\|x-z\|^2}{2\sigma^2}\right)$
- **Sigmoid:** $K(x,z)=\tanh(ax^Tz+b)$

---

## 14. 최종 결정함수
<img width="1462" height="740" alt="image" src="https://github.com/user-attachments/assets/2eb96cfe-1e48-471a-8940-52c2e13e1773" />

$$f(x)=\sum_i \alpha_i y_i K(x_i,x)+b$$
Support Vector들만이 예측 계산에 사용된다.

---

## 15. 정리
- SVM의 핵심은 **마진 최대화**이다.
- Soft margin은 슬랙 변수를 통해 **오류를 허용**하며, $C$로 복잡도를 조절한다.
- **Kernel Trick**을 통해 고차원 비선형 분류 문제를 선형 문제처럼 해결한다.

✅ **Lecture 2: Soft Margin & Kernel SVM 완전 정리 끝**
