{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO8Rh9AyOqq5cFmE+YbMt2/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JunHyeong-data/ML-DL-Study/blob/main/Basic-Deep-Learning/10_%EB%94%A5%EB%9F%AC%EB%8B%9D_%EA%B8%B0%EC%B4%88_%2C_%EC%96%BC%EB%A6%AC_%EC%8A%A4%ED%83%91%2C_%EC%A1%B0%EA%B8%B0%EB%A9%88%EC%B6%A4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Early Stopping (ÏñºÎ¶¨ Ïä§ÌÉëÌïë)\n",
        "\n",
        "## 1. Î∞∞Í≤Ω: OverfittingÍ≥º Regularization\n",
        "\n",
        "Ïù¥Ï†Ñ ÏãúÍ∞ÑÏóêÎäî **Overfitting ÌòÑÏÉÅ**Í≥º Ïù¥Î•º ÏôÑÌôîÌïòÍ∏∞ ÏúÑÌïú **Regularization** Í∏∞Î≤ïÎì§Ïóê ÎåÄÌï¥ ÏÇ¥Ìé¥Î≥¥ÏïòÎã§.  \n",
        "ÎåÄÌëúÏ†ÅÏù∏ Regularization Î∞©Î≤ïÏúºÎ°úÎäî Îã§ÏùåÍ≥º Í∞ôÏùÄ Í≤ÉÎì§Ïù¥ ÏûàÎã§.\n",
        "\n",
        "- Dropout\n",
        "- Batch Í¥ÄÎ†® Í∏∞Î≤ï\n",
        "- Normalization\n",
        "- Îç∞Ïù¥ÌÑ∞ Ï¶ùÍ∞ï (Data Augmentation)\n",
        "\n",
        "Í∑∏Î¶¨Í≥† **Í∞ÄÏû• Í∞ïÎ†•Ìïú Regularization Î∞©Î≤ïÏùÄ Ï∂©Î∂ÑÌïú Îç∞Ïù¥ÌÑ∞Î•º ÌôïÎ≥¥ÌïòÎäî Í≤É**Ïù¥ÎùºÎäî Ï†êÎèÑ Ìï®Íªò Ïñ∏Í∏âÌñàÎã§.\n",
        "\n",
        "Ïù¥Îü¨Ìïú Î∞©Î≤ïÎì§ Ï§ë, Îç∞Ïù¥ÌÑ∞ ÌôïÎ≥¥ÎßåÌÅºÏù¥ÎÇò **Ïã§Ï†úÎ°ú Îß§Ïö∞ practicalÌïòÍ≤å Ï§ëÏöîÌïú Regularization Í∏∞Î≤ï**Ïù¥ Î∞îÎ°ú **Early Stopping**Ïù¥Îã§.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Early Stopping Í∞úÎÖê\n",
        "\n",
        "Early StoppingÏùÄ Îßê Í∑∏ÎåÄÎ°ú **ÌïôÏäµÏùÑ ÎÅùÍπåÏßÄ ÏßÑÌñâÌïòÏßÄ ÏïäÍ≥†, Ï§ëÍ∞ÑÏóê Î©àÏ∂îÎäî Í∏∞Î≤ï**Ïù¥Îã§.  \n",
        "ÌõàÎ†® ÎèÑÏ§ë **Validation ÏÑ±Îä•Ïù¥ Îçî Ïù¥ÏÉÅ Ï¢ãÏïÑÏßÄÏßÄ ÏïäÏúºÎ©¥ ÌïôÏäµÏùÑ Ï¢ÖÎ£å**ÌïòÍ≥†,  \n",
        "Í∑∏ ÏãúÏ†êÍπåÏßÄ ÌïôÏäµÎêú Î™®Îç∏ÏùÑ ÏµúÏ¢Ö Î™®Îç∏Î°ú ÏÇ¨Ïö©ÌïúÎã§.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Ïã§Ìóò ÏÑ§Ï†ï\n",
        "\n",
        "- Îç∞Ïù¥ÌÑ∞: ÌöåÏò§Î¶¨(spiral) ÌòïÌÉúÏùò Îç∞Ïù¥ÌÑ∞\n",
        "- Îç∞Ïù¥ÌÑ∞ Î∂ÑÌï†: Training / Validation\n",
        "- Î™®Îç∏ Íµ¨Ï°∞\n",
        "  - Input dimension\n",
        "  - Hidden dimension = 16\n",
        "  - Hidden layers = 3\n",
        "  - Dropout layer = 2Í∞ú\n",
        "- Loss function: Binary Cross Entropy\n",
        "- Optimizer: Stochastic Gradient Descent (SGD)\n",
        "\n",
        "Í∏∞Ï°¥ ÏΩîÎìúÏóêÏÑúÎäî input size, hidden size, output sizeÍ∞Ä global variableÎ°ú ÏÑ†Ïñ∏ÎêòÏñ¥ ÏûàÏóàÏßÄÎßå,  \n",
        "Ïù¥Î•º **Neural NetworkÏùò ConstructorÏóêÏÑú Ï†ÑÎã¨Î∞õÎèÑÎ°ù ÏàòÏ†ï**ÌïòÏó¨ Îçî Ïû¨ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Íµ¨Ï°∞Î°ú Î≥ÄÍ≤ΩÌñàÎã§.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Training Loss vs Validation Loss Í¥ÄÏ∞∞\n",
        "\n",
        "ÌïôÏäµÏùÑ ÏßÑÌñâÌïòÎ©¥ÏÑú Îã§Ïùå ÌòÑÏÉÅÏùÑ ÌôïÏù∏Ìï† Ïàò ÏûàÎã§.\n",
        "\n",
        "- **Training loss**: ÏßÄÏÜçÏ†ÅÏúºÎ°ú Í∞êÏÜå\n",
        "- **Validation loss**:\n",
        "  - ÏïΩ 0.25ÍπåÏßÄ Í∞êÏÜå\n",
        "  - Ïù¥ÌõÑ Îã§Ïãú Ï¶ùÍ∞Ä\n",
        "\n",
        "Ïù¥Î•º Îçî Î™ÖÌôïÌûà Î≥¥Í∏∞ ÏúÑÌï¥:\n",
        "\n",
        "- Training loss Î¶¨Ïä§Ìä∏\n",
        "- Validation loss Î¶¨Ïä§Ìä∏\n",
        "\n",
        "Î•º Í∞ÅÍ∞Å Ï†ÄÏû•Ìïú Îí§, Í∑∏ÎûòÌîÑÎ°ú ÏãúÍ∞ÅÌôîÌñàÎã§.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Overfitting Î∞úÏÉù ÏãúÏ†ê\n",
        "\n",
        "Í∑∏ÎûòÌîÑÎ•º ÌÜµÌï¥ Îã§ÏùåÏùÑ ÌôïÏù∏Ìï† Ïàò ÏûàÎã§.\n",
        "\n",
        "- Training lossÎäî Í≥ÑÏÜç Í∞êÏÜå\n",
        "- Validation lossÎäî ÏïΩ **0.2ÍπåÏßÄ Í∞êÏÜå ÌõÑ Îã§Ïãú Ï¶ùÍ∞Ä**\n",
        "- ÏïΩ **1000 step Ïù¥ÌõÑÎ∂ÄÌÑ∞ OverfittingÏù¥ ÏãúÏûë**\n",
        "- **2500 step Î∂ÄÍ∑ºÏóêÏÑú Validation lossÍ∞Ä ÏµúÏÜå**\n",
        "\n",
        "Ï¶â, Ïù¥ Í≤ΩÏö∞ÏóêÎäî **2500Î≤àÏß∏ stepÏóêÏÑú ÌïôÏäµÏùÑ Î©àÏ∂îÎäî Í≤ÉÏù¥ Í∞ÄÏû• Ïù¥ÏÉÅÏ†Å**Ïù¥Îã§.\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Early Stopping Í∏∞Î≥∏ Íµ¨ÌòÑ ÏïÑÏù¥ÎîîÏñ¥\n",
        "\n",
        "Early StoppingÏùò ÌïµÏã¨ Î°úÏßÅÏùÄ Îß§Ïö∞ Îã®ÏàúÌïòÎã§.\n",
        "\n",
        "1. ÌòÑÏû¨ÍπåÏßÄÏùò **minimum validation loss**Î•º Ï†ÄÏû•\n",
        "2. ÌòÑÏû¨ validation lossÍ∞Ä Îçî ÏûëÎã§Î©¥:\n",
        "   - minimum validation loss Í∞±Ïã†\n",
        "   - Ìï¥Îãπ ÏãúÏ†êÏùò Î™®Îç∏ Ï†ÄÏû•\n",
        "3. validation lossÍ∞Ä Îçî Ïª§Ï°åÎã§Î©¥:\n",
        "   - ÌïôÏäµ Ï¢ÖÎ£å\n",
        "\n",
        "---\n",
        "\n",
        "## 7. Î¨∏Ï†úÏ†ê: Validation LossÏùò Noise\n",
        "\n",
        "ÌïòÏßÄÎßå ÏúÑ Î∞©ÏãùÏóêÎäî Î¨∏Ï†úÍ∞Ä ÏûàÎã§.\n",
        "\n",
        "- Validation lossÏóê **noiseÍ∞Ä Ïã¨ÌïòÍ≤å Ìè¨Ìï®**\n",
        "- Ïû†ÍπêÏùò ÏÉÅÏäπÎßåÏúºÎ°úÎèÑ ÌïôÏäµÏù¥ ÎÑàÎ¨¥ ÏùºÏ∞ç Ï¢ÖÎ£åÎê† Ïàò ÏûàÏùå\n",
        "- Ïã§Ï†úÎ°úÎäî 2500 stepÏóêÏÑú Î©àÏ∂∞Ïïº ÌïòÎäîÎç∞,\n",
        "  noise ÎïåÎ¨∏Ïóê 500 step Î∂ÄÍ∑ºÏóêÏÑú Î©àÏ∂∞Î≤ÑÎ¶¨Îäî ÌòÑÏÉÅ Î∞úÏÉù\n",
        "\n",
        "---\n",
        "\n",
        "## 8. Ìï¥Í≤∞Ï±Ö: Patience ÎèÑÏûÖ\n",
        "\n",
        "Ïù¥Î•º Ìï¥Í≤∞ÌïòÍ∏∞ ÏúÑÌï¥ **patience Î≥ÄÏàò**Î•º ÎèÑÏûÖÌïúÎã§.\n",
        "\n",
        "- `patience = 100`\n",
        "- ÏùòÎØ∏:\n",
        "  - Validation ÏÑ±Îä•Ïù¥ ÎÇòÎπ†ÏßÄÎçîÎùºÎèÑ\n",
        "  - **100 step ÎèôÏïàÏùÄ Í∞úÏÑ†Ïù¥ ÏóÜÏùÑ Í≤ΩÏö∞ÏóêÎßå ÌïôÏäµ Ï¢ÖÎ£å**\n",
        "\n",
        "Ï∂îÍ∞ÄÎ°ú ÌïÑÏöîÌïú Î≥ÄÏàòÎì§:\n",
        "\n",
        "- `steps_no_improve`: Í∞úÏÑ† ÏóÜÏù¥ ÏßÑÌñâÎêú step Ïàò\n",
        "- `best_step`: minimum validation lossÍ∞Ä Î∞úÏÉùÌïú step\n",
        "\n",
        "---\n",
        "\n",
        "## 9. Patience Í∏∞Î∞ò Early Stopping Î°úÏßÅ\n",
        "\n",
        "- Validation lossÍ∞Ä Í∞úÏÑ†ÎêòÏóàÏùÑ Í≤ΩÏö∞:\n",
        "  - minimum validation loss ÏóÖÎç∞Ïù¥Ìä∏\n",
        "  - best step ÏóÖÎç∞Ïù¥Ìä∏\n",
        "  - `steps_no_improve` Ï¥àÍ∏∞Ìôî\n",
        "  - **Î™®Îç∏ Ï†ÄÏû• (Ï§ëÏöî)**\n",
        "\n",
        "- Validation lossÍ∞Ä Í∞úÏÑ†ÎêòÏßÄ ÏïäÏïòÏùÑ Í≤ΩÏö∞:\n",
        "  - `steps_no_improve += 1`\n",
        "  - `steps_no_improve == patience` Ïù¥Î©¥ ÌïôÏäµ Ï¢ÖÎ£å\n",
        "\n",
        "---\n",
        "\n",
        "## 10. Early Stopping Ï†ÅÏö© Í≤∞Í≥º\n",
        "\n",
        "PatienceÎ•º Ï†ÅÏö©Ìïú Îí§ Îã§Ïãú ÌïôÏäµÌïòÎ©¥:\n",
        "\n",
        "- Validation lossÍ∞Ä Í∞úÏÑ†ÎêòÏßÄ ÏïäÏùÄ ÏÉÅÌÉúÎ°ú\n",
        "- **100 stepÏù¥ Îçî ÏßÑÌñâÎêú Îí§ ÌïôÏäµÏù¥ Ï¢ÖÎ£å**\n",
        "\n",
        "Ï¶â, **noiseÏóê Í∞ïÏù∏Ìïú Early StoppingÏù¥ Ï†úÎåÄÎ°ú ÎèôÏûë**Ìï®ÏùÑ ÌôïÏù∏Ìï† Ïàò ÏûàÎã§.\n",
        "\n",
        "---\n",
        "\n",
        "## 11. Ïñ¥Îñ§ MetricÏùÑ ÏÇ¨Ïö©Ìï† Ïàò ÏûàÏùÑÍπå?\n",
        "\n",
        "Early StoppingÏùÄ Î∞òÎìúÏãú validation lossÎßåÏùÑ Í∏∞Ï§ÄÏúºÎ°ú Ìï† ÌïÑÏöîÎäî ÏóÜÎã§.\n",
        "\n",
        "Î¨∏Ï†úÏóê Îî∞Îùº Îã§ÏùåÍ≥º Í∞ôÏùÄ metricÏùÑ ÏÇ¨Ïö©Ìï† Ïàò ÏûàÎã§.\n",
        "\n",
        "- Accuracy\n",
        "- Precision / Recall\n",
        "- F1-score\n",
        "- IoU (Intersection over Union)  \n",
        "  ‚Üí Computer Vision Î¨∏Ï†úÏóêÏÑú ÏûêÏ£º ÏÇ¨Ïö©\n",
        "\n",
        "**Ïñ¥Îñ§ metricÏù¥Îì†, validation ÏÑ±Îä•Ïù¥ Îçî Ïù¥ÏÉÅ Ï¢ãÏïÑÏßÄÏßÄ ÏïäÏúºÎ©¥ ÌïôÏäµÏùÑ Î©àÏ∂îÎäî Í≤É**Ïù¥ Early StoppingÏùò Î≥∏ÏßàÏù¥Îã§.\n",
        "\n",
        "---\n",
        "\n",
        "## 12. Ï∞∏Í≥† ÏÇ¨Ìï≠\n",
        "\n",
        "ÏùºÎ∂Ä ÎÖºÎ¨∏ÏóêÏÑúÎäî:\n",
        "\n",
        "- Îß§Ïö∞ ÌÅ∞ Deep Neural NetworkÏùò Í≤ΩÏö∞\n",
        "- Early Stopping ÏóÜÏù¥ Í≥ÑÏÜç ÌïôÏäµÌïòÎ©¥\n",
        "- Validation ÏÑ±Îä•Ïù¥ Îã§Ïãú Ï¢ãÏïÑÏßÑÎã§Îäî Ï£ºÏû•ÎèÑ ÏûàÏùå\n",
        "\n",
        "ÌïòÏßÄÎßå Ïù¥Îäî **ÎßâÎåÄÌïú Ïª¥Ìì®ÌåÖ ÌååÏõåÍ∞Ä ÌïÑÏöî**ÌïòÎ©∞,\n",
        "Ïã§Ï†úÎ°ú Ïã§ÌóòÌï¥Î≥¥Í∏¥ Ïñ¥Î†µÎã§.\n",
        "\n",
        "üëâ Ïó¨Í∏∞ÏÑúÎäî  \n",
        "‚ÄúÏù¥Îü∞ Ïó∞Íµ¨ÎèÑ ÏûàÎã§‚Äù Ï†ïÎèÑÎ°úÎßå ÏïåÏïÑÎëêÎ©¥ Ï∂©Î∂ÑÌïòÎã§.\n",
        "\n",
        "---\n",
        "\n",
        "## 13. Ï†ïÎ¶¨\n",
        "\n",
        "- Early StoppingÏùÄ Îß§Ïö∞ Í∞ÑÎã®ÌïòÏßÄÎßå Í∞ïÎ†•Ìïú Regularization Í∏∞Î≤ï\n",
        "- OverfittingÏùÑ Î∞©ÏßÄÌïòÍ≥† Î∂àÌïÑÏöîÌïú ÌïôÏäµÏùÑ Ï§ÑÏó¨Ï§å\n",
        "- PatienceÎ•º Ìï®Íªò ÏÇ¨Ïö©ÌïòÎ©¥ noise Î¨∏Ï†úÎèÑ Ìö®Í≥ºÏ†ÅÏúºÎ°ú ÏôÑÌôî Í∞ÄÎä•"
      ],
      "metadata": {
        "id": "7-2l6P1boWiA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZerrgjI_n4MZ",
        "outputId": "1c22be55-f70d-47eb-917d-f2508c5c0256"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'deepLearning'...\n",
            "remote: Enumerating objects: 318, done.\u001b[K\n",
            "remote: Counting objects: 100% (179/179), done.\u001b[K\n",
            "remote: Compressing objects: 100% (122/122), done.\u001b[K\n",
            "remote: Total 318 (delta 80), reused 134 (delta 55), pack-reused 139 (from 1)\u001b[K\n",
            "Receiving objects: 100% (318/318), 17.60 MiB | 22.14 MiB/s, done.\n",
            "Resolving deltas: 100% (116/116), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/NoCodeProgram/deepLearning.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "# Load the DataFrame from a CSV file\n",
        "df = pd.read_csv('./deepLearning/nn/swirl.csv')\n",
        "data = df[['x', 'y']].values\n",
        "labels = df['label'].values.reshape(-1, 1)\n",
        "\n",
        "# Print the shapes of the data and labels\n",
        "print('Data shape:', data.shape)\n",
        "print('Labels shape:', labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3Z4T5rDp8nj",
        "outputId": "f0050078-d5db-4e23-a920-cf629449680c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data shape: (280, 2)\n",
            "Labels shape: (280, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# Create a scatter plot\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.scatter(data[:,0], data[:,1], c=labels, cmap='viridis')\n",
        "# plt.scatter(val_data[:,0], val_data[:,1], c=val_labels, cmap='viridis')\n",
        "\n",
        "# Set the title and labels\n",
        "plt.title('2D Synthetic Data')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 718
        },
        "id": "A4Ho8hJfqGuI",
        "outputId": "8ebd1851-8754-4dc2-a219-dcd5fec26278"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Convert the numpy arrays to PyTorch tensors\n",
        "data_torch = torch.tensor(data, dtype=torch.float32)\n",
        "labels_torch = torch.tensor(labels, dtype=torch.float32)\n",
        "data_length = len(data_torch)\n",
        "split_length = int(0.7*data_length)\n",
        "\n",
        "train_data = data_torch[:split_length]\n",
        "train_labels = labels_torch[:split_length]\n",
        "val_data = data_torch[split_length:]\n",
        "val_labels = labels_torch[split_length:]\n",
        "\n",
        "print(train_data.shape, train_labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BQTpVQZqSYP",
        "outputId": "579659ca-3006-4c25-9d43-696b88f09f8c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([196, 2]) torch.Size([196, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "g = torch.Generator().manual_seed(42)\n",
        "g.manual_seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "class DNet(nn.Module):\n",
        "    def __init__(self, input_size: int, hidden_size: int, output_size: int):\n",
        "        super().__init__()\n",
        "        self.seq_model = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_size,bias=True),\n",
        "\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size,bias=True),\n",
        "\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size,bias=True),\n",
        "\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(hidden_size, hidden_size,bias=True),\n",
        "\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(hidden_size, output_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.seq_model(x)\n",
        "\n",
        "dnet = DNet(input_size=2,hidden_size=16,output_size=1)\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.SGD(dnet.parameters(), lr=0.01)\n",
        "\n",
        "t_losses = []\n",
        "val_losses = []\n",
        "\n",
        "for steps in range(50000):\n",
        "    dnet.train()\n",
        "\n",
        "    output = dnet(train_data)\n",
        "    train_loss = loss_fn(output, train_labels)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    train_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if steps % 10 == 0:\n",
        "        dnet.eval()\n",
        "        output = dnet(val_data)\n",
        "        val_loss = loss_fn(output, val_labels)\n",
        "        output = dnet(train_data)\n",
        "        t_loss = loss_fn(output,train_labels)\n",
        "        t_losses.append(t_loss.item())\n",
        "        val_losses.append(val_loss.item())\n",
        "        print(f\"{steps} val_loss: {val_loss.item()}, train_loss: {t_loss.item()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbDmXp1cqXOT",
        "outputId": "babfb9ad-8c69-4116-c0e5-6b0671c71c04"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 val_loss: 0.6986224055290222, train_loss: 0.6956242322921753\n",
            "10 val_loss: 0.6983293294906616, train_loss: 0.6954178214073181\n",
            "20 val_loss: 0.6980583667755127, train_loss: 0.6952297687530518\n",
            "30 val_loss: 0.6977936625480652, train_loss: 0.695051908493042\n",
            "40 val_loss: 0.6975544691085815, train_loss: 0.6948878169059753\n",
            "50 val_loss: 0.6973187327384949, train_loss: 0.6947313547134399\n",
            "60 val_loss: 0.6971179842948914, train_loss: 0.6945965886116028\n",
            "70 val_loss: 0.6969249248504639, train_loss: 0.694471538066864\n",
            "80 val_loss: 0.6967272758483887, train_loss: 0.694343626499176\n",
            "90 val_loss: 0.6965352296829224, train_loss: 0.694225013256073\n",
            "100 val_loss: 0.6963778734207153, train_loss: 0.6941238641738892\n",
            "110 val_loss: 0.6961909532546997, train_loss: 0.6940051317214966\n",
            "120 val_loss: 0.6960338354110718, train_loss: 0.6939107179641724\n",
            "130 val_loss: 0.6958953142166138, train_loss: 0.6938242316246033\n",
            "140 val_loss: 0.6957404017448425, train_loss: 0.6937288045883179\n",
            "150 val_loss: 0.6956164240837097, train_loss: 0.6936526894569397\n",
            "160 val_loss: 0.6954829096794128, train_loss: 0.6935704946517944\n",
            "170 val_loss: 0.6953544020652771, train_loss: 0.6934943795204163\n",
            "180 val_loss: 0.6952447295188904, train_loss: 0.6934296488761902\n",
            "190 val_loss: 0.6951436996459961, train_loss: 0.6933684945106506\n",
            "200 val_loss: 0.6950479745864868, train_loss: 0.6933143138885498\n",
            "210 val_loss: 0.6949601769447327, train_loss: 0.693263828754425\n",
            "220 val_loss: 0.6948691606521606, train_loss: 0.6932122111320496\n",
            "230 val_loss: 0.6947861909866333, train_loss: 0.6931622624397278\n",
            "240 val_loss: 0.6947013139724731, train_loss: 0.6931119561195374\n",
            "250 val_loss: 0.6946109533309937, train_loss: 0.6930546760559082\n",
            "260 val_loss: 0.694517970085144, train_loss: 0.6930022239685059\n",
            "270 val_loss: 0.6944314241409302, train_loss: 0.6929523944854736\n",
            "280 val_loss: 0.6943609714508057, train_loss: 0.6929115653038025\n",
            "290 val_loss: 0.6943004131317139, train_loss: 0.6928800344467163\n",
            "300 val_loss: 0.6942197680473328, train_loss: 0.692834734916687\n",
            "310 val_loss: 0.6941444873809814, train_loss: 0.6927933096885681\n",
            "320 val_loss: 0.6940871477127075, train_loss: 0.6927616596221924\n",
            "330 val_loss: 0.6940280795097351, train_loss: 0.6927245855331421\n",
            "340 val_loss: 0.6939780712127686, train_loss: 0.6926968693733215\n",
            "350 val_loss: 0.6939160823822021, train_loss: 0.6926615834236145\n",
            "360 val_loss: 0.6938666701316833, train_loss: 0.6926331520080566\n",
            "370 val_loss: 0.6938115358352661, train_loss: 0.6925972700119019\n",
            "380 val_loss: 0.6937521696090698, train_loss: 0.6925570964813232\n",
            "390 val_loss: 0.6937128901481628, train_loss: 0.6925361156463623\n",
            "400 val_loss: 0.6936817169189453, train_loss: 0.6925147771835327\n",
            "410 val_loss: 0.6936354637145996, train_loss: 0.6924836039543152\n",
            "420 val_loss: 0.693587064743042, train_loss: 0.6924529075622559\n",
            "430 val_loss: 0.6935314536094666, train_loss: 0.6924214959144592\n",
            "440 val_loss: 0.6934927701950073, train_loss: 0.6923958659172058\n",
            "450 val_loss: 0.6934506297111511, train_loss: 0.6923667192459106\n",
            "460 val_loss: 0.6934126615524292, train_loss: 0.6923388838768005\n",
            "470 val_loss: 0.6933730244636536, train_loss: 0.6923096179962158\n",
            "480 val_loss: 0.6933351159095764, train_loss: 0.6922845840454102\n",
            "490 val_loss: 0.6933022141456604, train_loss: 0.692267119884491\n",
            "500 val_loss: 0.6932724714279175, train_loss: 0.6922435164451599\n",
            "510 val_loss: 0.6932367086410522, train_loss: 0.6922217607498169\n",
            "520 val_loss: 0.6932129859924316, train_loss: 0.6922017335891724\n",
            "530 val_loss: 0.6931706666946411, train_loss: 0.692170262336731\n",
            "540 val_loss: 0.693138837814331, train_loss: 0.6921470761299133\n",
            "550 val_loss: 0.6931076049804688, train_loss: 0.6921270489692688\n",
            "560 val_loss: 0.693081796169281, train_loss: 0.6921055316925049\n",
            "570 val_loss: 0.6930547952651978, train_loss: 0.6920812726020813\n",
            "580 val_loss: 0.6930263042449951, train_loss: 0.6920593976974487\n",
            "590 val_loss: 0.693000078201294, train_loss: 0.6920377612113953\n",
            "600 val_loss: 0.6929676532745361, train_loss: 0.6920087337493896\n",
            "610 val_loss: 0.6929473876953125, train_loss: 0.6919886469841003\n",
            "620 val_loss: 0.6929133534431458, train_loss: 0.6919660568237305\n",
            "630 val_loss: 0.6928979158401489, train_loss: 0.6919482350349426\n",
            "640 val_loss: 0.6928694844245911, train_loss: 0.6919345259666443\n",
            "650 val_loss: 0.6928446888923645, train_loss: 0.69191974401474\n",
            "660 val_loss: 0.6928279399871826, train_loss: 0.6919099688529968\n",
            "670 val_loss: 0.6928048133850098, train_loss: 0.6918991208076477\n",
            "680 val_loss: 0.6927871108055115, train_loss: 0.6918908953666687\n",
            "690 val_loss: 0.6927605867385864, train_loss: 0.6918773055076599\n",
            "700 val_loss: 0.6927416920661926, train_loss: 0.6918665766716003\n",
            "710 val_loss: 0.692712128162384, train_loss: 0.6918469071388245\n",
            "720 val_loss: 0.6926930546760559, train_loss: 0.691831111907959\n",
            "730 val_loss: 0.6926857233047485, train_loss: 0.6918221116065979\n",
            "740 val_loss: 0.6926693320274353, train_loss: 0.6918085813522339\n",
            "750 val_loss: 0.6926559209823608, train_loss: 0.6917937397956848\n",
            "760 val_loss: 0.6926453113555908, train_loss: 0.6917803883552551\n",
            "770 val_loss: 0.6926349401473999, train_loss: 0.6917728781700134\n",
            "780 val_loss: 0.692615270614624, train_loss: 0.6917589902877808\n",
            "790 val_loss: 0.6926082968711853, train_loss: 0.6917476058006287\n",
            "800 val_loss: 0.6925990581512451, train_loss: 0.6917323470115662\n",
            "810 val_loss: 0.6925764679908752, train_loss: 0.6917212605476379\n",
            "820 val_loss: 0.6925652027130127, train_loss: 0.6917086243629456\n",
            "830 val_loss: 0.69255131483078, train_loss: 0.6916897296905518\n",
            "840 val_loss: 0.6925489902496338, train_loss: 0.6916791796684265\n",
            "850 val_loss: 0.692542314529419, train_loss: 0.6916695237159729\n",
            "860 val_loss: 0.6925341486930847, train_loss: 0.6916505694389343\n",
            "870 val_loss: 0.6925248503684998, train_loss: 0.6916411519050598\n",
            "880 val_loss: 0.6925165057182312, train_loss: 0.691631019115448\n",
            "890 val_loss: 0.6925035119056702, train_loss: 0.6916178464889526\n",
            "900 val_loss: 0.692492663860321, train_loss: 0.6916003823280334\n",
            "910 val_loss: 0.6924899220466614, train_loss: 0.6915880441665649\n",
            "920 val_loss: 0.6924691200256348, train_loss: 0.6915712356567383\n",
            "930 val_loss: 0.6924639940261841, train_loss: 0.6915575861930847\n",
            "940 val_loss: 0.692450225353241, train_loss: 0.6915435194969177\n",
            "950 val_loss: 0.6924433708190918, train_loss: 0.6915321946144104\n",
            "960 val_loss: 0.6924261450767517, train_loss: 0.6915150880813599\n",
            "970 val_loss: 0.6924098134040833, train_loss: 0.6914975047111511\n",
            "980 val_loss: 0.692395806312561, train_loss: 0.6914870738983154\n",
            "990 val_loss: 0.6923810243606567, train_loss: 0.6914725303649902\n",
            "1000 val_loss: 0.6923654079437256, train_loss: 0.6914576888084412\n",
            "1010 val_loss: 0.6923540234565735, train_loss: 0.6914436221122742\n",
            "1020 val_loss: 0.692337155342102, train_loss: 0.6914286017417908\n",
            "1030 val_loss: 0.6923214197158813, train_loss: 0.6914095878601074\n",
            "1040 val_loss: 0.6923008561134338, train_loss: 0.6913905143737793\n",
            "1050 val_loss: 0.6922875046730042, train_loss: 0.6913709044456482\n",
            "1060 val_loss: 0.69227534532547, train_loss: 0.6913602352142334\n",
            "1070 val_loss: 0.6922657489776611, train_loss: 0.6913480758666992\n",
            "1080 val_loss: 0.6922522783279419, train_loss: 0.6913310289382935\n",
            "1090 val_loss: 0.6922390460968018, train_loss: 0.6913096904754639\n",
            "1100 val_loss: 0.6922286152839661, train_loss: 0.6912931203842163\n",
            "1110 val_loss: 0.6922094821929932, train_loss: 0.6912749409675598\n",
            "1120 val_loss: 0.6921989917755127, train_loss: 0.6912592649459839\n",
            "1130 val_loss: 0.6921839714050293, train_loss: 0.6912394762039185\n",
            "1140 val_loss: 0.692175567150116, train_loss: 0.6912232041358948\n",
            "1150 val_loss: 0.6921657919883728, train_loss: 0.6912100911140442\n",
            "1160 val_loss: 0.6921536326408386, train_loss: 0.691197395324707\n",
            "1170 val_loss: 0.6921393275260925, train_loss: 0.6911804676055908\n",
            "1180 val_loss: 0.692124605178833, train_loss: 0.6911643743515015\n",
            "1190 val_loss: 0.692119300365448, train_loss: 0.6911507248878479\n",
            "1200 val_loss: 0.692103385925293, train_loss: 0.6911296248435974\n",
            "1210 val_loss: 0.6920764446258545, train_loss: 0.6911066174507141\n",
            "1220 val_loss: 0.6920592784881592, train_loss: 0.6910876035690308\n",
            "1230 val_loss: 0.6920510530471802, train_loss: 0.6910756826400757\n",
            "1240 val_loss: 0.6920400857925415, train_loss: 0.691062867641449\n",
            "1250 val_loss: 0.692030131816864, train_loss: 0.691041886806488\n",
            "1260 val_loss: 0.6920132637023926, train_loss: 0.691021203994751\n",
            "1270 val_loss: 0.6919946670532227, train_loss: 0.6909990310668945\n",
            "1280 val_loss: 0.6919823884963989, train_loss: 0.6909778118133545\n",
            "1290 val_loss: 0.6919683814048767, train_loss: 0.6909545063972473\n",
            "1300 val_loss: 0.6919470429420471, train_loss: 0.6909279227256775\n",
            "1310 val_loss: 0.6919277310371399, train_loss: 0.6909014582633972\n",
            "1320 val_loss: 0.6919124722480774, train_loss: 0.6908788084983826\n",
            "1330 val_loss: 0.6919035911560059, train_loss: 0.6908684372901917\n",
            "1340 val_loss: 0.6918809413909912, train_loss: 0.6908456683158875\n",
            "1350 val_loss: 0.6918681859970093, train_loss: 0.6908279657363892\n",
            "1360 val_loss: 0.6918532848358154, train_loss: 0.690807580947876\n",
            "1370 val_loss: 0.691830575466156, train_loss: 0.6907796859741211\n",
            "1380 val_loss: 0.6918042302131653, train_loss: 0.6907513737678528\n",
            "1390 val_loss: 0.6917850971221924, train_loss: 0.6907297968864441\n",
            "1400 val_loss: 0.6917664408683777, train_loss: 0.6907028555870056\n",
            "1410 val_loss: 0.6917487382888794, train_loss: 0.6906896233558655\n",
            "1420 val_loss: 0.6917386054992676, train_loss: 0.6906679272651672\n",
            "1430 val_loss: 0.6917333602905273, train_loss: 0.6906461119651794\n",
            "1440 val_loss: 0.691713273525238, train_loss: 0.6906196475028992\n",
            "1450 val_loss: 0.6916943788528442, train_loss: 0.6905945539474487\n",
            "1460 val_loss: 0.6916840672492981, train_loss: 0.6905768513679504\n",
            "1470 val_loss: 0.6916686296463013, train_loss: 0.690553605556488\n",
            "1480 val_loss: 0.6916513442993164, train_loss: 0.6905263662338257\n",
            "1490 val_loss: 0.6916332244873047, train_loss: 0.6905019283294678\n",
            "1500 val_loss: 0.6916108727455139, train_loss: 0.6904690265655518\n",
            "1510 val_loss: 0.6915917992591858, train_loss: 0.6904383897781372\n",
            "1520 val_loss: 0.6915658116340637, train_loss: 0.6904080510139465\n",
            "1530 val_loss: 0.6915551424026489, train_loss: 0.6903879642486572\n",
            "1540 val_loss: 0.6915367841720581, train_loss: 0.6903632879257202\n",
            "1550 val_loss: 0.6915205121040344, train_loss: 0.6903323531150818\n",
            "1560 val_loss: 0.6914879083633423, train_loss: 0.6902968883514404\n",
            "1570 val_loss: 0.691451370716095, train_loss: 0.6902621388435364\n",
            "1580 val_loss: 0.6914345026016235, train_loss: 0.6902385354042053\n",
            "1590 val_loss: 0.6914202570915222, train_loss: 0.6902155876159668\n",
            "1600 val_loss: 0.6914083361625671, train_loss: 0.6901975870132446\n",
            "1610 val_loss: 0.6913882493972778, train_loss: 0.6901669502258301\n",
            "1620 val_loss: 0.6913662552833557, train_loss: 0.6901354193687439\n",
            "1630 val_loss: 0.6913472414016724, train_loss: 0.6901125311851501\n",
            "1640 val_loss: 0.6913257837295532, train_loss: 0.6900789141654968\n",
            "1650 val_loss: 0.6913008689880371, train_loss: 0.6900506019592285\n",
            "1660 val_loss: 0.6912813186645508, train_loss: 0.6900215744972229\n",
            "1670 val_loss: 0.6912556290626526, train_loss: 0.689994752407074\n",
            "1680 val_loss: 0.6912278532981873, train_loss: 0.6899574398994446\n",
            "1690 val_loss: 0.6912108063697815, train_loss: 0.6899300217628479\n",
            "1700 val_loss: 0.6911808848381042, train_loss: 0.6898971796035767\n",
            "1710 val_loss: 0.6911530494689941, train_loss: 0.6898673176765442\n",
            "1720 val_loss: 0.6911274790763855, train_loss: 0.6898372173309326\n",
            "1730 val_loss: 0.6911112666130066, train_loss: 0.689805269241333\n",
            "1740 val_loss: 0.6910884976387024, train_loss: 0.6897783279418945\n",
            "1750 val_loss: 0.6910658478736877, train_loss: 0.6897458434104919\n",
            "1760 val_loss: 0.6910428404808044, train_loss: 0.6897184252738953\n",
            "1770 val_loss: 0.691013753414154, train_loss: 0.6896800398826599\n",
            "1780 val_loss: 0.69099360704422, train_loss: 0.6896454095840454\n",
            "1790 val_loss: 0.6909670829772949, train_loss: 0.6896083354949951\n",
            "1800 val_loss: 0.6909477710723877, train_loss: 0.6895802021026611\n",
            "1810 val_loss: 0.6909187436103821, train_loss: 0.6895432472229004\n",
            "1820 val_loss: 0.6908948421478271, train_loss: 0.6895135641098022\n",
            "1830 val_loss: 0.6908633708953857, train_loss: 0.6894843578338623\n",
            "1840 val_loss: 0.690834641456604, train_loss: 0.6894465684890747\n",
            "1850 val_loss: 0.6908021569252014, train_loss: 0.6894063949584961\n",
            "1860 val_loss: 0.690770149230957, train_loss: 0.6893640756607056\n",
            "1870 val_loss: 0.6907359957695007, train_loss: 0.6893229484558105\n",
            "1880 val_loss: 0.6907066106796265, train_loss: 0.6892911195755005\n",
            "1890 val_loss: 0.6906760334968567, train_loss: 0.6892403364181519\n",
            "1900 val_loss: 0.6906419396400452, train_loss: 0.689196765422821\n",
            "1910 val_loss: 0.6906115412712097, train_loss: 0.6891580820083618\n",
            "1920 val_loss: 0.690579891204834, train_loss: 0.6891138553619385\n",
            "1930 val_loss: 0.6905508041381836, train_loss: 0.6890732049942017\n",
            "1940 val_loss: 0.6905280351638794, train_loss: 0.689039409160614\n",
            "1950 val_loss: 0.690485417842865, train_loss: 0.6889907121658325\n",
            "1960 val_loss: 0.6904513835906982, train_loss: 0.6889459490776062\n",
            "1970 val_loss: 0.6904171705245972, train_loss: 0.6889020204544067\n",
            "1980 val_loss: 0.6903876066207886, train_loss: 0.6888641119003296\n",
            "1990 val_loss: 0.6903533339500427, train_loss: 0.6888247132301331\n",
            "2000 val_loss: 0.6903262138366699, train_loss: 0.6887895464897156\n",
            "2010 val_loss: 0.6902928352355957, train_loss: 0.6887494325637817\n",
            "2020 val_loss: 0.690255343914032, train_loss: 0.6887064576148987\n",
            "2030 val_loss: 0.6902247667312622, train_loss: 0.6886594295501709\n",
            "2040 val_loss: 0.6901849508285522, train_loss: 0.688621997833252\n",
            "2050 val_loss: 0.6901481747627258, train_loss: 0.6885727643966675\n",
            "2060 val_loss: 0.690118134021759, train_loss: 0.6885309815406799\n",
            "2070 val_loss: 0.6900849342346191, train_loss: 0.6884889602661133\n",
            "2080 val_loss: 0.6900469660758972, train_loss: 0.6884300112724304\n",
            "2090 val_loss: 0.6900128722190857, train_loss: 0.6883761286735535\n",
            "2100 val_loss: 0.6899760961532593, train_loss: 0.6883323192596436\n",
            "2110 val_loss: 0.6899369359016418, train_loss: 0.6882836222648621\n",
            "2120 val_loss: 0.6898897886276245, train_loss: 0.6882250308990479\n",
            "2130 val_loss: 0.6898581981658936, train_loss: 0.6881718635559082\n",
            "2140 val_loss: 0.689824640750885, train_loss: 0.6881197690963745\n",
            "2150 val_loss: 0.6897708177566528, train_loss: 0.6880603432655334\n",
            "2160 val_loss: 0.6897329688072205, train_loss: 0.6880126595497131\n",
            "2170 val_loss: 0.6896950006484985, train_loss: 0.6879717707633972\n",
            "2180 val_loss: 0.6896466612815857, train_loss: 0.6879116892814636\n",
            "2190 val_loss: 0.689594566822052, train_loss: 0.6878496408462524\n",
            "2200 val_loss: 0.6895480751991272, train_loss: 0.6877922415733337\n",
            "2210 val_loss: 0.689507007598877, train_loss: 0.6877380609512329\n",
            "2220 val_loss: 0.6894561052322388, train_loss: 0.6876806020736694\n",
            "2230 val_loss: 0.6894232630729675, train_loss: 0.687635600566864\n",
            "2240 val_loss: 0.6893734931945801, train_loss: 0.687575101852417\n",
            "2250 val_loss: 0.689323365688324, train_loss: 0.6875090599060059\n",
            "2260 val_loss: 0.6892748475074768, train_loss: 0.6874425411224365\n",
            "2270 val_loss: 0.6892293691635132, train_loss: 0.6873778700828552\n",
            "2280 val_loss: 0.689180850982666, train_loss: 0.6873236894607544\n",
            "2290 val_loss: 0.6891309022903442, train_loss: 0.6872603893280029\n",
            "2300 val_loss: 0.6890899538993835, train_loss: 0.6872100830078125\n",
            "2310 val_loss: 0.689030110836029, train_loss: 0.6871386766433716\n",
            "2320 val_loss: 0.68897545337677, train_loss: 0.6870733499526978\n",
            "2330 val_loss: 0.6889289617538452, train_loss: 0.6870109438896179\n",
            "2340 val_loss: 0.6888787150382996, train_loss: 0.6869519352912903\n",
            "2350 val_loss: 0.6888189315795898, train_loss: 0.6868777871131897\n",
            "2360 val_loss: 0.6887589693069458, train_loss: 0.6868054270744324\n",
            "2370 val_loss: 0.6886999607086182, train_loss: 0.6867334842681885\n",
            "2380 val_loss: 0.6886395812034607, train_loss: 0.6866650581359863\n",
            "2390 val_loss: 0.6885777711868286, train_loss: 0.6865783929824829\n",
            "2400 val_loss: 0.688523530960083, train_loss: 0.6865065693855286\n",
            "2410 val_loss: 0.6884713172912598, train_loss: 0.6864407658576965\n",
            "2420 val_loss: 0.6884165406227112, train_loss: 0.6863728165626526\n",
            "2430 val_loss: 0.6883531212806702, train_loss: 0.6862990856170654\n",
            "2440 val_loss: 0.6882927417755127, train_loss: 0.686225414276123\n",
            "2450 val_loss: 0.6882333755493164, train_loss: 0.6861626505851746\n",
            "2460 val_loss: 0.6881773471832275, train_loss: 0.6860947012901306\n",
            "2470 val_loss: 0.6880994439125061, train_loss: 0.6860064268112183\n",
            "2480 val_loss: 0.6880313754081726, train_loss: 0.685917317867279\n",
            "2490 val_loss: 0.6879633665084839, train_loss: 0.6858202219009399\n",
            "2500 val_loss: 0.6878875494003296, train_loss: 0.68573397397995\n",
            "2510 val_loss: 0.6878228187561035, train_loss: 0.6856511235237122\n",
            "2520 val_loss: 0.6877405047416687, train_loss: 0.6855527758598328\n",
            "2530 val_loss: 0.6876649260520935, train_loss: 0.6854680180549622\n",
            "2540 val_loss: 0.6876072883605957, train_loss: 0.6853936910629272\n",
            "2550 val_loss: 0.6875323057174683, train_loss: 0.6853064894676208\n",
            "2560 val_loss: 0.6874615550041199, train_loss: 0.6852167248725891\n",
            "2570 val_loss: 0.687403678894043, train_loss: 0.6851423382759094\n",
            "2580 val_loss: 0.6873351335525513, train_loss: 0.6850600242614746\n",
            "2590 val_loss: 0.6872660517692566, train_loss: 0.684979259967804\n",
            "2600 val_loss: 0.6871905326843262, train_loss: 0.6849002242088318\n",
            "2610 val_loss: 0.6871055960655212, train_loss: 0.6847993731498718\n",
            "2620 val_loss: 0.6870302557945251, train_loss: 0.6847106218338013\n",
            "2630 val_loss: 0.6869336366653442, train_loss: 0.6846033334732056\n",
            "2640 val_loss: 0.6868551969528198, train_loss: 0.6845070719718933\n",
            "2650 val_loss: 0.6867612600326538, train_loss: 0.6844019889831543\n",
            "2660 val_loss: 0.686683714389801, train_loss: 0.6843072772026062\n",
            "2670 val_loss: 0.6866061091423035, train_loss: 0.6842181086540222\n",
            "2680 val_loss: 0.6865079998970032, train_loss: 0.6841115951538086\n",
            "2690 val_loss: 0.6864218711853027, train_loss: 0.6840079426765442\n",
            "2700 val_loss: 0.6863453984260559, train_loss: 0.6839150190353394\n",
            "2710 val_loss: 0.6862525939941406, train_loss: 0.6838110089302063\n",
            "2720 val_loss: 0.6861550211906433, train_loss: 0.6836996078491211\n",
            "2730 val_loss: 0.6860664486885071, train_loss: 0.683595597743988\n",
            "2740 val_loss: 0.685973048210144, train_loss: 0.6834867000579834\n",
            "2750 val_loss: 0.6858584880828857, train_loss: 0.6833574771881104\n",
            "2760 val_loss: 0.6857647895812988, train_loss: 0.683245062828064\n",
            "2770 val_loss: 0.6856768727302551, train_loss: 0.6831390857696533\n",
            "2780 val_loss: 0.6855992078781128, train_loss: 0.6830434203147888\n",
            "2790 val_loss: 0.6854842901229858, train_loss: 0.6829075813293457\n",
            "2800 val_loss: 0.6853795051574707, train_loss: 0.6827861070632935\n",
            "2810 val_loss: 0.6852767467498779, train_loss: 0.682673990726471\n",
            "2820 val_loss: 0.6851783990859985, train_loss: 0.6825531721115112\n",
            "2830 val_loss: 0.6850837469100952, train_loss: 0.6824401617050171\n",
            "2840 val_loss: 0.6849681735038757, train_loss: 0.6823154091835022\n",
            "2850 val_loss: 0.6848772764205933, train_loss: 0.6822054982185364\n",
            "2860 val_loss: 0.6847710013389587, train_loss: 0.6820845603942871\n",
            "2870 val_loss: 0.6846726536750793, train_loss: 0.6819612383842468\n",
            "2880 val_loss: 0.6845801472663879, train_loss: 0.6818427443504333\n",
            "2890 val_loss: 0.6844708323478699, train_loss: 0.6817184686660767\n",
            "2900 val_loss: 0.6843516230583191, train_loss: 0.6815798282623291\n",
            "2910 val_loss: 0.6842455863952637, train_loss: 0.6814537048339844\n",
            "2920 val_loss: 0.6841077208518982, train_loss: 0.6813085675239563\n",
            "2930 val_loss: 0.6839762926101685, train_loss: 0.6811673045158386\n",
            "2940 val_loss: 0.6838461756706238, train_loss: 0.681024968624115\n",
            "2950 val_loss: 0.683715283870697, train_loss: 0.6808768510818481\n",
            "2960 val_loss: 0.6835989952087402, train_loss: 0.6807429194450378\n",
            "2970 val_loss: 0.6834843754768372, train_loss: 0.6806021928787231\n",
            "2980 val_loss: 0.6833773255348206, train_loss: 0.6804822087287903\n",
            "2990 val_loss: 0.6832519769668579, train_loss: 0.6803327202796936\n",
            "3000 val_loss: 0.6831033825874329, train_loss: 0.680171012878418\n",
            "3010 val_loss: 0.6829630136489868, train_loss: 0.6800069212913513\n",
            "3020 val_loss: 0.6828272342681885, train_loss: 0.6798508167266846\n",
            "3030 val_loss: 0.6827050447463989, train_loss: 0.6797124743461609\n",
            "3040 val_loss: 0.682545006275177, train_loss: 0.6795381903648376\n",
            "3050 val_loss: 0.6823950409889221, train_loss: 0.6793765425682068\n",
            "3060 val_loss: 0.6822415590286255, train_loss: 0.6792035102844238\n",
            "3070 val_loss: 0.6820880770683289, train_loss: 0.6790255904197693\n",
            "3080 val_loss: 0.681921660900116, train_loss: 0.6788455247879028\n",
            "3090 val_loss: 0.6817739009857178, train_loss: 0.6786760687828064\n",
            "3100 val_loss: 0.6816222071647644, train_loss: 0.6785080432891846\n",
            "3110 val_loss: 0.6814612746238708, train_loss: 0.6783158183097839\n",
            "3120 val_loss: 0.6813122034072876, train_loss: 0.6781452894210815\n",
            "3130 val_loss: 0.6811831593513489, train_loss: 0.6779947876930237\n",
            "3140 val_loss: 0.6810246109962463, train_loss: 0.6778190732002258\n",
            "3150 val_loss: 0.680870771408081, train_loss: 0.6776410937309265\n",
            "3160 val_loss: 0.6806967854499817, train_loss: 0.6774454712867737\n",
            "3170 val_loss: 0.6805256605148315, train_loss: 0.6772506833076477\n",
            "3180 val_loss: 0.6803719401359558, train_loss: 0.6770814657211304\n",
            "3190 val_loss: 0.6801813244819641, train_loss: 0.6768823862075806\n",
            "3200 val_loss: 0.6800089478492737, train_loss: 0.6766889691352844\n",
            "3210 val_loss: 0.6798503994941711, train_loss: 0.6764976382255554\n",
            "3220 val_loss: 0.6796746253967285, train_loss: 0.6763023138046265\n",
            "3230 val_loss: 0.6794949173927307, train_loss: 0.6761065125465393\n",
            "3240 val_loss: 0.6793080568313599, train_loss: 0.6758890748023987\n",
            "3250 val_loss: 0.6791293621063232, train_loss: 0.6756976246833801\n",
            "3260 val_loss: 0.6789529919624329, train_loss: 0.6755017042160034\n",
            "3270 val_loss: 0.6787773370742798, train_loss: 0.675298273563385\n",
            "3280 val_loss: 0.6785944104194641, train_loss: 0.6750997304916382\n",
            "3290 val_loss: 0.6784395575523376, train_loss: 0.6749128103256226\n",
            "3300 val_loss: 0.6782531142234802, train_loss: 0.6746999025344849\n",
            "3310 val_loss: 0.6780585050582886, train_loss: 0.6744892597198486\n",
            "3320 val_loss: 0.6778663396835327, train_loss: 0.6742731332778931\n",
            "3330 val_loss: 0.677655816078186, train_loss: 0.6740437746047974\n",
            "3340 val_loss: 0.6774687767028809, train_loss: 0.6738354563713074\n",
            "3350 val_loss: 0.6772563457489014, train_loss: 0.673606812953949\n",
            "3360 val_loss: 0.6770445108413696, train_loss: 0.673368513584137\n",
            "3370 val_loss: 0.676861047744751, train_loss: 0.673149049282074\n",
            "3380 val_loss: 0.6766699552536011, train_loss: 0.6729339361190796\n",
            "3390 val_loss: 0.6764528751373291, train_loss: 0.6726897954940796\n",
            "3400 val_loss: 0.6762309074401855, train_loss: 0.6724373698234558\n",
            "3410 val_loss: 0.676018238067627, train_loss: 0.6722002625465393\n",
            "3420 val_loss: 0.6757911443710327, train_loss: 0.6719452738761902\n",
            "3430 val_loss: 0.6755855679512024, train_loss: 0.6716957092285156\n",
            "3440 val_loss: 0.6753716468811035, train_loss: 0.6714497804641724\n",
            "3450 val_loss: 0.6751528382301331, train_loss: 0.6712146401405334\n",
            "3460 val_loss: 0.6748560667037964, train_loss: 0.6709063053131104\n",
            "3470 val_loss: 0.6746337413787842, train_loss: 0.6706537008285522\n",
            "3480 val_loss: 0.6744109392166138, train_loss: 0.6703832149505615\n",
            "3490 val_loss: 0.6741726994514465, train_loss: 0.6701153516769409\n",
            "3500 val_loss: 0.6739147305488586, train_loss: 0.6698322892189026\n",
            "3510 val_loss: 0.6736195683479309, train_loss: 0.6695502996444702\n",
            "3520 val_loss: 0.6733887791633606, train_loss: 0.6692769527435303\n",
            "3530 val_loss: 0.6731671690940857, train_loss: 0.6690343618392944\n",
            "3540 val_loss: 0.6729121804237366, train_loss: 0.6687579154968262\n",
            "3550 val_loss: 0.6726462244987488, train_loss: 0.6684806942939758\n",
            "3560 val_loss: 0.672343373298645, train_loss: 0.6681731939315796\n",
            "3570 val_loss: 0.6720587015151978, train_loss: 0.6678757667541504\n",
            "3580 val_loss: 0.671818733215332, train_loss: 0.6676129102706909\n",
            "3590 val_loss: 0.6715661287307739, train_loss: 0.6673434376716614\n",
            "3600 val_loss: 0.6712918281555176, train_loss: 0.6670634746551514\n",
            "3610 val_loss: 0.671053409576416, train_loss: 0.6667892336845398\n",
            "3620 val_loss: 0.670801043510437, train_loss: 0.6664988994598389\n",
            "3630 val_loss: 0.670545756816864, train_loss: 0.6662237048149109\n",
            "3640 val_loss: 0.6703036427497864, train_loss: 0.665956974029541\n",
            "3650 val_loss: 0.6700651049613953, train_loss: 0.665662407875061\n",
            "3660 val_loss: 0.6698288321495056, train_loss: 0.6654136776924133\n",
            "3670 val_loss: 0.6695912480354309, train_loss: 0.665126383304596\n",
            "3680 val_loss: 0.6693409085273743, train_loss: 0.6648737788200378\n",
            "3690 val_loss: 0.6690571308135986, train_loss: 0.6645723581314087\n",
            "3700 val_loss: 0.6687919497489929, train_loss: 0.6642923951148987\n",
            "3710 val_loss: 0.6685003638267517, train_loss: 0.6639950275421143\n",
            "3720 val_loss: 0.6682397127151489, train_loss: 0.6637044548988342\n",
            "3730 val_loss: 0.6679790019989014, train_loss: 0.6634131669998169\n",
            "3740 val_loss: 0.6676880121231079, train_loss: 0.6631130576133728\n",
            "3750 val_loss: 0.6673739552497864, train_loss: 0.6627768874168396\n",
            "3760 val_loss: 0.6670908331871033, train_loss: 0.6624619364738464\n",
            "3770 val_loss: 0.6667701005935669, train_loss: 0.6621515154838562\n",
            "3780 val_loss: 0.6664502024650574, train_loss: 0.6618155837059021\n",
            "3790 val_loss: 0.6661173701286316, train_loss: 0.6614658832550049\n",
            "3800 val_loss: 0.6658028364181519, train_loss: 0.661128580570221\n",
            "3810 val_loss: 0.6654982566833496, train_loss: 0.6607755422592163\n",
            "3820 val_loss: 0.6652161478996277, train_loss: 0.6604657769203186\n",
            "3830 val_loss: 0.6649101972579956, train_loss: 0.6601290106773376\n",
            "3840 val_loss: 0.6645761132240295, train_loss: 0.6597516536712646\n",
            "3850 val_loss: 0.6642141342163086, train_loss: 0.6594286561012268\n",
            "3860 val_loss: 0.6638185381889343, train_loss: 0.6590583920478821\n",
            "3870 val_loss: 0.6634661555290222, train_loss: 0.658698320388794\n",
            "3880 val_loss: 0.6632086634635925, train_loss: 0.658420741558075\n",
            "3890 val_loss: 0.6628998517990112, train_loss: 0.6580801010131836\n",
            "3900 val_loss: 0.6625658869743347, train_loss: 0.6577336192131042\n",
            "3910 val_loss: 0.6622132658958435, train_loss: 0.6573731303215027\n",
            "3920 val_loss: 0.6619254350662231, train_loss: 0.6570903062820435\n",
            "3930 val_loss: 0.6615701913833618, train_loss: 0.656733512878418\n",
            "3940 val_loss: 0.661217212677002, train_loss: 0.6563575267791748\n",
            "3950 val_loss: 0.6608832478523254, train_loss: 0.6560207009315491\n",
            "3960 val_loss: 0.6605696678161621, train_loss: 0.6557055711746216\n",
            "3970 val_loss: 0.6602540016174316, train_loss: 0.655366063117981\n",
            "3980 val_loss: 0.6599708199501038, train_loss: 0.6550473570823669\n",
            "3990 val_loss: 0.6595888137817383, train_loss: 0.6546609997749329\n",
            "4000 val_loss: 0.6592937707901001, train_loss: 0.6543227434158325\n",
            "4010 val_loss: 0.6589916944503784, train_loss: 0.6539637446403503\n",
            "4020 val_loss: 0.6585797667503357, train_loss: 0.6535127758979797\n",
            "4030 val_loss: 0.6582028269767761, train_loss: 0.6531832814216614\n",
            "4040 val_loss: 0.6578882932662964, train_loss: 0.6528133749961853\n",
            "4050 val_loss: 0.657508909702301, train_loss: 0.6524235010147095\n",
            "4060 val_loss: 0.6572394371032715, train_loss: 0.6520854234695435\n",
            "4070 val_loss: 0.6568812131881714, train_loss: 0.6517126560211182\n",
            "4080 val_loss: 0.6565251350402832, train_loss: 0.6513345837593079\n",
            "4090 val_loss: 0.6561588644981384, train_loss: 0.6509553790092468\n",
            "4100 val_loss: 0.6557793617248535, train_loss: 0.6505630016326904\n",
            "4110 val_loss: 0.6554513573646545, train_loss: 0.6502075791358948\n",
            "4120 val_loss: 0.6550891399383545, train_loss: 0.6498275399208069\n",
            "4130 val_loss: 0.6546616554260254, train_loss: 0.6493868827819824\n",
            "4140 val_loss: 0.654266893863678, train_loss: 0.6490181684494019\n",
            "4150 val_loss: 0.6538734436035156, train_loss: 0.6486322283744812\n",
            "4160 val_loss: 0.6535239219665527, train_loss: 0.648293137550354\n",
            "4170 val_loss: 0.6531838178634644, train_loss: 0.6479161977767944\n",
            "4180 val_loss: 0.6528518199920654, train_loss: 0.6475247144699097\n",
            "4190 val_loss: 0.6524460315704346, train_loss: 0.647127091884613\n",
            "4200 val_loss: 0.6520774364471436, train_loss: 0.6467592716217041\n",
            "4210 val_loss: 0.651740550994873, train_loss: 0.6464241147041321\n",
            "4220 val_loss: 0.6513009071350098, train_loss: 0.6460091471672058\n",
            "4230 val_loss: 0.6509673595428467, train_loss: 0.6456148028373718\n",
            "4240 val_loss: 0.6506623029708862, train_loss: 0.6452759504318237\n",
            "4250 val_loss: 0.6502701044082642, train_loss: 0.6448831558227539\n",
            "4260 val_loss: 0.6498813629150391, train_loss: 0.6445529460906982\n",
            "4270 val_loss: 0.649503231048584, train_loss: 0.644208550453186\n",
            "4280 val_loss: 0.6492449641227722, train_loss: 0.6439152956008911\n",
            "4290 val_loss: 0.6488741636276245, train_loss: 0.6436085104942322\n",
            "4300 val_loss: 0.6484020948410034, train_loss: 0.6431658267974854\n",
            "4310 val_loss: 0.6479979157447815, train_loss: 0.6427830457687378\n",
            "4320 val_loss: 0.647619903087616, train_loss: 0.6424037218093872\n",
            "4330 val_loss: 0.6472945809364319, train_loss: 0.6420552730560303\n",
            "4340 val_loss: 0.6469590067863464, train_loss: 0.6417323350906372\n",
            "4350 val_loss: 0.6466874480247498, train_loss: 0.6414711475372314\n",
            "4360 val_loss: 0.6462952494621277, train_loss: 0.6410967111587524\n",
            "4370 val_loss: 0.6458495259284973, train_loss: 0.6407297253608704\n",
            "4380 val_loss: 0.6455853581428528, train_loss: 0.640363335609436\n",
            "4390 val_loss: 0.6451488137245178, train_loss: 0.6399995684623718\n",
            "4400 val_loss: 0.6448283791542053, train_loss: 0.639696478843689\n",
            "4410 val_loss: 0.6444969773292542, train_loss: 0.6393244862556458\n",
            "4420 val_loss: 0.6441681385040283, train_loss: 0.6390001177787781\n",
            "4430 val_loss: 0.6437085270881653, train_loss: 0.6385906934738159\n",
            "4440 val_loss: 0.6434256434440613, train_loss: 0.6383084654808044\n",
            "4450 val_loss: 0.6430193781852722, train_loss: 0.6379140615463257\n",
            "4460 val_loss: 0.6426339745521545, train_loss: 0.637499213218689\n",
            "4470 val_loss: 0.6422685384750366, train_loss: 0.6371474862098694\n",
            "4480 val_loss: 0.6419422626495361, train_loss: 0.6367800235748291\n",
            "4490 val_loss: 0.6416324973106384, train_loss: 0.6364262104034424\n",
            "4500 val_loss: 0.6412345767021179, train_loss: 0.6360344290733337\n",
            "4510 val_loss: 0.640893280506134, train_loss: 0.6357014179229736\n",
            "4520 val_loss: 0.6406243443489075, train_loss: 0.635438859462738\n",
            "4530 val_loss: 0.6402496099472046, train_loss: 0.6351349353790283\n",
            "4540 val_loss: 0.6397803425788879, train_loss: 0.6347197890281677\n",
            "4550 val_loss: 0.6394184231758118, train_loss: 0.6343712210655212\n",
            "4560 val_loss: 0.6389883756637573, train_loss: 0.6339492201805115\n",
            "4570 val_loss: 0.6385979652404785, train_loss: 0.6335861682891846\n",
            "4580 val_loss: 0.6383095979690552, train_loss: 0.6333737373352051\n",
            "4590 val_loss: 0.6378412842750549, train_loss: 0.6329628229141235\n",
            "4600 val_loss: 0.6375012397766113, train_loss: 0.6325554251670837\n",
            "4610 val_loss: 0.6371413469314575, train_loss: 0.632283091545105\n",
            "4620 val_loss: 0.6368339657783508, train_loss: 0.6319414377212524\n",
            "4630 val_loss: 0.6364619135856628, train_loss: 0.6315850615501404\n",
            "4640 val_loss: 0.6360760927200317, train_loss: 0.6312009692192078\n",
            "4650 val_loss: 0.635509192943573, train_loss: 0.6308238506317139\n",
            "4660 val_loss: 0.6352477669715881, train_loss: 0.6305866837501526\n",
            "4670 val_loss: 0.6348550915718079, train_loss: 0.6302743554115295\n",
            "4680 val_loss: 0.634522557258606, train_loss: 0.6299237012863159\n",
            "4690 val_loss: 0.6341801285743713, train_loss: 0.6295885443687439\n",
            "4700 val_loss: 0.634026050567627, train_loss: 0.629353404045105\n",
            "4710 val_loss: 0.6337133049964905, train_loss: 0.6289740800857544\n",
            "4720 val_loss: 0.6332865357398987, train_loss: 0.6286162734031677\n",
            "4730 val_loss: 0.6329030394554138, train_loss: 0.6282593607902527\n",
            "4740 val_loss: 0.6325415372848511, train_loss: 0.627913236618042\n",
            "4750 val_loss: 0.6322413682937622, train_loss: 0.6276620030403137\n",
            "4760 val_loss: 0.6319000124931335, train_loss: 0.6273483633995056\n",
            "4770 val_loss: 0.6316350698471069, train_loss: 0.6270868182182312\n",
            "4780 val_loss: 0.6312702298164368, train_loss: 0.6266937851905823\n",
            "4790 val_loss: 0.6310082077980042, train_loss: 0.6264171004295349\n",
            "4800 val_loss: 0.6306294202804565, train_loss: 0.626100480556488\n",
            "4810 val_loss: 0.6304202079772949, train_loss: 0.6257889866828918\n",
            "4820 val_loss: 0.630085825920105, train_loss: 0.6254781484603882\n",
            "4830 val_loss: 0.6297529935836792, train_loss: 0.6251668334007263\n",
            "4840 val_loss: 0.6294655799865723, train_loss: 0.6248778700828552\n",
            "4850 val_loss: 0.6290258765220642, train_loss: 0.6245282888412476\n",
            "4860 val_loss: 0.6285697817802429, train_loss: 0.6241620182991028\n",
            "4870 val_loss: 0.6281866431236267, train_loss: 0.623863697052002\n",
            "4880 val_loss: 0.6278535723686218, train_loss: 0.6235570907592773\n",
            "4890 val_loss: 0.6275841593742371, train_loss: 0.6232805848121643\n",
            "4900 val_loss: 0.6273034811019897, train_loss: 0.6230308413505554\n",
            "4910 val_loss: 0.6268534660339355, train_loss: 0.622675895690918\n",
            "4920 val_loss: 0.6265021562576294, train_loss: 0.6223731637001038\n",
            "4930 val_loss: 0.6263625025749207, train_loss: 0.6220873594284058\n",
            "4940 val_loss: 0.6261624097824097, train_loss: 0.621830940246582\n",
            "4950 val_loss: 0.6258133053779602, train_loss: 0.6215416193008423\n",
            "4960 val_loss: 0.6254352331161499, train_loss: 0.6212507486343384\n",
            "4970 val_loss: 0.6252071261405945, train_loss: 0.6210405826568604\n",
            "4980 val_loss: 0.6248894333839417, train_loss: 0.6207610368728638\n",
            "4990 val_loss: 0.6246770620346069, train_loss: 0.6204658150672913\n",
            "5000 val_loss: 0.6242577433586121, train_loss: 0.6201533079147339\n",
            "5010 val_loss: 0.6237369775772095, train_loss: 0.6198142170906067\n",
            "5020 val_loss: 0.6235141158103943, train_loss: 0.6194726824760437\n",
            "5030 val_loss: 0.6231625080108643, train_loss: 0.6192026138305664\n",
            "5040 val_loss: 0.6227681636810303, train_loss: 0.6188870072364807\n",
            "5050 val_loss: 0.6225457191467285, train_loss: 0.6187494993209839\n",
            "5060 val_loss: 0.6222453117370605, train_loss: 0.6185738444328308\n",
            "5070 val_loss: 0.6219573020935059, train_loss: 0.6183935403823853\n",
            "5080 val_loss: 0.6216834187507629, train_loss: 0.6181025505065918\n",
            "5090 val_loss: 0.6215617060661316, train_loss: 0.6178857684135437\n",
            "5100 val_loss: 0.6212765574455261, train_loss: 0.6176608204841614\n",
            "5110 val_loss: 0.6208785176277161, train_loss: 0.6174302101135254\n",
            "5120 val_loss: 0.6205536723136902, train_loss: 0.6172225475311279\n",
            "5130 val_loss: 0.6205281019210815, train_loss: 0.6169254779815674\n",
            "5140 val_loss: 0.6202301979064941, train_loss: 0.616701602935791\n",
            "5150 val_loss: 0.6200282573699951, train_loss: 0.6165173053741455\n",
            "5160 val_loss: 0.6198156476020813, train_loss: 0.6162906289100647\n",
            "5170 val_loss: 0.6195511817932129, train_loss: 0.6160334944725037\n",
            "5180 val_loss: 0.6192814111709595, train_loss: 0.6157220602035522\n",
            "5190 val_loss: 0.618927538394928, train_loss: 0.6153983473777771\n",
            "5200 val_loss: 0.6188479661941528, train_loss: 0.615293562412262\n",
            "5210 val_loss: 0.6184794902801514, train_loss: 0.6151230931282043\n",
            "5220 val_loss: 0.6181150078773499, train_loss: 0.6148635149002075\n",
            "5230 val_loss: 0.6179253458976746, train_loss: 0.6147159934043884\n",
            "5240 val_loss: 0.6175398230552673, train_loss: 0.6143696308135986\n",
            "5250 val_loss: 0.6173771023750305, train_loss: 0.6142217516899109\n",
            "5260 val_loss: 0.6171913146972656, train_loss: 0.6139795184135437\n",
            "5270 val_loss: 0.6168872117996216, train_loss: 0.6137323975563049\n",
            "5280 val_loss: 0.6167824864387512, train_loss: 0.6136595010757446\n",
            "5290 val_loss: 0.6166066527366638, train_loss: 0.613514244556427\n",
            "5300 val_loss: 0.6165452003479004, train_loss: 0.613371729850769\n",
            "5310 val_loss: 0.6161021590232849, train_loss: 0.6131219863891602\n",
            "5320 val_loss: 0.6156366467475891, train_loss: 0.6127740740776062\n",
            "5330 val_loss: 0.6152507066726685, train_loss: 0.6124951243400574\n",
            "5340 val_loss: 0.6150350570678711, train_loss: 0.6123208999633789\n",
            "5350 val_loss: 0.6150395274162292, train_loss: 0.6121586561203003\n",
            "5360 val_loss: 0.6147735714912415, train_loss: 0.6118996143341064\n",
            "5370 val_loss: 0.614423930644989, train_loss: 0.6116752028465271\n",
            "5380 val_loss: 0.6141225695610046, train_loss: 0.6114745140075684\n",
            "5390 val_loss: 0.6140490174293518, train_loss: 0.6113418936729431\n",
            "5400 val_loss: 0.6140183210372925, train_loss: 0.6112262010574341\n",
            "5410 val_loss: 0.6138455271720886, train_loss: 0.611025869846344\n",
            "5420 val_loss: 0.613776683807373, train_loss: 0.6109455823898315\n",
            "5430 val_loss: 0.6138899326324463, train_loss: 0.6107617020606995\n",
            "5440 val_loss: 0.6135324835777283, train_loss: 0.6106576323509216\n",
            "5450 val_loss: 0.6132705807685852, train_loss: 0.6104406118392944\n",
            "5460 val_loss: 0.6131632328033447, train_loss: 0.6102659702301025\n",
            "5470 val_loss: 0.6128105521202087, train_loss: 0.609959065914154\n",
            "5480 val_loss: 0.6126933097839355, train_loss: 0.609810471534729\n",
            "5490 val_loss: 0.6122987270355225, train_loss: 0.6095718741416931\n",
            "5500 val_loss: 0.6122496724128723, train_loss: 0.609468400478363\n",
            "5510 val_loss: 0.6120702028274536, train_loss: 0.609294593334198\n",
            "5520 val_loss: 0.6118142604827881, train_loss: 0.6090309023857117\n",
            "5530 val_loss: 0.6114562153816223, train_loss: 0.6087396144866943\n",
            "5540 val_loss: 0.6109752655029297, train_loss: 0.6083982586860657\n",
            "5550 val_loss: 0.6106830835342407, train_loss: 0.6081734299659729\n",
            "5560 val_loss: 0.6103375554084778, train_loss: 0.6079453229904175\n",
            "5570 val_loss: 0.6102282404899597, train_loss: 0.6077890396118164\n",
            "5580 val_loss: 0.6098854541778564, train_loss: 0.6075986623764038\n",
            "5590 val_loss: 0.6097257137298584, train_loss: 0.6074668169021606\n",
            "5600 val_loss: 0.609662652015686, train_loss: 0.6073017716407776\n",
            "5610 val_loss: 0.6094748377799988, train_loss: 0.6072283983230591\n",
            "5620 val_loss: 0.6092461347579956, train_loss: 0.6070858240127563\n",
            "5630 val_loss: 0.6093142032623291, train_loss: 0.6070083975791931\n",
            "5640 val_loss: 0.6090800762176514, train_loss: 0.6067920327186584\n",
            "5650 val_loss: 0.6087974309921265, train_loss: 0.6065917611122131\n",
            "5660 val_loss: 0.6087458729743958, train_loss: 0.6065469980239868\n",
            "5670 val_loss: 0.6088342666625977, train_loss: 0.6063793897628784\n",
            "5680 val_loss: 0.6083491444587708, train_loss: 0.6061356067657471\n",
            "5690 val_loss: 0.6082218885421753, train_loss: 0.6059436202049255\n",
            "5700 val_loss: 0.6079646348953247, train_loss: 0.6058782339096069\n",
            "5710 val_loss: 0.6077215075492859, train_loss: 0.6057218909263611\n",
            "5720 val_loss: 0.6073668599128723, train_loss: 0.6054142713546753\n",
            "5730 val_loss: 0.606975793838501, train_loss: 0.6051961183547974\n",
            "5740 val_loss: 0.606854259967804, train_loss: 0.6051638722419739\n",
            "5750 val_loss: 0.6068010926246643, train_loss: 0.6049778461456299\n",
            "5760 val_loss: 0.6065722703933716, train_loss: 0.6047394871711731\n",
            "5770 val_loss: 0.6063623428344727, train_loss: 0.6046431660652161\n",
            "5780 val_loss: 0.6063714623451233, train_loss: 0.6044951677322388\n",
            "5790 val_loss: 0.6063219904899597, train_loss: 0.6043784022331238\n",
            "5800 val_loss: 0.6061162352561951, train_loss: 0.6040830612182617\n",
            "5810 val_loss: 0.6058634519577026, train_loss: 0.6038650870323181\n",
            "5820 val_loss: 0.6054764986038208, train_loss: 0.6036843061447144\n",
            "5830 val_loss: 0.6054474115371704, train_loss: 0.6036405563354492\n",
            "5840 val_loss: 0.6052955389022827, train_loss: 0.6034671664237976\n",
            "5850 val_loss: 0.6052611470222473, train_loss: 0.6033222675323486\n",
            "5860 val_loss: 0.6051745414733887, train_loss: 0.6031450629234314\n",
            "5870 val_loss: 0.6049972772598267, train_loss: 0.6029499769210815\n",
            "5880 val_loss: 0.6048609018325806, train_loss: 0.6028245687484741\n",
            "5890 val_loss: 0.6046655774116516, train_loss: 0.6026393175125122\n",
            "5900 val_loss: 0.604297935962677, train_loss: 0.6024566888809204\n",
            "5910 val_loss: 0.6041351556777954, train_loss: 0.6023196578025818\n",
            "5920 val_loss: 0.6039586067199707, train_loss: 0.602107584476471\n",
            "5930 val_loss: 0.6036795377731323, train_loss: 0.6018645167350769\n",
            "5940 val_loss: 0.6035979390144348, train_loss: 0.6017791032791138\n",
            "5950 val_loss: 0.6033595204353333, train_loss: 0.60170978307724\n",
            "5960 val_loss: 0.6031261086463928, train_loss: 0.6015623211860657\n",
            "5970 val_loss: 0.6031267642974854, train_loss: 0.6014155149459839\n",
            "5980 val_loss: 0.6030212640762329, train_loss: 0.601379930973053\n",
            "5990 val_loss: 0.6028286814689636, train_loss: 0.6012709140777588\n",
            "6000 val_loss: 0.6028269529342651, train_loss: 0.6011606454849243\n",
            "6010 val_loss: 0.602933406829834, train_loss: 0.6010597944259644\n",
            "6020 val_loss: 0.6029488444328308, train_loss: 0.6009480953216553\n",
            "6030 val_loss: 0.6026135087013245, train_loss: 0.6008623838424683\n",
            "6040 val_loss: 0.6024283170700073, train_loss: 0.6006767749786377\n",
            "6050 val_loss: 0.6023871898651123, train_loss: 0.6004834175109863\n",
            "6060 val_loss: 0.6020447611808777, train_loss: 0.6003344058990479\n",
            "6070 val_loss: 0.6017912030220032, train_loss: 0.600047767162323\n",
            "6080 val_loss: 0.6015965342521667, train_loss: 0.5998185873031616\n",
            "6090 val_loss: 0.6014218330383301, train_loss: 0.5996910333633423\n",
            "6100 val_loss: 0.6013422608375549, train_loss: 0.599516749382019\n",
            "6110 val_loss: 0.601425290107727, train_loss: 0.5993648171424866\n",
            "6120 val_loss: 0.6011454463005066, train_loss: 0.5991799235343933\n",
            "6130 val_loss: 0.6009027361869812, train_loss: 0.599088728427887\n",
            "6140 val_loss: 0.6008232831954956, train_loss: 0.5990334153175354\n",
            "6150 val_loss: 0.600391149520874, train_loss: 0.5986687541007996\n",
            "6160 val_loss: 0.6004163026809692, train_loss: 0.598579466342926\n",
            "6170 val_loss: 0.599976122379303, train_loss: 0.5983620882034302\n",
            "6180 val_loss: 0.5997391939163208, train_loss: 0.5981928110122681\n",
            "6190 val_loss: 0.5997671484947205, train_loss: 0.598198652267456\n",
            "6200 val_loss: 0.5996521711349487, train_loss: 0.5980016589164734\n",
            "6210 val_loss: 0.5996999144554138, train_loss: 0.5979533791542053\n",
            "6220 val_loss: 0.5994362235069275, train_loss: 0.597766101360321\n",
            "6230 val_loss: 0.599259078502655, train_loss: 0.5977658033370972\n",
            "6240 val_loss: 0.5993134379386902, train_loss: 0.5976335406303406\n",
            "6250 val_loss: 0.5991250276565552, train_loss: 0.5974687337875366\n",
            "6260 val_loss: 0.5991584062576294, train_loss: 0.597380518913269\n",
            "6270 val_loss: 0.5989680290222168, train_loss: 0.5972079038619995\n",
            "6280 val_loss: 0.5989850163459778, train_loss: 0.5971452593803406\n",
            "6290 val_loss: 0.5987796783447266, train_loss: 0.5969545245170593\n",
            "6300 val_loss: 0.5985170006752014, train_loss: 0.5967483520507812\n",
            "6310 val_loss: 0.5983124375343323, train_loss: 0.5965422987937927\n",
            "6320 val_loss: 0.5980904698371887, train_loss: 0.5964728593826294\n",
            "6330 val_loss: 0.5980396866798401, train_loss: 0.5963512063026428\n",
            "6340 val_loss: 0.5977253913879395, train_loss: 0.5961760878562927\n",
            "6350 val_loss: 0.5976181030273438, train_loss: 0.5959292054176331\n",
            "6360 val_loss: 0.5973889827728271, train_loss: 0.5958731770515442\n",
            "6370 val_loss: 0.5972456932067871, train_loss: 0.5957441329956055\n",
            "6380 val_loss: 0.5969998240470886, train_loss: 0.5954268574714661\n",
            "6390 val_loss: 0.596823513507843, train_loss: 0.59536212682724\n",
            "6400 val_loss: 0.5968239903450012, train_loss: 0.5953572392463684\n",
            "6410 val_loss: 0.5966448187828064, train_loss: 0.5951943397521973\n",
            "6420 val_loss: 0.5965254902839661, train_loss: 0.5949563980102539\n",
            "6430 val_loss: 0.5962739586830139, train_loss: 0.5947653651237488\n",
            "6440 val_loss: 0.5964592099189758, train_loss: 0.5947219133377075\n",
            "6450 val_loss: 0.596245288848877, train_loss: 0.5945801138877869\n",
            "6460 val_loss: 0.5961722731590271, train_loss: 0.5943669080734253\n",
            "6470 val_loss: 0.5959957838058472, train_loss: 0.5942468643188477\n",
            "6480 val_loss: 0.5956457853317261, train_loss: 0.5940291881561279\n",
            "6490 val_loss: 0.5956014394760132, train_loss: 0.5938295125961304\n",
            "6500 val_loss: 0.59550541639328, train_loss: 0.5936834216117859\n",
            "6510 val_loss: 0.595312774181366, train_loss: 0.5935348272323608\n",
            "6520 val_loss: 0.5950793027877808, train_loss: 0.5933533310890198\n",
            "6530 val_loss: 0.5947885513305664, train_loss: 0.5931153297424316\n",
            "6540 val_loss: 0.5945723652839661, train_loss: 0.5930508971214294\n",
            "6550 val_loss: 0.5944182276725769, train_loss: 0.5928375720977783\n",
            "6560 val_loss: 0.5940924882888794, train_loss: 0.5926766395568848\n",
            "6570 val_loss: 0.5939753651618958, train_loss: 0.5925447344779968\n",
            "6580 val_loss: 0.5939902663230896, train_loss: 0.5924328565597534\n",
            "6590 val_loss: 0.5937662720680237, train_loss: 0.592258095741272\n",
            "6600 val_loss: 0.5936889052391052, train_loss: 0.5920707583427429\n",
            "6610 val_loss: 0.5936886072158813, train_loss: 0.5920354723930359\n",
            "6620 val_loss: 0.5935370326042175, train_loss: 0.5918643474578857\n",
            "6630 val_loss: 0.5934966206550598, train_loss: 0.5916658639907837\n",
            "6640 val_loss: 0.5936398506164551, train_loss: 0.5915749669075012\n",
            "6650 val_loss: 0.5934016108512878, train_loss: 0.5915260910987854\n",
            "6660 val_loss: 0.5931822061538696, train_loss: 0.5913406610488892\n",
            "6670 val_loss: 0.5929356217384338, train_loss: 0.5911549925804138\n",
            "6680 val_loss: 0.592841625213623, train_loss: 0.5909183025360107\n",
            "6690 val_loss: 0.5923855900764465, train_loss: 0.5906869769096375\n",
            "6700 val_loss: 0.5922929644584656, train_loss: 0.5905781984329224\n",
            "6710 val_loss: 0.5923270583152771, train_loss: 0.5905010104179382\n",
            "6720 val_loss: 0.5921155214309692, train_loss: 0.590318500995636\n",
            "6730 val_loss: 0.592037558555603, train_loss: 0.5902622938156128\n",
            "6740 val_loss: 0.5916988253593445, train_loss: 0.589998722076416\n",
            "6750 val_loss: 0.5914452075958252, train_loss: 0.5897382497787476\n",
            "6760 val_loss: 0.5913680195808411, train_loss: 0.5895848274230957\n",
            "6770 val_loss: 0.5911073088645935, train_loss: 0.5894457101821899\n",
            "6780 val_loss: 0.59095698595047, train_loss: 0.5893798470497131\n",
            "6790 val_loss: 0.5907496213912964, train_loss: 0.589179515838623\n",
            "6800 val_loss: 0.5906404256820679, train_loss: 0.5889194011688232\n",
            "6810 val_loss: 0.5905564427375793, train_loss: 0.5887634754180908\n",
            "6820 val_loss: 0.5903976559638977, train_loss: 0.5886327624320984\n",
            "6830 val_loss: 0.5904320478439331, train_loss: 0.588391125202179\n",
            "6840 val_loss: 0.5902264714241028, train_loss: 0.5881359577178955\n",
            "6850 val_loss: 0.5901223421096802, train_loss: 0.5880827903747559\n",
            "6860 val_loss: 0.5897772908210754, train_loss: 0.5877793431282043\n",
            "6870 val_loss: 0.5894010066986084, train_loss: 0.5874480605125427\n",
            "6880 val_loss: 0.5893216133117676, train_loss: 0.5873962044715881\n",
            "6890 val_loss: 0.5890825986862183, train_loss: 0.5871919393539429\n",
            "6900 val_loss: 0.5889776349067688, train_loss: 0.5870673656463623\n",
            "6910 val_loss: 0.5890358686447144, train_loss: 0.5869370698928833\n",
            "6920 val_loss: 0.5888607501983643, train_loss: 0.5867076516151428\n",
            "6930 val_loss: 0.5887089967727661, train_loss: 0.5865599513053894\n",
            "6940 val_loss: 0.5886368751525879, train_loss: 0.586461067199707\n",
            "6950 val_loss: 0.5884369611740112, train_loss: 0.5863019824028015\n",
            "6960 val_loss: 0.5883446335792542, train_loss: 0.5861758589744568\n",
            "6970 val_loss: 0.5881390571594238, train_loss: 0.5860718488693237\n",
            "6980 val_loss: 0.5880264043807983, train_loss: 0.58576899766922\n",
            "6990 val_loss: 0.5878139734268188, train_loss: 0.5854128003120422\n",
            "7000 val_loss: 0.5876692533493042, train_loss: 0.5851942300796509\n",
            "7010 val_loss: 0.5873951315879822, train_loss: 0.5851131081581116\n",
            "7020 val_loss: 0.5871762633323669, train_loss: 0.5849299430847168\n",
            "7030 val_loss: 0.5871214270591736, train_loss: 0.5847318172454834\n",
            "7040 val_loss: 0.5869765281677246, train_loss: 0.5844873189926147\n",
            "7050 val_loss: 0.5867056250572205, train_loss: 0.5842885971069336\n",
            "7060 val_loss: 0.586494505405426, train_loss: 0.5840914845466614\n",
            "7070 val_loss: 0.5863066911697388, train_loss: 0.5838852524757385\n",
            "7080 val_loss: 0.5861282348632812, train_loss: 0.5836690664291382\n",
            "7090 val_loss: 0.5859858393669128, train_loss: 0.5836304426193237\n",
            "7100 val_loss: 0.585942804813385, train_loss: 0.5835062265396118\n",
            "7110 val_loss: 0.5857594013214111, train_loss: 0.5833058953285217\n",
            "7120 val_loss: 0.5857400298118591, train_loss: 0.5832056999206543\n",
            "7130 val_loss: 0.5855915546417236, train_loss: 0.5830667018890381\n",
            "7140 val_loss: 0.5853369832038879, train_loss: 0.5827426314353943\n",
            "7150 val_loss: 0.5852323174476624, train_loss: 0.5826020240783691\n",
            "7160 val_loss: 0.5850775837898254, train_loss: 0.5823199152946472\n",
            "7170 val_loss: 0.5848053693771362, train_loss: 0.5820813179016113\n",
            "7180 val_loss: 0.584679126739502, train_loss: 0.5819636583328247\n",
            "7190 val_loss: 0.5846694111824036, train_loss: 0.5818500518798828\n",
            "7200 val_loss: 0.5843797326087952, train_loss: 0.5816831588745117\n",
            "7210 val_loss: 0.5843337774276733, train_loss: 0.5815507769584656\n",
            "7220 val_loss: 0.5839409828186035, train_loss: 0.5812165141105652\n",
            "7230 val_loss: 0.58368980884552, train_loss: 0.5808870196342468\n",
            "7240 val_loss: 0.5834605097770691, train_loss: 0.5807468295097351\n",
            "7250 val_loss: 0.5833989977836609, train_loss: 0.5804264545440674\n",
            "7260 val_loss: 0.583189845085144, train_loss: 0.5802440047264099\n",
            "7270 val_loss: 0.5829567909240723, train_loss: 0.5800737142562866\n",
            "7280 val_loss: 0.582752525806427, train_loss: 0.5799224376678467\n",
            "7290 val_loss: 0.5826051831245422, train_loss: 0.579716682434082\n",
            "7300 val_loss: 0.582348644733429, train_loss: 0.5795209407806396\n",
            "7310 val_loss: 0.5819997191429138, train_loss: 0.5792425870895386\n",
            "7320 val_loss: 0.5818659663200378, train_loss: 0.578983724117279\n",
            "7330 val_loss: 0.5815184712409973, train_loss: 0.5788060426712036\n",
            "7340 val_loss: 0.5813871026039124, train_loss: 0.5787347555160522\n",
            "7350 val_loss: 0.5812374949455261, train_loss: 0.578392744064331\n",
            "7360 val_loss: 0.5810481309890747, train_loss: 0.5781755447387695\n",
            "7370 val_loss: 0.5808407068252563, train_loss: 0.5780627727508545\n",
            "7380 val_loss: 0.5807328820228577, train_loss: 0.5779892206192017\n",
            "7390 val_loss: 0.5807229280471802, train_loss: 0.5779000520706177\n",
            "7400 val_loss: 0.5804902911186218, train_loss: 0.5776745676994324\n",
            "7410 val_loss: 0.5803540349006653, train_loss: 0.5774356126785278\n",
            "7420 val_loss: 0.5800621509552002, train_loss: 0.5772081017494202\n",
            "7430 val_loss: 0.5798386335372925, train_loss: 0.5770449638366699\n",
            "7440 val_loss: 0.5796425938606262, train_loss: 0.5768185257911682\n",
            "7450 val_loss: 0.5795450210571289, train_loss: 0.5766566395759583\n",
            "7460 val_loss: 0.5794147849082947, train_loss: 0.5764984488487244\n",
            "7470 val_loss: 0.5791492462158203, train_loss: 0.5764251351356506\n",
            "7480 val_loss: 0.5789881944656372, train_loss: 0.5761953592300415\n",
            "7490 val_loss: 0.5787073373794556, train_loss: 0.5759016275405884\n",
            "7500 val_loss: 0.5784358382225037, train_loss: 0.5757016539573669\n",
            "7510 val_loss: 0.5783419609069824, train_loss: 0.5755257606506348\n",
            "7520 val_loss: 0.5781392455101013, train_loss: 0.5753283500671387\n",
            "7530 val_loss: 0.578004002571106, train_loss: 0.5749848484992981\n",
            "7540 val_loss: 0.5778072476387024, train_loss: 0.574883222579956\n",
            "7550 val_loss: 0.5775454044342041, train_loss: 0.5745598077774048\n",
            "7560 val_loss: 0.5773219466209412, train_loss: 0.5743934512138367\n",
            "7570 val_loss: 0.5772467255592346, train_loss: 0.5742524862289429\n",
            "7580 val_loss: 0.5769791603088379, train_loss: 0.5739018321037292\n",
            "7590 val_loss: 0.5768701434135437, train_loss: 0.573630690574646\n",
            "7600 val_loss: 0.5768172144889832, train_loss: 0.5735790729522705\n",
            "7610 val_loss: 0.5764992833137512, train_loss: 0.5733199119567871\n",
            "7620 val_loss: 0.576281726360321, train_loss: 0.5730469226837158\n",
            "7630 val_loss: 0.5760035514831543, train_loss: 0.5729357004165649\n",
            "7640 val_loss: 0.575954794883728, train_loss: 0.5726698040962219\n",
            "7650 val_loss: 0.5757937431335449, train_loss: 0.5725910067558289\n",
            "7660 val_loss: 0.5755942463874817, train_loss: 0.5723918080329895\n",
            "7670 val_loss: 0.5752539038658142, train_loss: 0.5721660256385803\n",
            "7680 val_loss: 0.5749095678329468, train_loss: 0.5717523097991943\n",
            "7690 val_loss: 0.5746632814407349, train_loss: 0.5715596079826355\n",
            "7700 val_loss: 0.574626624584198, train_loss: 0.5714114308357239\n",
            "7710 val_loss: 0.5743156671524048, train_loss: 0.5711530447006226\n",
            "7720 val_loss: 0.5740495920181274, train_loss: 0.5708757638931274\n",
            "7730 val_loss: 0.5739378929138184, train_loss: 0.5706937909126282\n",
            "7740 val_loss: 0.5737948417663574, train_loss: 0.5704032778739929\n",
            "7750 val_loss: 0.5736214518547058, train_loss: 0.5702441930770874\n",
            "7760 val_loss: 0.5735393166542053, train_loss: 0.5699965357780457\n",
            "7770 val_loss: 0.5734117031097412, train_loss: 0.5697734951972961\n",
            "7780 val_loss: 0.5732735395431519, train_loss: 0.5695498585700989\n",
            "7790 val_loss: 0.5729559063911438, train_loss: 0.5692886710166931\n",
            "7800 val_loss: 0.5726994276046753, train_loss: 0.5689964890480042\n",
            "7810 val_loss: 0.5725582242012024, train_loss: 0.5688337683677673\n",
            "7820 val_loss: 0.5720393061637878, train_loss: 0.5683777928352356\n",
            "7830 val_loss: 0.5718472599983215, train_loss: 0.5680191516876221\n",
            "7840 val_loss: 0.5716788172721863, train_loss: 0.567650556564331\n",
            "7850 val_loss: 0.5714073181152344, train_loss: 0.5672886967658997\n",
            "7860 val_loss: 0.5710583925247192, train_loss: 0.5669234395027161\n",
            "7870 val_loss: 0.5708451271057129, train_loss: 0.5666354894638062\n",
            "7880 val_loss: 0.5705150365829468, train_loss: 0.5662828683853149\n",
            "7890 val_loss: 0.5703755617141724, train_loss: 0.5661397576332092\n",
            "7900 val_loss: 0.569850504398346, train_loss: 0.5656001567840576\n",
            "7910 val_loss: 0.5699000358581543, train_loss: 0.5655829310417175\n",
            "7920 val_loss: 0.5697224736213684, train_loss: 0.5653547048568726\n",
            "7930 val_loss: 0.569412112236023, train_loss: 0.5650832653045654\n",
            "7940 val_loss: 0.5692481994628906, train_loss: 0.5648300647735596\n",
            "7950 val_loss: 0.5689868330955505, train_loss: 0.5644676685333252\n",
            "7960 val_loss: 0.5686052441596985, train_loss: 0.5640597939491272\n",
            "7970 val_loss: 0.5682169795036316, train_loss: 0.5636633038520813\n",
            "7980 val_loss: 0.5679783225059509, train_loss: 0.563409149646759\n",
            "7990 val_loss: 0.5677762627601624, train_loss: 0.563144862651825\n",
            "8000 val_loss: 0.5675116777420044, train_loss: 0.5628044009208679\n",
            "8010 val_loss: 0.5671948790550232, train_loss: 0.5624730587005615\n",
            "8020 val_loss: 0.5668993592262268, train_loss: 0.5621185898780823\n",
            "8030 val_loss: 0.5664306879043579, train_loss: 0.5616574287414551\n",
            "8040 val_loss: 0.5663602352142334, train_loss: 0.5614825487136841\n",
            "8050 val_loss: 0.5660677552223206, train_loss: 0.5612598061561584\n",
            "8060 val_loss: 0.5658656358718872, train_loss: 0.5610691905021667\n",
            "8070 val_loss: 0.5654690861701965, train_loss: 0.5605299472808838\n",
            "8080 val_loss: 0.5651754140853882, train_loss: 0.5600978136062622\n",
            "8090 val_loss: 0.5647174715995789, train_loss: 0.5597266554832458\n",
            "8100 val_loss: 0.5645036101341248, train_loss: 0.5594866871833801\n",
            "8110 val_loss: 0.5643334984779358, train_loss: 0.5592591166496277\n",
            "8120 val_loss: 0.5640179514884949, train_loss: 0.5589051246643066\n",
            "8130 val_loss: 0.5636868476867676, train_loss: 0.5585359334945679\n",
            "8140 val_loss: 0.5632476806640625, train_loss: 0.5580840110778809\n",
            "8150 val_loss: 0.5628820657730103, train_loss: 0.5576756000518799\n",
            "8160 val_loss: 0.5626928210258484, train_loss: 0.5574308633804321\n",
            "8170 val_loss: 0.562468945980072, train_loss: 0.5571774244308472\n",
            "8180 val_loss: 0.5621726512908936, train_loss: 0.5567793846130371\n",
            "8190 val_loss: 0.5618398785591125, train_loss: 0.5563706159591675\n",
            "8200 val_loss: 0.561403214931488, train_loss: 0.5560051798820496\n",
            "8210 val_loss: 0.5609057545661926, train_loss: 0.5554579496383667\n",
            "8220 val_loss: 0.5605728030204773, train_loss: 0.5552029609680176\n",
            "8230 val_loss: 0.5600427985191345, train_loss: 0.5546259880065918\n",
            "8240 val_loss: 0.5594925284385681, train_loss: 0.5542386770248413\n",
            "8250 val_loss: 0.5590718984603882, train_loss: 0.5538283586502075\n",
            "8260 val_loss: 0.5587480664253235, train_loss: 0.5534314513206482\n",
            "8270 val_loss: 0.5584851503372192, train_loss: 0.5531595349311829\n",
            "8280 val_loss: 0.5579005479812622, train_loss: 0.5526989102363586\n",
            "8290 val_loss: 0.5574902296066284, train_loss: 0.5522814989089966\n",
            "8300 val_loss: 0.5570148825645447, train_loss: 0.5517567992210388\n",
            "8310 val_loss: 0.5565062165260315, train_loss: 0.5512194633483887\n",
            "8320 val_loss: 0.5561042428016663, train_loss: 0.5507702231407166\n",
            "8330 val_loss: 0.5557681322097778, train_loss: 0.5504470467567444\n",
            "8340 val_loss: 0.555530309677124, train_loss: 0.5500965714454651\n",
            "8350 val_loss: 0.5551391839981079, train_loss: 0.5497145652770996\n",
            "8360 val_loss: 0.5548736453056335, train_loss: 0.5494062304496765\n",
            "8370 val_loss: 0.5545501112937927, train_loss: 0.549091637134552\n",
            "8380 val_loss: 0.5540693998336792, train_loss: 0.5487446188926697\n",
            "8390 val_loss: 0.553516149520874, train_loss: 0.548030436038971\n",
            "8400 val_loss: 0.5531742572784424, train_loss: 0.5476118922233582\n",
            "8410 val_loss: 0.5527861714363098, train_loss: 0.5470492839813232\n",
            "8420 val_loss: 0.5523634552955627, train_loss: 0.5465940833091736\n",
            "8430 val_loss: 0.5520000457763672, train_loss: 0.5461955070495605\n",
            "8440 val_loss: 0.5514198541641235, train_loss: 0.5455213785171509\n",
            "8450 val_loss: 0.5511384010314941, train_loss: 0.5453270673751831\n",
            "8460 val_loss: 0.5508332252502441, train_loss: 0.5449395775794983\n",
            "8470 val_loss: 0.5504509210586548, train_loss: 0.5445430278778076\n",
            "8480 val_loss: 0.5500794053077698, train_loss: 0.544176459312439\n",
            "8490 val_loss: 0.5495539903640747, train_loss: 0.543648898601532\n",
            "8500 val_loss: 0.5490842461585999, train_loss: 0.5432297587394714\n",
            "8510 val_loss: 0.5486226081848145, train_loss: 0.5426803231239319\n",
            "8520 val_loss: 0.5483914613723755, train_loss: 0.5424314737319946\n",
            "8530 val_loss: 0.5477584004402161, train_loss: 0.5417989492416382\n",
            "8540 val_loss: 0.5473716259002686, train_loss: 0.5413919687271118\n",
            "8550 val_loss: 0.5469319224357605, train_loss: 0.5407350659370422\n",
            "8560 val_loss: 0.5464328527450562, train_loss: 0.5403305888175964\n",
            "8570 val_loss: 0.5458946228027344, train_loss: 0.5397782325744629\n",
            "8580 val_loss: 0.5453571081161499, train_loss: 0.5391932725906372\n",
            "8590 val_loss: 0.5449572801589966, train_loss: 0.5388005375862122\n",
            "8600 val_loss: 0.5444783568382263, train_loss: 0.5383170247077942\n",
            "8610 val_loss: 0.5439543128013611, train_loss: 0.5378372669219971\n",
            "8620 val_loss: 0.5436951518058777, train_loss: 0.5374528765678406\n",
            "8630 val_loss: 0.5432088971138, train_loss: 0.5369429588317871\n",
            "8640 val_loss: 0.5429022908210754, train_loss: 0.5364884734153748\n",
            "8650 val_loss: 0.5425719022750854, train_loss: 0.5361049175262451\n",
            "8660 val_loss: 0.5422198176383972, train_loss: 0.5355995893478394\n",
            "8670 val_loss: 0.5416285395622253, train_loss: 0.5350499153137207\n",
            "8680 val_loss: 0.5412226319313049, train_loss: 0.5345404744148254\n",
            "8690 val_loss: 0.54078209400177, train_loss: 0.5340667366981506\n",
            "8700 val_loss: 0.5403481125831604, train_loss: 0.5337096452713013\n",
            "8710 val_loss: 0.5400503277778625, train_loss: 0.5332555770874023\n",
            "8720 val_loss: 0.5394537448883057, train_loss: 0.5326768755912781\n",
            "8730 val_loss: 0.5388897657394409, train_loss: 0.5321184396743774\n",
            "8740 val_loss: 0.538279116153717, train_loss: 0.5314152240753174\n",
            "8750 val_loss: 0.53779536485672, train_loss: 0.5308663845062256\n",
            "8760 val_loss: 0.5372307300567627, train_loss: 0.5303460955619812\n",
            "8770 val_loss: 0.5368340015411377, train_loss: 0.5298196077346802\n",
            "8780 val_loss: 0.5363761782646179, train_loss: 0.5293542742729187\n",
            "8790 val_loss: 0.5359399318695068, train_loss: 0.5288733839988708\n",
            "8800 val_loss: 0.5357557535171509, train_loss: 0.5285369157791138\n",
            "8810 val_loss: 0.5350989103317261, train_loss: 0.5278904438018799\n",
            "8820 val_loss: 0.5347450375556946, train_loss: 0.5274531841278076\n",
            "8830 val_loss: 0.534176230430603, train_loss: 0.5268597602844238\n",
            "8840 val_loss: 0.5336724519729614, train_loss: 0.5262000560760498\n",
            "8850 val_loss: 0.5332218408584595, train_loss: 0.5257108211517334\n",
            "8860 val_loss: 0.5328881144523621, train_loss: 0.525201141834259\n",
            "8870 val_loss: 0.5322719216346741, train_loss: 0.5246323943138123\n",
            "8880 val_loss: 0.5317436456680298, train_loss: 0.5240607261657715\n",
            "8890 val_loss: 0.5314305424690247, train_loss: 0.5235449075698853\n",
            "8900 val_loss: 0.5309833288192749, train_loss: 0.5229715704917908\n",
            "8910 val_loss: 0.5303705334663391, train_loss: 0.5223137736320496\n",
            "8920 val_loss: 0.5301137566566467, train_loss: 0.5219363570213318\n",
            "8930 val_loss: 0.5296441912651062, train_loss: 0.521391749382019\n",
            "8940 val_loss: 0.5294619798660278, train_loss: 0.5209252238273621\n",
            "8950 val_loss: 0.5286321043968201, train_loss: 0.5201992392539978\n",
            "8960 val_loss: 0.5279610753059387, train_loss: 0.5196038484573364\n",
            "8970 val_loss: 0.5278629660606384, train_loss: 0.5193656086921692\n",
            "8980 val_loss: 0.527458667755127, train_loss: 0.5188406109809875\n",
            "8990 val_loss: 0.5271332859992981, train_loss: 0.5183871388435364\n",
            "9000 val_loss: 0.5266045331954956, train_loss: 0.5178608894348145\n",
            "9010 val_loss: 0.5261720418930054, train_loss: 0.5174108147621155\n",
            "9020 val_loss: 0.5257084369659424, train_loss: 0.5169172286987305\n",
            "9030 val_loss: 0.5252253413200378, train_loss: 0.516252875328064\n",
            "9040 val_loss: 0.5247632265090942, train_loss: 0.5157504677772522\n",
            "9050 val_loss: 0.5240346193313599, train_loss: 0.5150596499443054\n",
            "9060 val_loss: 0.5235846638679504, train_loss: 0.5144948959350586\n",
            "9070 val_loss: 0.5228094458580017, train_loss: 0.5138666033744812\n",
            "9080 val_loss: 0.5223702192306519, train_loss: 0.5134721994400024\n",
            "9090 val_loss: 0.522020161151886, train_loss: 0.5131463408470154\n",
            "9100 val_loss: 0.5218862891197205, train_loss: 0.5127697587013245\n",
            "9110 val_loss: 0.521327555179596, train_loss: 0.5122542381286621\n",
            "9120 val_loss: 0.5207787156105042, train_loss: 0.511570930480957\n",
            "9130 val_loss: 0.5203630328178406, train_loss: 0.5111226439476013\n",
            "9140 val_loss: 0.5196865200996399, train_loss: 0.5104386806488037\n",
            "9150 val_loss: 0.5192546248435974, train_loss: 0.509890615940094\n",
            "9160 val_loss: 0.5188248157501221, train_loss: 0.5092336535453796\n",
            "9170 val_loss: 0.5181118845939636, train_loss: 0.508586049079895\n",
            "9180 val_loss: 0.51755291223526, train_loss: 0.5079812407493591\n",
            "9190 val_loss: 0.5169757008552551, train_loss: 0.5073671340942383\n",
            "9200 val_loss: 0.5167237520217896, train_loss: 0.5069918632507324\n",
            "9210 val_loss: 0.51613849401474, train_loss: 0.5063885450363159\n",
            "9220 val_loss: 0.5157236456871033, train_loss: 0.5058038830757141\n",
            "9230 val_loss: 0.5148497223854065, train_loss: 0.5051364302635193\n",
            "9240 val_loss: 0.5147501230239868, train_loss: 0.5046420693397522\n",
            "9250 val_loss: 0.5143738985061646, train_loss: 0.5042119026184082\n",
            "9260 val_loss: 0.5139754414558411, train_loss: 0.5038232803344727\n",
            "9270 val_loss: 0.513689398765564, train_loss: 0.5033593773841858\n",
            "9280 val_loss: 0.5135148763656616, train_loss: 0.5028697848320007\n",
            "9290 val_loss: 0.513177752494812, train_loss: 0.5024707317352295\n",
            "9300 val_loss: 0.5127846002578735, train_loss: 0.5018839836120605\n",
            "9310 val_loss: 0.512362003326416, train_loss: 0.5013233423233032\n",
            "9320 val_loss: 0.5121050477027893, train_loss: 0.5008887648582458\n",
            "9330 val_loss: 0.5115339159965515, train_loss: 0.500325083732605\n",
            "9340 val_loss: 0.51103675365448, train_loss: 0.49967053532600403\n",
            "9350 val_loss: 0.5102115273475647, train_loss: 0.4990186393260956\n",
            "9360 val_loss: 0.5096399784088135, train_loss: 0.49842149019241333\n",
            "9370 val_loss: 0.5094035267829895, train_loss: 0.49798986315727234\n",
            "9380 val_loss: 0.5090287327766418, train_loss: 0.49748992919921875\n",
            "9390 val_loss: 0.5087658166885376, train_loss: 0.4969809055328369\n",
            "9400 val_loss: 0.5083687901496887, train_loss: 0.4964238405227661\n",
            "9410 val_loss: 0.5080053806304932, train_loss: 0.49601659178733826\n",
            "9420 val_loss: 0.5076178312301636, train_loss: 0.49557650089263916\n",
            "9430 val_loss: 0.5072008967399597, train_loss: 0.49496281147003174\n",
            "9440 val_loss: 0.5066792964935303, train_loss: 0.4942948818206787\n",
            "9450 val_loss: 0.5065491795539856, train_loss: 0.49385762214660645\n",
            "9460 val_loss: 0.5060026049613953, train_loss: 0.4932573139667511\n",
            "9470 val_loss: 0.5054606199264526, train_loss: 0.49261462688446045\n",
            "9480 val_loss: 0.5051836967468262, train_loss: 0.49207282066345215\n",
            "9490 val_loss: 0.5048438310623169, train_loss: 0.4915430545806885\n",
            "9500 val_loss: 0.5042909979820251, train_loss: 0.4909801185131073\n",
            "9510 val_loss: 0.5036119222640991, train_loss: 0.49040472507476807\n",
            "9520 val_loss: 0.5033804774284363, train_loss: 0.48987317085266113\n",
            "9530 val_loss: 0.5031295418739319, train_loss: 0.48945653438568115\n",
            "9540 val_loss: 0.5025179982185364, train_loss: 0.48871278762817383\n",
            "9550 val_loss: 0.5022618770599365, train_loss: 0.4882461726665497\n",
            "9560 val_loss: 0.5018223524093628, train_loss: 0.48774901032447815\n",
            "9570 val_loss: 0.5011994242668152, train_loss: 0.48708659410476685\n",
            "9580 val_loss: 0.5009240508079529, train_loss: 0.4866311252117157\n",
            "9590 val_loss: 0.5006693005561829, train_loss: 0.48633140325546265\n",
            "9600 val_loss: 0.5000542402267456, train_loss: 0.48564863204956055\n",
            "9610 val_loss: 0.49938100576400757, train_loss: 0.4849773049354553\n",
            "9620 val_loss: 0.4989367127418518, train_loss: 0.48439574241638184\n",
            "9630 val_loss: 0.49857330322265625, train_loss: 0.4838566780090332\n",
            "9640 val_loss: 0.4983426034450531, train_loss: 0.4833451211452484\n",
            "9650 val_loss: 0.49789583683013916, train_loss: 0.4827417731285095\n",
            "9660 val_loss: 0.4974573254585266, train_loss: 0.48225337266921997\n",
            "9670 val_loss: 0.4970107674598694, train_loss: 0.4818108081817627\n",
            "9680 val_loss: 0.49624648690223694, train_loss: 0.48097148537635803\n",
            "9690 val_loss: 0.49634188413619995, train_loss: 0.4805923104286194\n",
            "9700 val_loss: 0.49654343724250793, train_loss: 0.4804489314556122\n",
            "9710 val_loss: 0.4958367645740509, train_loss: 0.4795692265033722\n",
            "9720 val_loss: 0.4951282739639282, train_loss: 0.47868144512176514\n",
            "9730 val_loss: 0.4949359893798828, train_loss: 0.4782099425792694\n",
            "9740 val_loss: 0.4945088326931, train_loss: 0.47752416133880615\n",
            "9750 val_loss: 0.4939873516559601, train_loss: 0.47687798738479614\n",
            "9760 val_loss: 0.4933735132217407, train_loss: 0.47626471519470215\n",
            "9770 val_loss: 0.4930640757083893, train_loss: 0.4757346212863922\n",
            "9780 val_loss: 0.4924204349517822, train_loss: 0.4750312566757202\n",
            "9790 val_loss: 0.49199581146240234, train_loss: 0.47446972131729126\n",
            "9800 val_loss: 0.4915698766708374, train_loss: 0.4738183617591858\n",
            "9810 val_loss: 0.4915490746498108, train_loss: 0.4734534025192261\n",
            "9820 val_loss: 0.4917248785495758, train_loss: 0.47325384616851807\n",
            "9830 val_loss: 0.4911597967147827, train_loss: 0.4723941385746002\n",
            "9840 val_loss: 0.4905489385128021, train_loss: 0.47156304121017456\n",
            "9850 val_loss: 0.4900362491607666, train_loss: 0.4709847569465637\n",
            "9860 val_loss: 0.48983368277549744, train_loss: 0.4705943167209625\n",
            "9870 val_loss: 0.4895354211330414, train_loss: 0.4700114130973816\n",
            "9880 val_loss: 0.48919376730918884, train_loss: 0.46939799189567566\n",
            "9890 val_loss: 0.4888583719730377, train_loss: 0.4688224494457245\n",
            "9900 val_loss: 0.4881766140460968, train_loss: 0.468182772397995\n",
            "9910 val_loss: 0.4877742528915405, train_loss: 0.46759113669395447\n",
            "9920 val_loss: 0.4874584674835205, train_loss: 0.46697938442230225\n",
            "9930 val_loss: 0.4872036576271057, train_loss: 0.46642419695854187\n",
            "9940 val_loss: 0.4868955612182617, train_loss: 0.4659014344215393\n",
            "9950 val_loss: 0.4862421154975891, train_loss: 0.4652358889579773\n",
            "9960 val_loss: 0.4857126772403717, train_loss: 0.4643731117248535\n",
            "9970 val_loss: 0.48551228642463684, train_loss: 0.4638531804084778\n",
            "9980 val_loss: 0.48524466156959534, train_loss: 0.4632056951522827\n",
            "9990 val_loss: 0.4850849509239197, train_loss: 0.46281445026397705\n",
            "10000 val_loss: 0.4843011498451233, train_loss: 0.4617747366428375\n",
            "10010 val_loss: 0.48394352197647095, train_loss: 0.46114322543144226\n",
            "10020 val_loss: 0.48365187644958496, train_loss: 0.4605167806148529\n",
            "10030 val_loss: 0.4832276701927185, train_loss: 0.45984867215156555\n",
            "10040 val_loss: 0.48230889439582825, train_loss: 0.45863088965415955\n",
            "10050 val_loss: 0.4819685220718384, train_loss: 0.4579971730709076\n",
            "10060 val_loss: 0.4817347228527069, train_loss: 0.45762643218040466\n",
            "10070 val_loss: 0.4813292324542999, train_loss: 0.45670509338378906\n",
            "10080 val_loss: 0.48089754581451416, train_loss: 0.45600563287734985\n",
            "10090 val_loss: 0.480511337518692, train_loss: 0.4552895724773407\n",
            "10100 val_loss: 0.48012223839759827, train_loss: 0.4546242654323578\n",
            "10110 val_loss: 0.4796099364757538, train_loss: 0.45387929677963257\n",
            "10120 val_loss: 0.4791942238807678, train_loss: 0.4531567692756653\n",
            "10130 val_loss: 0.4789251685142517, train_loss: 0.45261573791503906\n",
            "10140 val_loss: 0.47842735052108765, train_loss: 0.4519338011741638\n",
            "10150 val_loss: 0.4779159128665924, train_loss: 0.45118317008018494\n",
            "10160 val_loss: 0.47736087441444397, train_loss: 0.4501732289791107\n",
            "10170 val_loss: 0.47718843817710876, train_loss: 0.4496433138847351\n",
            "10180 val_loss: 0.47680041193962097, train_loss: 0.4489918053150177\n",
            "10190 val_loss: 0.4763846695423126, train_loss: 0.4482506811618805\n",
            "10200 val_loss: 0.4760631024837494, train_loss: 0.4477098286151886\n",
            "10210 val_loss: 0.47550272941589355, train_loss: 0.4467460513114929\n",
            "10220 val_loss: 0.47504591941833496, train_loss: 0.44602352380752563\n",
            "10230 val_loss: 0.4750840365886688, train_loss: 0.44571802020072937\n",
            "10240 val_loss: 0.4747048020362854, train_loss: 0.44490480422973633\n",
            "10250 val_loss: 0.47402453422546387, train_loss: 0.44389304518699646\n",
            "10260 val_loss: 0.47414690256118774, train_loss: 0.4436376094818115\n",
            "10270 val_loss: 0.47359925508499146, train_loss: 0.44286665320396423\n",
            "10280 val_loss: 0.4728715121746063, train_loss: 0.44176238775253296\n",
            "10290 val_loss: 0.4726651906967163, train_loss: 0.4411138892173767\n",
            "10300 val_loss: 0.4722471833229065, train_loss: 0.4404035210609436\n",
            "10310 val_loss: 0.4716246426105499, train_loss: 0.4394069314002991\n",
            "10320 val_loss: 0.4712761342525482, train_loss: 0.43851926922798157\n",
            "10330 val_loss: 0.47097623348236084, train_loss: 0.43777450919151306\n",
            "10340 val_loss: 0.470289945602417, train_loss: 0.4366757273674011\n",
            "10350 val_loss: 0.4704107642173767, train_loss: 0.4365234673023224\n",
            "10360 val_loss: 0.46967247128486633, train_loss: 0.435437947511673\n",
            "10370 val_loss: 0.4692547619342804, train_loss: 0.4345611333847046\n",
            "10380 val_loss: 0.4686175286769867, train_loss: 0.4337043762207031\n",
            "10390 val_loss: 0.46810904145240784, train_loss: 0.43260639905929565\n",
            "10400 val_loss: 0.4679476320743561, train_loss: 0.4320191442966461\n",
            "10410 val_loss: 0.4677177965641022, train_loss: 0.4315296411514282\n",
            "10420 val_loss: 0.4672359824180603, train_loss: 0.43082940578460693\n",
            "10430 val_loss: 0.4667205810546875, train_loss: 0.4297502934932709\n",
            "10440 val_loss: 0.46646618843078613, train_loss: 0.42916277050971985\n",
            "10450 val_loss: 0.46569007635116577, train_loss: 0.4280163049697876\n",
            "10460 val_loss: 0.4653319716453552, train_loss: 0.42720454931259155\n",
            "10470 val_loss: 0.4648136794567108, train_loss: 0.4263644218444824\n",
            "10480 val_loss: 0.46459832787513733, train_loss: 0.4259386658668518\n",
            "10490 val_loss: 0.46411141753196716, train_loss: 0.4252275824546814\n",
            "10500 val_loss: 0.46361687779426575, train_loss: 0.4242261052131653\n",
            "10510 val_loss: 0.4630855917930603, train_loss: 0.423328697681427\n",
            "10520 val_loss: 0.4628613293170929, train_loss: 0.42265838384628296\n",
            "10530 val_loss: 0.4623587429523468, train_loss: 0.4218807518482208\n",
            "10540 val_loss: 0.46192827820777893, train_loss: 0.42099088430404663\n",
            "10550 val_loss: 0.4616212248802185, train_loss: 0.42059722542762756\n",
            "10560 val_loss: 0.46145662665367126, train_loss: 0.42004895210266113\n",
            "10570 val_loss: 0.4611022472381592, train_loss: 0.4194028675556183\n",
            "10580 val_loss: 0.46070170402526855, train_loss: 0.41849127411842346\n",
            "10590 val_loss: 0.46011874079704285, train_loss: 0.4176363945007324\n",
            "10600 val_loss: 0.4600977301597595, train_loss: 0.41709038615226746\n",
            "10610 val_loss: 0.45942360162734985, train_loss: 0.4160943031311035\n",
            "10620 val_loss: 0.459505170583725, train_loss: 0.41614091396331787\n",
            "10630 val_loss: 0.4587344527244568, train_loss: 0.4149748384952545\n",
            "10640 val_loss: 0.4581727087497711, train_loss: 0.41401785612106323\n",
            "10650 val_loss: 0.4577265679836273, train_loss: 0.41328904032707214\n",
            "10660 val_loss: 0.457476943731308, train_loss: 0.41291576623916626\n",
            "10670 val_loss: 0.4570368826389313, train_loss: 0.4121369421482086\n",
            "10680 val_loss: 0.45637696981430054, train_loss: 0.4109984338283539\n",
            "10690 val_loss: 0.45593270659446716, train_loss: 0.41009241342544556\n",
            "10700 val_loss: 0.455719530582428, train_loss: 0.4097367823123932\n",
            "10710 val_loss: 0.45452558994293213, train_loss: 0.40823644399642944\n",
            "10720 val_loss: 0.454739511013031, train_loss: 0.40820372104644775\n",
            "10730 val_loss: 0.45416712760925293, train_loss: 0.40707385540008545\n",
            "10740 val_loss: 0.4537922441959381, train_loss: 0.40653523802757263\n",
            "10750 val_loss: 0.45317625999450684, train_loss: 0.4057283103466034\n",
            "10760 val_loss: 0.4530125558376312, train_loss: 0.40535518527030945\n",
            "10770 val_loss: 0.4527040719985962, train_loss: 0.40477094054222107\n",
            "10780 val_loss: 0.452042818069458, train_loss: 0.40384840965270996\n",
            "10790 val_loss: 0.4516442120075226, train_loss: 0.40314966440200806\n",
            "10800 val_loss: 0.45143502950668335, train_loss: 0.402677446603775\n",
            "10810 val_loss: 0.45117151737213135, train_loss: 0.40197324752807617\n",
            "10820 val_loss: 0.45052751898765564, train_loss: 0.4009602665901184\n",
            "10830 val_loss: 0.4502005875110626, train_loss: 0.3999808430671692\n",
            "10840 val_loss: 0.4499850869178772, train_loss: 0.3995715081691742\n",
            "10850 val_loss: 0.44979172945022583, train_loss: 0.39914408326148987\n",
            "10860 val_loss: 0.4496382772922516, train_loss: 0.39825278520584106\n",
            "10870 val_loss: 0.4493260979652405, train_loss: 0.3975544273853302\n",
            "10880 val_loss: 0.4490535855293274, train_loss: 0.39670372009277344\n",
            "10890 val_loss: 0.44890084862709045, train_loss: 0.39608243107795715\n",
            "10900 val_loss: 0.4485020339488983, train_loss: 0.3950207233428955\n",
            "10910 val_loss: 0.4480758309364319, train_loss: 0.39417529106140137\n",
            "10920 val_loss: 0.44767528772354126, train_loss: 0.393154501914978\n",
            "10930 val_loss: 0.44748109579086304, train_loss: 0.39292871952056885\n",
            "10940 val_loss: 0.4473075866699219, train_loss: 0.3922607898712158\n",
            "10950 val_loss: 0.44696298241615295, train_loss: 0.3913973569869995\n",
            "10960 val_loss: 0.44678905606269836, train_loss: 0.3908115327358246\n",
            "10970 val_loss: 0.4461265206336975, train_loss: 0.39018550515174866\n",
            "10980 val_loss: 0.44626036286354065, train_loss: 0.3899692893028259\n",
            "10990 val_loss: 0.4466189742088318, train_loss: 0.38994330167770386\n",
            "11000 val_loss: 0.44579315185546875, train_loss: 0.38874953985214233\n",
            "11010 val_loss: 0.44556546211242676, train_loss: 0.3882436156272888\n",
            "11020 val_loss: 0.4447901248931885, train_loss: 0.3871200680732727\n",
            "11030 val_loss: 0.4443294405937195, train_loss: 0.3863998055458069\n",
            "11040 val_loss: 0.44461795687675476, train_loss: 0.38634413480758667\n",
            "11050 val_loss: 0.4445513188838959, train_loss: 0.3862200379371643\n",
            "11060 val_loss: 0.4441993832588196, train_loss: 0.38548725843429565\n",
            "11070 val_loss: 0.4439678192138672, train_loss: 0.3849077820777893\n",
            "11080 val_loss: 0.4432966709136963, train_loss: 0.3838423490524292\n",
            "11090 val_loss: 0.44304966926574707, train_loss: 0.3832699656486511\n",
            "11100 val_loss: 0.4427521526813507, train_loss: 0.3827000558376312\n",
            "11110 val_loss: 0.4425228536128998, train_loss: 0.38240620493888855\n",
            "11120 val_loss: 0.4422663748264313, train_loss: 0.38191574811935425\n",
            "11130 val_loss: 0.4417788088321686, train_loss: 0.38120612502098083\n",
            "11140 val_loss: 0.44132182002067566, train_loss: 0.3807489573955536\n",
            "11150 val_loss: 0.4412100315093994, train_loss: 0.3803633451461792\n",
            "11160 val_loss: 0.4410513937473297, train_loss: 0.38002827763557434\n",
            "11170 val_loss: 0.4406290054321289, train_loss: 0.37929201126098633\n",
            "11180 val_loss: 0.4405086040496826, train_loss: 0.37901750206947327\n",
            "11190 val_loss: 0.4401988983154297, train_loss: 0.37848079204559326\n",
            "11200 val_loss: 0.4398448169231415, train_loss: 0.37795907258987427\n",
            "11210 val_loss: 0.4397132396697998, train_loss: 0.3775414824485779\n",
            "11220 val_loss: 0.4393027722835541, train_loss: 0.3773325979709625\n",
            "11230 val_loss: 0.43899399042129517, train_loss: 0.37677454948425293\n",
            "11240 val_loss: 0.4383503198623657, train_loss: 0.37617039680480957\n",
            "11250 val_loss: 0.43809500336647034, train_loss: 0.3753916323184967\n",
            "11260 val_loss: 0.43801039457321167, train_loss: 0.3749644160270691\n",
            "11270 val_loss: 0.4375320076942444, train_loss: 0.37435826659202576\n",
            "11280 val_loss: 0.43691036105155945, train_loss: 0.37397128343582153\n",
            "11290 val_loss: 0.43667393922805786, train_loss: 0.37323492765426636\n",
            "11300 val_loss: 0.4362120032310486, train_loss: 0.37288016080856323\n",
            "11310 val_loss: 0.4358636736869812, train_loss: 0.37208276987075806\n",
            "11320 val_loss: 0.4355938732624054, train_loss: 0.3719346225261688\n",
            "11330 val_loss: 0.4352932870388031, train_loss: 0.37108683586120605\n",
            "11340 val_loss: 0.43500417470932007, train_loss: 0.37115517258644104\n",
            "11350 val_loss: 0.4349197745323181, train_loss: 0.3709680736064911\n",
            "11360 val_loss: 0.4347485899925232, train_loss: 0.3699455261230469\n",
            "11370 val_loss: 0.43453988432884216, train_loss: 0.3700537383556366\n",
            "11380 val_loss: 0.4342091977596283, train_loss: 0.36963051557540894\n",
            "11390 val_loss: 0.43379491567611694, train_loss: 0.3691970407962799\n",
            "11400 val_loss: 0.43325772881507874, train_loss: 0.3680427074432373\n",
            "11410 val_loss: 0.43307632207870483, train_loss: 0.36750975251197815\n",
            "11420 val_loss: 0.4326620101928711, train_loss: 0.3672793507575989\n",
            "11430 val_loss: 0.4323942959308624, train_loss: 0.3669602572917938\n",
            "11440 val_loss: 0.4322030246257782, train_loss: 0.3664287328720093\n",
            "11450 val_loss: 0.4317479729652405, train_loss: 0.3659147024154663\n",
            "11460 val_loss: 0.4310501217842102, train_loss: 0.3649533987045288\n",
            "11470 val_loss: 0.4308793246746063, train_loss: 0.3645729124546051\n",
            "11480 val_loss: 0.430367112159729, train_loss: 0.3641434907913208\n",
            "11490 val_loss: 0.43009141087532043, train_loss: 0.36352086067199707\n",
            "11500 val_loss: 0.4297178387641907, train_loss: 0.36279624700546265\n",
            "11510 val_loss: 0.42941638827323914, train_loss: 0.36216872930526733\n",
            "11520 val_loss: 0.4292714297771454, train_loss: 0.36223167181015015\n",
            "11530 val_loss: 0.4284791052341461, train_loss: 0.3613385856151581\n",
            "11540 val_loss: 0.42805004119873047, train_loss: 0.3605017066001892\n",
            "11550 val_loss: 0.4276493489742279, train_loss: 0.35994189977645874\n",
            "11560 val_loss: 0.42727628350257874, train_loss: 0.35959362983703613\n",
            "11570 val_loss: 0.42675963044166565, train_loss: 0.358739972114563\n",
            "11580 val_loss: 0.4264352023601532, train_loss: 0.3582158386707306\n",
            "11590 val_loss: 0.4259181618690491, train_loss: 0.3580106496810913\n",
            "11600 val_loss: 0.4258217215538025, train_loss: 0.35737165808677673\n",
            "11610 val_loss: 0.42540422081947327, train_loss: 0.35707494616508484\n",
            "11620 val_loss: 0.42506399750709534, train_loss: 0.3564686179161072\n",
            "11630 val_loss: 0.4245562255382538, train_loss: 0.3557388186454773\n",
            "11640 val_loss: 0.42455440759658813, train_loss: 0.35562530159950256\n",
            "11650 val_loss: 0.42383480072021484, train_loss: 0.35502180457115173\n",
            "11660 val_loss: 0.4236588180065155, train_loss: 0.35465773940086365\n",
            "11670 val_loss: 0.4233936071395874, train_loss: 0.35424143075942993\n",
            "11680 val_loss: 0.42326807975769043, train_loss: 0.3534359335899353\n",
            "11690 val_loss: 0.4225548803806305, train_loss: 0.3530735373497009\n",
            "11700 val_loss: 0.42242369055747986, train_loss: 0.35294660925865173\n",
            "11710 val_loss: 0.42184603214263916, train_loss: 0.35230207443237305\n",
            "11720 val_loss: 0.4217209219932556, train_loss: 0.35237401723861694\n",
            "11730 val_loss: 0.42111414670944214, train_loss: 0.35097888112068176\n",
            "11740 val_loss: 0.42055442929267883, train_loss: 0.35041505098342896\n",
            "11750 val_loss: 0.4201202392578125, train_loss: 0.3497658669948578\n",
            "11760 val_loss: 0.4198403060436249, train_loss: 0.3493221700191498\n",
            "11770 val_loss: 0.4192955791950226, train_loss: 0.34889817237854004\n",
            "11780 val_loss: 0.41891539096832275, train_loss: 0.3483564555644989\n",
            "11790 val_loss: 0.4189586043357849, train_loss: 0.3478379547595978\n",
            "11800 val_loss: 0.41803860664367676, train_loss: 0.34733960032463074\n",
            "11810 val_loss: 0.4178980886936188, train_loss: 0.3471105992794037\n",
            "11820 val_loss: 0.417470782995224, train_loss: 0.34626504778862\n",
            "11830 val_loss: 0.4171486496925354, train_loss: 0.34583842754364014\n",
            "11840 val_loss: 0.41680899262428284, train_loss: 0.3457774817943573\n",
            "11850 val_loss: 0.4168115258216858, train_loss: 0.3458418548107147\n",
            "11860 val_loss: 0.41624265909194946, train_loss: 0.34499701857566833\n",
            "11870 val_loss: 0.4155040979385376, train_loss: 0.34390968084335327\n",
            "11880 val_loss: 0.4151436388492584, train_loss: 0.3433706760406494\n",
            "11890 val_loss: 0.41473615169525146, train_loss: 0.34293967485427856\n",
            "11900 val_loss: 0.4143849015235901, train_loss: 0.3423115909099579\n",
            "11910 val_loss: 0.41450318694114685, train_loss: 0.34239575266838074\n",
            "11920 val_loss: 0.4139362573623657, train_loss: 0.34165456891059875\n",
            "11930 val_loss: 0.41367262601852417, train_loss: 0.3413371741771698\n",
            "11940 val_loss: 0.4134574234485626, train_loss: 0.34100255370140076\n",
            "11950 val_loss: 0.41319525241851807, train_loss: 0.3409128785133362\n",
            "11960 val_loss: 0.4127921462059021, train_loss: 0.34047022461891174\n",
            "11970 val_loss: 0.41245073080062866, train_loss: 0.3400251567363739\n",
            "11980 val_loss: 0.4119257628917694, train_loss: 0.33927005529403687\n",
            "11990 val_loss: 0.41157132387161255, train_loss: 0.33868107199668884\n",
            "12000 val_loss: 0.41124966740608215, train_loss: 0.33817681670188904\n",
            "12010 val_loss: 0.4109536111354828, train_loss: 0.3379099369049072\n",
            "12020 val_loss: 0.4106444716453552, train_loss: 0.3376476764678955\n",
            "12030 val_loss: 0.4102870523929596, train_loss: 0.3372560143470764\n",
            "12040 val_loss: 0.40967410802841187, train_loss: 0.33633583784103394\n",
            "12050 val_loss: 0.4095200002193451, train_loss: 0.3359723687171936\n",
            "12060 val_loss: 0.4093133509159088, train_loss: 0.3355282247066498\n",
            "12070 val_loss: 0.40897637605667114, train_loss: 0.3352520167827606\n",
            "12080 val_loss: 0.40886297821998596, train_loss: 0.334997296333313\n",
            "12090 val_loss: 0.40835312008857727, train_loss: 0.3347227871417999\n",
            "12100 val_loss: 0.4079836308956146, train_loss: 0.33411088585853577\n",
            "12110 val_loss: 0.4075528085231781, train_loss: 0.3337808847427368\n",
            "12120 val_loss: 0.406901091337204, train_loss: 0.33345943689346313\n",
            "12130 val_loss: 0.40670812129974365, train_loss: 0.33292582631111145\n",
            "12140 val_loss: 0.406302273273468, train_loss: 0.3325760066509247\n",
            "12150 val_loss: 0.4060593545436859, train_loss: 0.33255505561828613\n",
            "12160 val_loss: 0.40582501888275146, train_loss: 0.3322145640850067\n",
            "12170 val_loss: 0.40561217069625854, train_loss: 0.33173519372940063\n",
            "12180 val_loss: 0.4049939513206482, train_loss: 0.33102214336395264\n",
            "12190 val_loss: 0.40481752157211304, train_loss: 0.33112555742263794\n",
            "12200 val_loss: 0.4047907888889313, train_loss: 0.330695241689682\n",
            "12210 val_loss: 0.40460094809532166, train_loss: 0.33040088415145874\n",
            "12220 val_loss: 0.40417569875717163, train_loss: 0.33016276359558105\n",
            "12230 val_loss: 0.40394333004951477, train_loss: 0.3296753764152527\n",
            "12240 val_loss: 0.4034983813762665, train_loss: 0.3293226957321167\n",
            "12250 val_loss: 0.40319398045539856, train_loss: 0.3289813995361328\n",
            "12260 val_loss: 0.4028507471084595, train_loss: 0.328653484582901\n",
            "12270 val_loss: 0.40264692902565, train_loss: 0.32802635431289673\n",
            "12280 val_loss: 0.4021128714084625, train_loss: 0.3275775611400604\n",
            "12290 val_loss: 0.40165475010871887, train_loss: 0.3272307217121124\n",
            "12300 val_loss: 0.40138572454452515, train_loss: 0.32669901847839355\n",
            "12310 val_loss: 0.4012039601802826, train_loss: 0.32661351561546326\n",
            "12320 val_loss: 0.40110111236572266, train_loss: 0.3261636793613434\n",
            "12330 val_loss: 0.40080025792121887, train_loss: 0.3258657157421112\n",
            "12340 val_loss: 0.4005625247955322, train_loss: 0.3255960941314697\n",
            "12350 val_loss: 0.40020987391471863, train_loss: 0.32529816031455994\n",
            "12360 val_loss: 0.4001498520374298, train_loss: 0.3250309228897095\n",
            "12370 val_loss: 0.40003108978271484, train_loss: 0.3249264359474182\n",
            "12380 val_loss: 0.39996543526649475, train_loss: 0.3245503902435303\n",
            "12390 val_loss: 0.3996509909629822, train_loss: 0.32441815733909607\n",
            "12400 val_loss: 0.39901646971702576, train_loss: 0.32361701130867004\n",
            "12410 val_loss: 0.3988952040672302, train_loss: 0.3232962489128113\n",
            "12420 val_loss: 0.39848726987838745, train_loss: 0.3229616582393646\n",
            "12430 val_loss: 0.3982885479927063, train_loss: 0.3226741552352905\n",
            "12440 val_loss: 0.39788001775741577, train_loss: 0.3222077190876007\n",
            "12450 val_loss: 0.39761292934417725, train_loss: 0.3220183253288269\n",
            "12460 val_loss: 0.39747923612594604, train_loss: 0.3218659460544586\n",
            "12470 val_loss: 0.39729219675064087, train_loss: 0.321694016456604\n",
            "12480 val_loss: 0.3969481885433197, train_loss: 0.32127511501312256\n",
            "12490 val_loss: 0.39685577154159546, train_loss: 0.32113659381866455\n",
            "12500 val_loss: 0.3962409496307373, train_loss: 0.3205321729183197\n",
            "12510 val_loss: 0.39615732431411743, train_loss: 0.3201436996459961\n",
            "12520 val_loss: 0.3959968090057373, train_loss: 0.31979772448539734\n",
            "12530 val_loss: 0.3959766924381256, train_loss: 0.31963104009628296\n",
            "12540 val_loss: 0.395500123500824, train_loss: 0.3192030191421509\n",
            "12550 val_loss: 0.3953310549259186, train_loss: 0.3191019892692566\n",
            "12560 val_loss: 0.3950520157814026, train_loss: 0.3188191056251526\n",
            "12570 val_loss: 0.3950125277042389, train_loss: 0.31880736351013184\n",
            "12580 val_loss: 0.3947114944458008, train_loss: 0.3182356059551239\n",
            "12590 val_loss: 0.3944476544857025, train_loss: 0.31803369522094727\n",
            "12600 val_loss: 0.3943191170692444, train_loss: 0.31786859035491943\n",
            "12610 val_loss: 0.39384549856185913, train_loss: 0.31731653213500977\n",
            "12620 val_loss: 0.3933497965335846, train_loss: 0.3168562352657318\n",
            "12630 val_loss: 0.3930985629558563, train_loss: 0.3168156147003174\n",
            "12640 val_loss: 0.3927915096282959, train_loss: 0.3164650797843933\n",
            "12650 val_loss: 0.39237770438194275, train_loss: 0.31618446111679077\n",
            "12660 val_loss: 0.39237484335899353, train_loss: 0.3158862888813019\n",
            "12670 val_loss: 0.3919437527656555, train_loss: 0.3154046833515167\n",
            "12680 val_loss: 0.39190489053726196, train_loss: 0.31517359614372253\n",
            "12690 val_loss: 0.39159253239631653, train_loss: 0.31485339999198914\n",
            "12700 val_loss: 0.3914342522621155, train_loss: 0.31450310349464417\n",
            "12710 val_loss: 0.39100682735443115, train_loss: 0.3142017126083374\n",
            "12720 val_loss: 0.39093106985092163, train_loss: 0.3140745460987091\n",
            "12730 val_loss: 0.3908348083496094, train_loss: 0.31384044885635376\n",
            "12740 val_loss: 0.3906172811985016, train_loss: 0.31353527307510376\n",
            "12750 val_loss: 0.3900288939476013, train_loss: 0.3132016360759735\n",
            "12760 val_loss: 0.38985925912857056, train_loss: 0.31291255354881287\n",
            "12770 val_loss: 0.3893744945526123, train_loss: 0.3124375641345978\n",
            "12780 val_loss: 0.38897696137428284, train_loss: 0.3120964765548706\n",
            "12790 val_loss: 0.38881751894950867, train_loss: 0.31185805797576904\n",
            "12800 val_loss: 0.388871967792511, train_loss: 0.3119035065174103\n",
            "12810 val_loss: 0.38853028416633606, train_loss: 0.31150802969932556\n",
            "12820 val_loss: 0.38970255851745605, train_loss: 0.312418133020401\n",
            "12830 val_loss: 0.3881416320800781, train_loss: 0.3112243413925171\n",
            "12840 val_loss: 0.3880818784236908, train_loss: 0.3110191524028778\n",
            "12850 val_loss: 0.3876425325870514, train_loss: 0.3107297718524933\n",
            "12860 val_loss: 0.3875124752521515, train_loss: 0.310558944940567\n",
            "12870 val_loss: 0.38736405968666077, train_loss: 0.31009429693222046\n",
            "12880 val_loss: 0.38696256279945374, train_loss: 0.3095085024833679\n",
            "12890 val_loss: 0.3867930471897125, train_loss: 0.3093244135379791\n",
            "12900 val_loss: 0.3863714337348938, train_loss: 0.3089064061641693\n",
            "12910 val_loss: 0.38624095916748047, train_loss: 0.30872198939323425\n",
            "12920 val_loss: 0.38593927025794983, train_loss: 0.3082638680934906\n",
            "12930 val_loss: 0.38572031259536743, train_loss: 0.3078325688838959\n",
            "12940 val_loss: 0.3855840563774109, train_loss: 0.3077133893966675\n",
            "12950 val_loss: 0.38539254665374756, train_loss: 0.3074597418308258\n",
            "12960 val_loss: 0.385259211063385, train_loss: 0.3071967661380768\n",
            "12970 val_loss: 0.3845997750759125, train_loss: 0.306832879781723\n",
            "12980 val_loss: 0.38471588492393494, train_loss: 0.3063059449195862\n",
            "12990 val_loss: 0.3844330608844757, train_loss: 0.3060566782951355\n",
            "13000 val_loss: 0.3839856684207916, train_loss: 0.30575031042099\n",
            "13010 val_loss: 0.3841704726219177, train_loss: 0.3061854839324951\n",
            "13020 val_loss: 0.3835142254829407, train_loss: 0.30544155836105347\n",
            "13030 val_loss: 0.3835074007511139, train_loss: 0.30542969703674316\n",
            "13040 val_loss: 0.383107990026474, train_loss: 0.3049323260784149\n",
            "13050 val_loss: 0.38308483362197876, train_loss: 0.3051658272743225\n",
            "13060 val_loss: 0.38281404972076416, train_loss: 0.30472928285598755\n",
            "13070 val_loss: 0.3825690746307373, train_loss: 0.30434125661849976\n",
            "13080 val_loss: 0.38242703676223755, train_loss: 0.3039616644382477\n",
            "13090 val_loss: 0.3822283148765564, train_loss: 0.3038782775402069\n",
            "13100 val_loss: 0.3818233609199524, train_loss: 0.3034239411354065\n",
            "13110 val_loss: 0.3818861246109009, train_loss: 0.3032839000225067\n",
            "13120 val_loss: 0.3816051483154297, train_loss: 0.30315127968788147\n",
            "13130 val_loss: 0.38133594393730164, train_loss: 0.3026128113269806\n",
            "13140 val_loss: 0.38202258944511414, train_loss: 0.3030458092689514\n",
            "13150 val_loss: 0.38089147210121155, train_loss: 0.30217403173446655\n",
            "13160 val_loss: 0.3805204927921295, train_loss: 0.3018037676811218\n",
            "13170 val_loss: 0.380673348903656, train_loss: 0.301673948764801\n",
            "13180 val_loss: 0.38043972849845886, train_loss: 0.3017786741256714\n",
            "13190 val_loss: 0.38035330176353455, train_loss: 0.301369309425354\n",
            "13200 val_loss: 0.38044771552085876, train_loss: 0.30135253071784973\n",
            "13210 val_loss: 0.37944668531417847, train_loss: 0.30094799399375916\n",
            "13220 val_loss: 0.3796636164188385, train_loss: 0.3009207248687744\n",
            "13230 val_loss: 0.379195898771286, train_loss: 0.3004719018936157\n",
            "13240 val_loss: 0.3789328932762146, train_loss: 0.3001329004764557\n",
            "13250 val_loss: 0.37873733043670654, train_loss: 0.3000757694244385\n",
            "13260 val_loss: 0.3784814476966858, train_loss: 0.29985812306404114\n",
            "13270 val_loss: 0.3780288100242615, train_loss: 0.2994648218154907\n",
            "13280 val_loss: 0.3779946565628052, train_loss: 0.29918164014816284\n",
            "13290 val_loss: 0.3778962790966034, train_loss: 0.29917147755622864\n",
            "13300 val_loss: 0.37788987159729004, train_loss: 0.2992383539676666\n",
            "13310 val_loss: 0.3777107000350952, train_loss: 0.2989518642425537\n",
            "13320 val_loss: 0.37768808007240295, train_loss: 0.2987203896045685\n",
            "13330 val_loss: 0.3780076801776886, train_loss: 0.29885628819465637\n",
            "13340 val_loss: 0.37690168619155884, train_loss: 0.29799777269363403\n",
            "13350 val_loss: 0.3767479360103607, train_loss: 0.2977330982685089\n",
            "13360 val_loss: 0.3766937553882599, train_loss: 0.29767587780952454\n",
            "13370 val_loss: 0.37615031003952026, train_loss: 0.2969426214694977\n",
            "13380 val_loss: 0.3758200705051422, train_loss: 0.29672548174858093\n",
            "13390 val_loss: 0.37598419189453125, train_loss: 0.296588659286499\n",
            "13400 val_loss: 0.37546804547309875, train_loss: 0.296284019947052\n",
            "13410 val_loss: 0.3751800060272217, train_loss: 0.29609960317611694\n",
            "13420 val_loss: 0.37526053190231323, train_loss: 0.2959330379962921\n",
            "13430 val_loss: 0.37493687868118286, train_loss: 0.2954498827457428\n",
            "13440 val_loss: 0.374808132648468, train_loss: 0.29544737935066223\n",
            "13450 val_loss: 0.3745731711387634, train_loss: 0.295230895280838\n",
            "13460 val_loss: 0.37438011169433594, train_loss: 0.29487061500549316\n",
            "13470 val_loss: 0.3743405044078827, train_loss: 0.29477205872535706\n",
            "13480 val_loss: 0.37423938512802124, train_loss: 0.2944856882095337\n",
            "13490 val_loss: 0.37397998571395874, train_loss: 0.29441389441490173\n",
            "13500 val_loss: 0.3735884726047516, train_loss: 0.29394179582595825\n",
            "13510 val_loss: 0.37345069646835327, train_loss: 0.29399803280830383\n",
            "13520 val_loss: 0.3733687102794647, train_loss: 0.2938149869441986\n",
            "13530 val_loss: 0.3733668923377991, train_loss: 0.29359954595565796\n",
            "13540 val_loss: 0.3729124963283539, train_loss: 0.29311972856521606\n",
            "13550 val_loss: 0.37261635065078735, train_loss: 0.29273685812950134\n",
            "13560 val_loss: 0.3724302053451538, train_loss: 0.29244956374168396\n",
            "13570 val_loss: 0.37200531363487244, train_loss: 0.29215025901794434\n",
            "13580 val_loss: 0.37172454595565796, train_loss: 0.2919449210166931\n",
            "13590 val_loss: 0.3716031312942505, train_loss: 0.2916576862335205\n",
            "13600 val_loss: 0.37178704142570496, train_loss: 0.2918021082878113\n",
            "13610 val_loss: 0.3717957139015198, train_loss: 0.2917066812515259\n",
            "13620 val_loss: 0.37203946709632874, train_loss: 0.29191088676452637\n",
            "13630 val_loss: 0.3707101345062256, train_loss: 0.29112038016319275\n",
            "13640 val_loss: 0.3705366551876068, train_loss: 0.29062896966934204\n",
            "13650 val_loss: 0.3703421354293823, train_loss: 0.29037952423095703\n",
            "13660 val_loss: 0.37047094106674194, train_loss: 0.29038751125335693\n",
            "13670 val_loss: 0.3702508807182312, train_loss: 0.2902257442474365\n",
            "13680 val_loss: 0.3698873519897461, train_loss: 0.29003942012786865\n",
            "13690 val_loss: 0.36945241689682007, train_loss: 0.28991568088531494\n",
            "13700 val_loss: 0.36941754817962646, train_loss: 0.2898048758506775\n",
            "13710 val_loss: 0.3691309690475464, train_loss: 0.2895662784576416\n",
            "13720 val_loss: 0.36889684200286865, train_loss: 0.28905802965164185\n",
            "13730 val_loss: 0.36876678466796875, train_loss: 0.28911951184272766\n",
            "13740 val_loss: 0.3687613904476166, train_loss: 0.2888215482234955\n",
            "13750 val_loss: 0.36832085251808167, train_loss: 0.2883964478969574\n",
            "13760 val_loss: 0.3682612478733063, train_loss: 0.2881179451942444\n",
            "13770 val_loss: 0.36858782172203064, train_loss: 0.2882560193538666\n",
            "13780 val_loss: 0.3682917058467865, train_loss: 0.2880415916442871\n",
            "13790 val_loss: 0.36799126863479614, train_loss: 0.2878301441669464\n",
            "13800 val_loss: 0.3680013418197632, train_loss: 0.2875540554523468\n",
            "13810 val_loss: 0.3674817681312561, train_loss: 0.28716573119163513\n",
            "13820 val_loss: 0.3672512471675873, train_loss: 0.28671935200691223\n",
            "13830 val_loss: 0.3672613799571991, train_loss: 0.28670430183410645\n",
            "13840 val_loss: 0.3668053150177002, train_loss: 0.28627270460128784\n",
            "13850 val_loss: 0.3664815127849579, train_loss: 0.28575652837753296\n",
            "13860 val_loss: 0.3661172091960907, train_loss: 0.28545430302619934\n",
            "13870 val_loss: 0.36595168709754944, train_loss: 0.285126268863678\n",
            "13880 val_loss: 0.3656345009803772, train_loss: 0.2849496603012085\n",
            "13890 val_loss: 0.36566901206970215, train_loss: 0.2851034998893738\n",
            "13900 val_loss: 0.36526912450790405, train_loss: 0.28472331166267395\n",
            "13910 val_loss: 0.3653368651866913, train_loss: 0.2844569683074951\n",
            "13920 val_loss: 0.36468565464019775, train_loss: 0.2839924097061157\n",
            "13930 val_loss: 0.3648030757904053, train_loss: 0.28386548161506653\n",
            "13940 val_loss: 0.3646768629550934, train_loss: 0.28356876969337463\n",
            "13950 val_loss: 0.36440038681030273, train_loss: 0.2834725081920624\n",
            "13960 val_loss: 0.36425328254699707, train_loss: 0.283484548330307\n",
            "13970 val_loss: 0.36395999789237976, train_loss: 0.283158540725708\n",
            "13980 val_loss: 0.3639141619205475, train_loss: 0.2830457389354706\n",
            "13990 val_loss: 0.36372846364974976, train_loss: 0.2827988564968109\n",
            "14000 val_loss: 0.36358320713043213, train_loss: 0.2826900780200958\n",
            "14010 val_loss: 0.36333030462265015, train_loss: 0.28245529532432556\n",
            "14020 val_loss: 0.3631754517555237, train_loss: 0.2821161448955536\n",
            "14030 val_loss: 0.3629092574119568, train_loss: 0.28198951482772827\n",
            "14040 val_loss: 0.3630588948726654, train_loss: 0.2819141745567322\n",
            "14050 val_loss: 0.36286473274230957, train_loss: 0.2816806435585022\n",
            "14060 val_loss: 0.36276936531066895, train_loss: 0.28148385882377625\n",
            "14070 val_loss: 0.3624502718448639, train_loss: 0.2812226712703705\n",
            "14080 val_loss: 0.36245104670524597, train_loss: 0.28128916025161743\n",
            "14090 val_loss: 0.3622523248195648, train_loss: 0.2810015380382538\n",
            "14100 val_loss: 0.36213356256484985, train_loss: 0.2806916832923889\n",
            "14110 val_loss: 0.36168304085731506, train_loss: 0.28032851219177246\n",
            "14120 val_loss: 0.3617084324359894, train_loss: 0.28021568059921265\n",
            "14130 val_loss: 0.3618081212043762, train_loss: 0.2801372706890106\n",
            "14140 val_loss: 0.3615790009498596, train_loss: 0.28001734614372253\n",
            "14150 val_loss: 0.36130017042160034, train_loss: 0.2796553075313568\n",
            "14160 val_loss: 0.3609082102775574, train_loss: 0.27931010723114014\n",
            "14170 val_loss: 0.36092689633369446, train_loss: 0.2792629301548004\n",
            "14180 val_loss: 0.3608429431915283, train_loss: 0.2789231836795807\n",
            "14190 val_loss: 0.36046990752220154, train_loss: 0.2786542773246765\n",
            "14200 val_loss: 0.3605673313140869, train_loss: 0.27861487865448\n",
            "14210 val_loss: 0.3603208363056183, train_loss: 0.278301477432251\n",
            "14220 val_loss: 0.360098659992218, train_loss: 0.27812156081199646\n",
            "14230 val_loss: 0.35983651876449585, train_loss: 0.2778461277484894\n",
            "14240 val_loss: 0.35968470573425293, train_loss: 0.2777886688709259\n",
            "14250 val_loss: 0.359512597322464, train_loss: 0.2773820757865906\n",
            "14260 val_loss: 0.35970088839530945, train_loss: 0.2773652970790863\n",
            "14270 val_loss: 0.359214723110199, train_loss: 0.2769019901752472\n",
            "14280 val_loss: 0.3591952919960022, train_loss: 0.2767679691314697\n",
            "14290 val_loss: 0.3588966727256775, train_loss: 0.2763594686985016\n",
            "14300 val_loss: 0.35924583673477173, train_loss: 0.2764970362186432\n",
            "14310 val_loss: 0.35916799306869507, train_loss: 0.276429682970047\n",
            "14320 val_loss: 0.35866808891296387, train_loss: 0.2760562598705292\n",
            "14330 val_loss: 0.3585429787635803, train_loss: 0.2758178412914276\n",
            "14340 val_loss: 0.3584035634994507, train_loss: 0.27556923031806946\n",
            "14350 val_loss: 0.3585294187068939, train_loss: 0.275500625371933\n",
            "14360 val_loss: 0.3585285544395447, train_loss: 0.2756476402282715\n",
            "14370 val_loss: 0.3581303656101227, train_loss: 0.2750694453716278\n",
            "14380 val_loss: 0.3578656315803528, train_loss: 0.2747347056865692\n",
            "14390 val_loss: 0.35742974281311035, train_loss: 0.2744499444961548\n",
            "14400 val_loss: 0.35729408264160156, train_loss: 0.27428174018859863\n",
            "14410 val_loss: 0.3572816550731659, train_loss: 0.2741489112377167\n",
            "14420 val_loss: 0.3572853207588196, train_loss: 0.273921400308609\n",
            "14430 val_loss: 0.3571280837059021, train_loss: 0.27383852005004883\n",
            "14440 val_loss: 0.35698360204696655, train_loss: 0.27363333106040955\n",
            "14450 val_loss: 0.3568904995918274, train_loss: 0.27338019013404846\n",
            "14460 val_loss: 0.35712429881095886, train_loss: 0.27355825901031494\n",
            "14470 val_loss: 0.35738691687583923, train_loss: 0.27364790439605713\n",
            "14480 val_loss: 0.35656166076660156, train_loss: 0.2731878459453583\n",
            "14490 val_loss: 0.3564327359199524, train_loss: 0.2729758322238922\n",
            "14500 val_loss: 0.35600122809410095, train_loss: 0.27262231707572937\n",
            "14510 val_loss: 0.3563026785850525, train_loss: 0.2728336751461029\n",
            "14520 val_loss: 0.35608676075935364, train_loss: 0.27255702018737793\n",
            "14530 val_loss: 0.3561132848262787, train_loss: 0.2725682556629181\n",
            "14540 val_loss: 0.3561176061630249, train_loss: 0.27276453375816345\n",
            "14550 val_loss: 0.35583004355430603, train_loss: 0.27202239632606506\n",
            "14560 val_loss: 0.3555358350276947, train_loss: 0.2715831398963928\n",
            "14570 val_loss: 0.3554156720638275, train_loss: 0.2716272175312042\n",
            "14580 val_loss: 0.3551253080368042, train_loss: 0.2713521718978882\n",
            "14590 val_loss: 0.3552325367927551, train_loss: 0.27149805426597595\n",
            "14600 val_loss: 0.3557591438293457, train_loss: 0.27176567912101746\n",
            "14610 val_loss: 0.3550602197647095, train_loss: 0.27126067876815796\n",
            "14620 val_loss: 0.3554422855377197, train_loss: 0.27125445008277893\n",
            "14630 val_loss: 0.3545668125152588, train_loss: 0.2705477774143219\n",
            "14640 val_loss: 0.3547864556312561, train_loss: 0.2704852521419525\n",
            "14650 val_loss: 0.3542574942111969, train_loss: 0.2697671055793762\n",
            "14660 val_loss: 0.35413840413093567, train_loss: 0.26952123641967773\n",
            "14670 val_loss: 0.3540728986263275, train_loss: 0.26934319734573364\n",
            "14680 val_loss: 0.35395151376724243, train_loss: 0.26958897709846497\n",
            "14690 val_loss: 0.3537967801094055, train_loss: 0.26915448904037476\n",
            "14700 val_loss: 0.3533869683742523, train_loss: 0.26873669028282166\n",
            "14710 val_loss: 0.35321298241615295, train_loss: 0.26852843165397644\n",
            "14720 val_loss: 0.3531375527381897, train_loss: 0.26837024092674255\n",
            "14730 val_loss: 0.35304704308509827, train_loss: 0.268386572599411\n",
            "14740 val_loss: 0.3535872995853424, train_loss: 0.2684313654899597\n",
            "14750 val_loss: 0.352600634098053, train_loss: 0.2677704989910126\n",
            "14760 val_loss: 0.3523654341697693, train_loss: 0.2675597667694092\n",
            "14770 val_loss: 0.3524470925331116, train_loss: 0.26732710003852844\n",
            "14780 val_loss: 0.3520730435848236, train_loss: 0.26690754294395447\n",
            "14790 val_loss: 0.35194021463394165, train_loss: 0.266823410987854\n",
            "14800 val_loss: 0.3519468903541565, train_loss: 0.26685190200805664\n",
            "14810 val_loss: 0.3520941734313965, train_loss: 0.26675117015838623\n",
            "14820 val_loss: 0.352013498544693, train_loss: 0.2665826380252838\n",
            "14830 val_loss: 0.35248225927352905, train_loss: 0.2668308615684509\n",
            "14840 val_loss: 0.35239365696907043, train_loss: 0.26652565598487854\n",
            "14850 val_loss: 0.3521880507469177, train_loss: 0.26592767238616943\n",
            "14860 val_loss: 0.3524654805660248, train_loss: 0.2656831443309784\n",
            "14870 val_loss: 0.35211122035980225, train_loss: 0.26572564244270325\n",
            "14880 val_loss: 0.3517894148826599, train_loss: 0.2652890086174011\n",
            "14890 val_loss: 0.35183006525039673, train_loss: 0.264914870262146\n",
            "14900 val_loss: 0.3513706624507904, train_loss: 0.2642081081867218\n",
            "14910 val_loss: 0.3514152765274048, train_loss: 0.2638460397720337\n",
            "14920 val_loss: 0.3507530689239502, train_loss: 0.2639116048812866\n",
            "14930 val_loss: 0.35086193680763245, train_loss: 0.2638285756111145\n",
            "14940 val_loss: 0.35047680139541626, train_loss: 0.2635628283023834\n",
            "14950 val_loss: 0.3507721424102783, train_loss: 0.2630384564399719\n",
            "14960 val_loss: 0.3504244387149811, train_loss: 0.2628744840621948\n",
            "14970 val_loss: 0.3504633605480194, train_loss: 0.262470006942749\n",
            "14980 val_loss: 0.3500562012195587, train_loss: 0.2621808350086212\n",
            "14990 val_loss: 0.3500042259693146, train_loss: 0.26179584860801697\n",
            "15000 val_loss: 0.35025468468666077, train_loss: 0.2616565227508545\n",
            "15010 val_loss: 0.3498741686344147, train_loss: 0.26140913367271423\n",
            "15020 val_loss: 0.3492874205112457, train_loss: 0.2614102363586426\n",
            "15030 val_loss: 0.3495416045188904, train_loss: 0.2609767019748688\n",
            "15040 val_loss: 0.3492051661014557, train_loss: 0.26090314984321594\n",
            "15050 val_loss: 0.34942954778671265, train_loss: 0.2605408728122711\n",
            "15060 val_loss: 0.3490165174007416, train_loss: 0.2607472538948059\n",
            "15070 val_loss: 0.3487488925457001, train_loss: 0.2601797878742218\n",
            "15080 val_loss: 0.3489963710308075, train_loss: 0.2597179710865021\n",
            "15090 val_loss: 0.34859371185302734, train_loss: 0.2596825957298279\n",
            "15100 val_loss: 0.3495124578475952, train_loss: 0.25928232073783875\n",
            "15110 val_loss: 0.3485606610774994, train_loss: 0.25886550545692444\n",
            "15120 val_loss: 0.348149836063385, train_loss: 0.25860312581062317\n",
            "15130 val_loss: 0.34841322898864746, train_loss: 0.25823044776916504\n",
            "15140 val_loss: 0.34768256545066833, train_loss: 0.257915735244751\n",
            "15150 val_loss: 0.34722229838371277, train_loss: 0.2577102482318878\n",
            "15160 val_loss: 0.34686362743377686, train_loss: 0.2575817108154297\n",
            "15170 val_loss: 0.346916526556015, train_loss: 0.2575368583202362\n",
            "15180 val_loss: 0.3485192060470581, train_loss: 0.2573234438896179\n",
            "15190 val_loss: 0.34693393111228943, train_loss: 0.2565706968307495\n",
            "15200 val_loss: 0.34756723046302795, train_loss: 0.2565166652202606\n",
            "15210 val_loss: 0.3470941483974457, train_loss: 0.25634992122650146\n",
            "15220 val_loss: 0.3467184603214264, train_loss: 0.25594577193260193\n",
            "15230 val_loss: 0.34643200039863586, train_loss: 0.2555519640445709\n",
            "15240 val_loss: 0.3459007143974304, train_loss: 0.25531506538391113\n",
            "15250 val_loss: 0.34638601541519165, train_loss: 0.25516852736473083\n",
            "15260 val_loss: 0.34591060876846313, train_loss: 0.25481289625167847\n",
            "15270 val_loss: 0.3456988036632538, train_loss: 0.25465649366378784\n",
            "15280 val_loss: 0.34587961435317993, train_loss: 0.2546692490577698\n",
            "15290 val_loss: 0.34641730785369873, train_loss: 0.2544555962085724\n",
            "15300 val_loss: 0.3465266227722168, train_loss: 0.25436294078826904\n",
            "15310 val_loss: 0.34480270743370056, train_loss: 0.2536543607711792\n",
            "15320 val_loss: 0.3445935547351837, train_loss: 0.25351089239120483\n",
            "15330 val_loss: 0.344940721988678, train_loss: 0.2529619634151459\n",
            "15340 val_loss: 0.3441484868526459, train_loss: 0.25286760926246643\n",
            "15350 val_loss: 0.3439686894416809, train_loss: 0.25236570835113525\n",
            "15360 val_loss: 0.3442934453487396, train_loss: 0.25218695402145386\n",
            "15370 val_loss: 0.34490305185317993, train_loss: 0.2520054280757904\n",
            "15380 val_loss: 0.34486353397369385, train_loss: 0.25172293186187744\n",
            "15390 val_loss: 0.3452708125114441, train_loss: 0.2516404092311859\n",
            "15400 val_loss: 0.3450368344783783, train_loss: 0.2513939142227173\n",
            "15410 val_loss: 0.3451415002346039, train_loss: 0.2512974441051483\n",
            "15420 val_loss: 0.3449941873550415, train_loss: 0.25110214948654175\n",
            "15430 val_loss: 0.34417787194252014, train_loss: 0.25065526366233826\n",
            "15440 val_loss: 0.34365004301071167, train_loss: 0.25022831559181213\n",
            "15450 val_loss: 0.3439846634864807, train_loss: 0.2499387562274933\n",
            "15460 val_loss: 0.34295547008514404, train_loss: 0.2498670518398285\n",
            "15470 val_loss: 0.3433302044868469, train_loss: 0.24938108026981354\n",
            "15480 val_loss: 0.343432754278183, train_loss: 0.24952973425388336\n",
            "15490 val_loss: 0.3426218032836914, train_loss: 0.24966508150100708\n",
            "15500 val_loss: 0.3450412154197693, train_loss: 0.24967163801193237\n",
            "15510 val_loss: 0.3428999185562134, train_loss: 0.24870789051055908\n",
            "15520 val_loss: 0.342651903629303, train_loss: 0.24861398339271545\n",
            "15530 val_loss: 0.3434518277645111, train_loss: 0.24831987917423248\n",
            "15540 val_loss: 0.3427562713623047, train_loss: 0.24800823628902435\n",
            "15550 val_loss: 0.3428729772567749, train_loss: 0.2477497160434723\n",
            "15560 val_loss: 0.34106481075286865, train_loss: 0.24729914963245392\n",
            "15570 val_loss: 0.3429771661758423, train_loss: 0.24751712381839752\n",
            "15580 val_loss: 0.3407975435256958, train_loss: 0.24778732657432556\n",
            "15590 val_loss: 0.3406834602355957, train_loss: 0.24678337574005127\n",
            "15600 val_loss: 0.3414868414402008, train_loss: 0.24653199315071106\n",
            "15610 val_loss: 0.34329143166542053, train_loss: 0.2470148801803589\n",
            "15620 val_loss: 0.3403100371360779, train_loss: 0.24674279987812042\n",
            "15630 val_loss: 0.34216856956481934, train_loss: 0.24611440300941467\n",
            "15640 val_loss: 0.34161463379859924, train_loss: 0.2456691563129425\n",
            "15650 val_loss: 0.3424883186817169, train_loss: 0.2454036921262741\n",
            "15660 val_loss: 0.3405012786388397, train_loss: 0.24513296782970428\n",
            "15670 val_loss: 0.3404170870780945, train_loss: 0.2447691559791565\n",
            "15680 val_loss: 0.3415336608886719, train_loss: 0.2443479299545288\n",
            "15690 val_loss: 0.34138578176498413, train_loss: 0.24425199627876282\n",
            "15700 val_loss: 0.34096863865852356, train_loss: 0.24394948780536652\n",
            "15710 val_loss: 0.3406887650489807, train_loss: 0.24348239600658417\n",
            "15720 val_loss: 0.3430570065975189, train_loss: 0.24409139156341553\n",
            "15730 val_loss: 0.34180372953414917, train_loss: 0.24355430901050568\n",
            "15740 val_loss: 0.34047189354896545, train_loss: 0.24310055375099182\n",
            "15750 val_loss: 0.3418061137199402, train_loss: 0.24311184883117676\n",
            "15760 val_loss: 0.33977261185646057, train_loss: 0.24270930886268616\n",
            "15770 val_loss: 0.34178096055984497, train_loss: 0.24241401255130768\n",
            "15780 val_loss: 0.34101176261901855, train_loss: 0.24206990003585815\n",
            "15790 val_loss: 0.3395242393016815, train_loss: 0.2415429651737213\n",
            "15800 val_loss: 0.3396938741207123, train_loss: 0.2413182407617569\n",
            "15810 val_loss: 0.3382929265499115, train_loss: 0.24098022282123566\n",
            "15820 val_loss: 0.3383443355560303, train_loss: 0.24055741727352142\n",
            "15830 val_loss: 0.3381522595882416, train_loss: 0.24019566178321838\n",
            "15840 val_loss: 0.33810800313949585, train_loss: 0.2404845654964447\n",
            "15850 val_loss: 0.3395145833492279, train_loss: 0.2400824874639511\n",
            "15860 val_loss: 0.3393822908401489, train_loss: 0.23987796902656555\n",
            "15870 val_loss: 0.3390357196331024, train_loss: 0.23946736752986908\n",
            "15880 val_loss: 0.3385358154773712, train_loss: 0.2388371080160141\n",
            "15890 val_loss: 0.3398990035057068, train_loss: 0.23925378918647766\n",
            "15900 val_loss: 0.3395012319087982, train_loss: 0.23904113471508026\n",
            "15910 val_loss: 0.3392631411552429, train_loss: 0.23863837122917175\n",
            "15920 val_loss: 0.340555340051651, train_loss: 0.2384214848279953\n",
            "15930 val_loss: 0.3381066918373108, train_loss: 0.23779499530792236\n",
            "15940 val_loss: 0.3403962254524231, train_loss: 0.23816922307014465\n",
            "15950 val_loss: 0.3403778672218323, train_loss: 0.23797999322414398\n",
            "15960 val_loss: 0.33896738290786743, train_loss: 0.23745958507061005\n",
            "15970 val_loss: 0.33758655190467834, train_loss: 0.23690509796142578\n",
            "15980 val_loss: 0.33635133504867554, train_loss: 0.23666706681251526\n",
            "15990 val_loss: 0.3393127918243408, train_loss: 0.23670928180217743\n",
            "16000 val_loss: 0.3391035199165344, train_loss: 0.23622086644172668\n",
            "16010 val_loss: 0.3403017222881317, train_loss: 0.23617517948150635\n",
            "16020 val_loss: 0.33769720792770386, train_loss: 0.23525559902191162\n",
            "16030 val_loss: 0.33884894847869873, train_loss: 0.2351975291967392\n",
            "16040 val_loss: 0.34030881524086, train_loss: 0.235203817486763\n",
            "16050 val_loss: 0.33871299028396606, train_loss: 0.23404742777347565\n",
            "16060 val_loss: 0.3376289904117584, train_loss: 0.23349516093730927\n",
            "16070 val_loss: 0.3356010317802429, train_loss: 0.23339298367500305\n",
            "16080 val_loss: 0.3376905918121338, train_loss: 0.23281578719615936\n",
            "16090 val_loss: 0.33957788348197937, train_loss: 0.23310725390911102\n",
            "16100 val_loss: 0.33829784393310547, train_loss: 0.23254820704460144\n",
            "16110 val_loss: 0.3385382890701294, train_loss: 0.23204544186592102\n",
            "16120 val_loss: 0.3372509777545929, train_loss: 0.23141887784004211\n",
            "16130 val_loss: 0.33678945899009705, train_loss: 0.23134447634220123\n",
            "16140 val_loss: 0.33644595742225647, train_loss: 0.23095780611038208\n",
            "16150 val_loss: 0.3380240797996521, train_loss: 0.23134151101112366\n",
            "16160 val_loss: 0.3382973372936249, train_loss: 0.2308763712644577\n",
            "16170 val_loss: 0.3392292559146881, train_loss: 0.23073960840702057\n",
            "16180 val_loss: 0.3352356553077698, train_loss: 0.23025216162204742\n",
            "16190 val_loss: 0.33748677372932434, train_loss: 0.22968889772891998\n",
            "16200 val_loss: 0.33709031343460083, train_loss: 0.22941970825195312\n",
            "16210 val_loss: 0.3354448974132538, train_loss: 0.22911368310451508\n",
            "16220 val_loss: 0.33902621269226074, train_loss: 0.22913359105587006\n",
            "16230 val_loss: 0.33798229694366455, train_loss: 0.2285420000553131\n",
            "16240 val_loss: 0.3367651104927063, train_loss: 0.2287474274635315\n",
            "16250 val_loss: 0.3366619646549225, train_loss: 0.22837689518928528\n",
            "16260 val_loss: 0.3348143994808197, train_loss: 0.22841127216815948\n",
            "16270 val_loss: 0.33567261695861816, train_loss: 0.22767160832881927\n",
            "16280 val_loss: 0.3386231064796448, train_loss: 0.22772197425365448\n",
            "16290 val_loss: 0.33475393056869507, train_loss: 0.2268480807542801\n",
            "16300 val_loss: 0.33538728952407837, train_loss: 0.22674119472503662\n",
            "16310 val_loss: 0.33550965785980225, train_loss: 0.22642938792705536\n",
            "16320 val_loss: 0.33772537112236023, train_loss: 0.2263496220111847\n",
            "16330 val_loss: 0.3368435502052307, train_loss: 0.22581462562084198\n",
            "16340 val_loss: 0.33587169647216797, train_loss: 0.2254020720720291\n",
            "16350 val_loss: 0.3366909623146057, train_loss: 0.2251567840576172\n",
            "16360 val_loss: 0.33540523052215576, train_loss: 0.22494705021381378\n",
            "16370 val_loss: 0.3390839099884033, train_loss: 0.22527897357940674\n",
            "16380 val_loss: 0.33379560708999634, train_loss: 0.22425618767738342\n",
            "16390 val_loss: 0.33575814962387085, train_loss: 0.22398661077022552\n",
            "16400 val_loss: 0.33315309882164, train_loss: 0.22387224435806274\n",
            "16410 val_loss: 0.33130815625190735, train_loss: 0.22396759688854218\n",
            "16420 val_loss: 0.33761507272720337, train_loss: 0.22395716607570648\n",
            "16430 val_loss: 0.33408474922180176, train_loss: 0.22314110398292542\n",
            "16440 val_loss: 0.3358900845050812, train_loss: 0.2231084704399109\n",
            "16450 val_loss: 0.3366663157939911, train_loss: 0.2228272557258606\n",
            "16460 val_loss: 0.33338406682014465, train_loss: 0.22189240157604218\n",
            "16470 val_loss: 0.3323300778865814, train_loss: 0.22183144092559814\n",
            "16480 val_loss: 0.3325604498386383, train_loss: 0.22128981351852417\n",
            "16490 val_loss: 0.33224520087242126, train_loss: 0.2210627645254135\n",
            "16500 val_loss: 0.3347541391849518, train_loss: 0.22114506363868713\n",
            "16510 val_loss: 0.3360283076763153, train_loss: 0.2213522046804428\n",
            "16520 val_loss: 0.33899131417274475, train_loss: 0.22194325923919678\n",
            "16530 val_loss: 0.3346828818321228, train_loss: 0.22079098224639893\n",
            "16540 val_loss: 0.3321212828159332, train_loss: 0.21991553902626038\n",
            "16550 val_loss: 0.3366686999797821, train_loss: 0.2200111299753189\n",
            "16560 val_loss: 0.3335553705692291, train_loss: 0.21969261765480042\n",
            "16570 val_loss: 0.33087944984436035, train_loss: 0.21938782930374146\n",
            "16580 val_loss: 0.33168715238571167, train_loss: 0.21910084784030914\n",
            "16590 val_loss: 0.3321560323238373, train_loss: 0.2186141312122345\n",
            "16600 val_loss: 0.33478477597236633, train_loss: 0.2186872512102127\n",
            "16610 val_loss: 0.33092960715293884, train_loss: 0.21863828599452972\n",
            "16620 val_loss: 0.33045700192451477, train_loss: 0.21808743476867676\n",
            "16630 val_loss: 0.3329918384552002, train_loss: 0.21805094182491302\n",
            "16640 val_loss: 0.32911333441734314, train_loss: 0.21803291141986847\n",
            "16650 val_loss: 0.3325658142566681, train_loss: 0.21721096336841583\n",
            "16660 val_loss: 0.34016260504722595, train_loss: 0.21919484436511993\n",
            "16670 val_loss: 0.33533939719200134, train_loss: 0.21727505326271057\n",
            "16680 val_loss: 0.33378809690475464, train_loss: 0.21692612767219543\n",
            "16690 val_loss: 0.33175164461135864, train_loss: 0.2164974957704544\n",
            "16700 val_loss: 0.3299618363380432, train_loss: 0.21596679091453552\n",
            "16710 val_loss: 0.3304517865180969, train_loss: 0.21604347229003906\n",
            "16720 val_loss: 0.3297170400619507, train_loss: 0.21559211611747742\n",
            "16730 val_loss: 0.32913702726364136, train_loss: 0.21549543738365173\n",
            "16740 val_loss: 0.33171334862709045, train_loss: 0.21531395614147186\n",
            "16750 val_loss: 0.33328959345817566, train_loss: 0.2152763456106186\n",
            "16760 val_loss: 0.32913053035736084, train_loss: 0.21533800661563873\n",
            "16770 val_loss: 0.3318435549736023, train_loss: 0.21506056189537048\n",
            "16780 val_loss: 0.33069148659706116, train_loss: 0.21437931060791016\n",
            "16790 val_loss: 0.3332902789115906, train_loss: 0.21466884016990662\n",
            "16800 val_loss: 0.33118700981140137, train_loss: 0.21381129324436188\n",
            "16810 val_loss: 0.32951104640960693, train_loss: 0.2138558030128479\n",
            "16820 val_loss: 0.3285362124443054, train_loss: 0.2135227769613266\n",
            "16830 val_loss: 0.333111435174942, train_loss: 0.2137143462896347\n",
            "16840 val_loss: 0.325662761926651, train_loss: 0.21349027752876282\n",
            "16850 val_loss: 0.33261650800704956, train_loss: 0.21373692154884338\n",
            "16860 val_loss: 0.3357490003108978, train_loss: 0.21386641263961792\n",
            "16870 val_loss: 0.33100923895835876, train_loss: 0.21291910111904144\n",
            "16880 val_loss: 0.3275676965713501, train_loss: 0.21333420276641846\n",
            "16890 val_loss: 0.32596850395202637, train_loss: 0.21320714056491852\n",
            "16900 val_loss: 0.3287373185157776, train_loss: 0.21219992637634277\n",
            "16910 val_loss: 0.3305746018886566, train_loss: 0.21235327422618866\n",
            "16920 val_loss: 0.3327522277832031, train_loss: 0.21261721849441528\n",
            "16930 val_loss: 0.3323988616466522, train_loss: 0.2120838165283203\n",
            "16940 val_loss: 0.3292244076728821, train_loss: 0.2111218273639679\n",
            "16950 val_loss: 0.329728901386261, train_loss: 0.21093925833702087\n",
            "16960 val_loss: 0.33069366216659546, train_loss: 0.2109217792749405\n",
            "16970 val_loss: 0.333222359418869, train_loss: 0.21111099421977997\n",
            "16980 val_loss: 0.3345701992511749, train_loss: 0.21127116680145264\n",
            "16990 val_loss: 0.3269037902355194, train_loss: 0.2102990299463272\n",
            "17000 val_loss: 0.3318251967430115, train_loss: 0.21039073169231415\n",
            "17010 val_loss: 0.33055949211120605, train_loss: 0.2099796086549759\n",
            "17020 val_loss: 0.33242887258529663, train_loss: 0.2099592387676239\n",
            "17030 val_loss: 0.32814329862594604, train_loss: 0.20945441722869873\n",
            "17040 val_loss: 0.33045706152915955, train_loss: 0.20923970639705658\n",
            "17050 val_loss: 0.3325847089290619, train_loss: 0.20964665710926056\n",
            "17060 val_loss: 0.33059296011924744, train_loss: 0.20940715074539185\n",
            "17070 val_loss: 0.3318606913089752, train_loss: 0.20903518795967102\n",
            "17080 val_loss: 0.3273533582687378, train_loss: 0.20860043168067932\n",
            "17090 val_loss: 0.33179008960723877, train_loss: 0.208778515458107\n",
            "17100 val_loss: 0.32763707637786865, train_loss: 0.20798227190971375\n",
            "17110 val_loss: 0.3349681496620178, train_loss: 0.20938821136951447\n",
            "17120 val_loss: 0.32798999547958374, train_loss: 0.20752085745334625\n",
            "17130 val_loss: 0.32741495966911316, train_loss: 0.20735815167427063\n",
            "17140 val_loss: 0.3262536823749542, train_loss: 0.2070206254720688\n",
            "17150 val_loss: 0.32517093420028687, train_loss: 0.20690427720546722\n",
            "17160 val_loss: 0.3331826627254486, train_loss: 0.20776844024658203\n",
            "17170 val_loss: 0.32875514030456543, train_loss: 0.20668639242649078\n",
            "17180 val_loss: 0.32724085450172424, train_loss: 0.20630910992622375\n",
            "17190 val_loss: 0.3329199254512787, train_loss: 0.2072959840297699\n",
            "17200 val_loss: 0.3324734568595886, train_loss: 0.206280916929245\n",
            "17210 val_loss: 0.3303912878036499, train_loss: 0.20568077266216278\n",
            "17220 val_loss: 0.32940366864204407, train_loss: 0.20591530203819275\n",
            "17230 val_loss: 0.32607510685920715, train_loss: 0.20512118935585022\n",
            "17240 val_loss: 0.33190977573394775, train_loss: 0.20535193383693695\n",
            "17250 val_loss: 0.33071190118789673, train_loss: 0.2049400806427002\n",
            "17260 val_loss: 0.33530113101005554, train_loss: 0.20566493272781372\n",
            "17270 val_loss: 0.32522493600845337, train_loss: 0.20433983206748962\n",
            "17280 val_loss: 0.3367912173271179, train_loss: 0.2058330625295639\n",
            "17290 val_loss: 0.33205142617225647, train_loss: 0.20455673336982727\n",
            "17300 val_loss: 0.3281899392604828, train_loss: 0.20354044437408447\n",
            "17310 val_loss: 0.33463144302368164, train_loss: 0.2043200135231018\n",
            "17320 val_loss: 0.3298085033893585, train_loss: 0.20363971590995789\n",
            "17330 val_loss: 0.3244834542274475, train_loss: 0.2032669633626938\n",
            "17340 val_loss: 0.33171629905700684, train_loss: 0.2033856064081192\n",
            "17350 val_loss: 0.33353981375694275, train_loss: 0.20378002524375916\n",
            "17360 val_loss: 0.32697129249572754, train_loss: 0.2024480104446411\n",
            "17370 val_loss: 0.32594773173332214, train_loss: 0.20241808891296387\n",
            "17380 val_loss: 0.32610899209976196, train_loss: 0.2019134759902954\n",
            "17390 val_loss: 0.32822808623313904, train_loss: 0.2017756998538971\n",
            "17400 val_loss: 0.33098429441452026, train_loss: 0.2016988843679428\n",
            "17410 val_loss: 0.3250388503074646, train_loss: 0.20123590528964996\n",
            "17420 val_loss: 0.3284953832626343, train_loss: 0.20107905566692352\n",
            "17430 val_loss: 0.32716089487075806, train_loss: 0.2014322280883789\n",
            "17440 val_loss: 0.3311367332935333, train_loss: 0.20125484466552734\n",
            "17450 val_loss: 0.33974480628967285, train_loss: 0.2031029462814331\n",
            "17460 val_loss: 0.3279772102832794, train_loss: 0.20029962062835693\n",
            "17470 val_loss: 0.33051738142967224, train_loss: 0.2000793218612671\n",
            "17480 val_loss: 0.33012035489082336, train_loss: 0.19990098476409912\n",
            "17490 val_loss: 0.3308389186859131, train_loss: 0.1997675597667694\n",
            "17500 val_loss: 0.3324702978134155, train_loss: 0.20002470910549164\n",
            "17510 val_loss: 0.32810941338539124, train_loss: 0.19964422285556793\n",
            "17520 val_loss: 0.3335593640804291, train_loss: 0.20011526346206665\n",
            "17530 val_loss: 0.32295963168144226, train_loss: 0.19932380318641663\n",
            "17540 val_loss: 0.3256755769252777, train_loss: 0.198553204536438\n",
            "17550 val_loss: 0.33512935042381287, train_loss: 0.20021900534629822\n",
            "17560 val_loss: 0.3209885060787201, train_loss: 0.1987144500017166\n",
            "17570 val_loss: 0.33060890436172485, train_loss: 0.19854754209518433\n",
            "17580 val_loss: 0.3278433382511139, train_loss: 0.19810810685157776\n",
            "17590 val_loss: 0.32636183500289917, train_loss: 0.19787004590034485\n",
            "17600 val_loss: 0.3256697356700897, train_loss: 0.19803236424922943\n",
            "17610 val_loss: 0.3252066969871521, train_loss: 0.1977105438709259\n",
            "17620 val_loss: 0.3301825225353241, train_loss: 0.19799844920635223\n",
            "17630 val_loss: 0.3261980712413788, train_loss: 0.19747556746006012\n",
            "17640 val_loss: 0.3191179633140564, train_loss: 0.1975313276052475\n",
            "17650 val_loss: 0.3265377879142761, train_loss: 0.19779491424560547\n",
            "17660 val_loss: 0.3327990174293518, train_loss: 0.19842331111431122\n",
            "17670 val_loss: 0.3280738592147827, train_loss: 0.19735197722911835\n",
            "17680 val_loss: 0.3234444260597229, train_loss: 0.196188285946846\n",
            "17690 val_loss: 0.3281913995742798, train_loss: 0.19667738676071167\n",
            "17700 val_loss: 0.3258609175682068, train_loss: 0.19628405570983887\n",
            "17710 val_loss: 0.3248770833015442, train_loss: 0.19606058299541473\n",
            "17720 val_loss: 0.32692280411720276, train_loss: 0.19570070505142212\n",
            "17730 val_loss: 0.3189701437950134, train_loss: 0.19557586312294006\n",
            "17740 val_loss: 0.33275631070137024, train_loss: 0.19659805297851562\n",
            "17750 val_loss: 0.32542458176612854, train_loss: 0.19523169100284576\n",
            "17760 val_loss: 0.322041779756546, train_loss: 0.19503916800022125\n",
            "17770 val_loss: 0.32088035345077515, train_loss: 0.19505855441093445\n",
            "17780 val_loss: 0.3249821364879608, train_loss: 0.19501541554927826\n",
            "17790 val_loss: 0.32983481884002686, train_loss: 0.1958475112915039\n",
            "17800 val_loss: 0.326079785823822, train_loss: 0.1948126256465912\n",
            "17810 val_loss: 0.32672181725502014, train_loss: 0.19492293894290924\n",
            "17820 val_loss: 0.3271268308162689, train_loss: 0.19463172554969788\n",
            "17830 val_loss: 0.3215610980987549, train_loss: 0.19391007721424103\n",
            "17840 val_loss: 0.31946468353271484, train_loss: 0.19424889981746674\n",
            "17850 val_loss: 0.3246966302394867, train_loss: 0.19408662617206573\n",
            "17860 val_loss: 0.32475823163986206, train_loss: 0.19390921294689178\n",
            "17870 val_loss: 0.33076155185699463, train_loss: 0.1938474178314209\n",
            "17880 val_loss: 0.32617706060409546, train_loss: 0.1938725858926773\n",
            "17890 val_loss: 0.32314518094062805, train_loss: 0.19304096698760986\n",
            "17900 val_loss: 0.327105849981308, train_loss: 0.19321665167808533\n",
            "17910 val_loss: 0.31446149945259094, train_loss: 0.19364778697490692\n",
            "17920 val_loss: 0.32747581601142883, train_loss: 0.19315433502197266\n",
            "17930 val_loss: 0.3248414695262909, train_loss: 0.19240552186965942\n",
            "17940 val_loss: 0.3274698257446289, train_loss: 0.19253121316432953\n",
            "17950 val_loss: 0.32601529359817505, train_loss: 0.19225890934467316\n",
            "17960 val_loss: 0.32488465309143066, train_loss: 0.1922711730003357\n",
            "17970 val_loss: 0.32351794838905334, train_loss: 0.19161121547222137\n",
            "17980 val_loss: 0.3331085741519928, train_loss: 0.19357866048812866\n",
            "17990 val_loss: 0.3277825713157654, train_loss: 0.19218546152114868\n",
            "18000 val_loss: 0.3310798108577728, train_loss: 0.19310730695724487\n",
            "18010 val_loss: 0.3222070336341858, train_loss: 0.19136467576026917\n",
            "18020 val_loss: 0.31609615683555603, train_loss: 0.1909928172826767\n",
            "18030 val_loss: 0.33464112877845764, train_loss: 0.1933005005121231\n",
            "18040 val_loss: 0.31733131408691406, train_loss: 0.1906615048646927\n",
            "18050 val_loss: 0.3187292516231537, train_loss: 0.18987853825092316\n",
            "18060 val_loss: 0.32119032740592957, train_loss: 0.19020813703536987\n",
            "18070 val_loss: 0.32191792130470276, train_loss: 0.18970096111297607\n",
            "18080 val_loss: 0.32356736063957214, train_loss: 0.19006779789924622\n",
            "18090 val_loss: 0.3198768198490143, train_loss: 0.1895246058702469\n",
            "18100 val_loss: 0.32219016551971436, train_loss: 0.1889709234237671\n",
            "18110 val_loss: 0.32719454169273376, train_loss: 0.19006969034671783\n",
            "18120 val_loss: 0.32370203733444214, train_loss: 0.18907271325588226\n",
            "18130 val_loss: 0.3193054795265198, train_loss: 0.18789221346378326\n",
            "18140 val_loss: 0.31985726952552795, train_loss: 0.18842913210391998\n",
            "18150 val_loss: 0.31971725821495056, train_loss: 0.1879505068063736\n",
            "18160 val_loss: 0.32363399863243103, train_loss: 0.18869329988956451\n",
            "18170 val_loss: 0.3195079565048218, train_loss: 0.18807975947856903\n",
            "18180 val_loss: 0.3216899633407593, train_loss: 0.18801146745681763\n",
            "18190 val_loss: 0.319246381521225, train_loss: 0.1878557801246643\n",
            "18200 val_loss: 0.32218417525291443, train_loss: 0.18815797567367554\n",
            "18210 val_loss: 0.3146410286426544, train_loss: 0.18750330805778503\n",
            "18220 val_loss: 0.3201596438884735, train_loss: 0.1874081790447235\n",
            "18230 val_loss: 0.32294219732284546, train_loss: 0.1870364099740982\n",
            "18240 val_loss: 0.3234628140926361, train_loss: 0.18750643730163574\n",
            "18250 val_loss: 0.31950506567955017, train_loss: 0.18630635738372803\n",
            "18260 val_loss: 0.31670716404914856, train_loss: 0.18576355278491974\n",
            "18270 val_loss: 0.31552720069885254, train_loss: 0.18518461287021637\n",
            "18280 val_loss: 0.3172202706336975, train_loss: 0.18525786697864532\n",
            "18290 val_loss: 0.3307664394378662, train_loss: 0.18765728175640106\n",
            "18300 val_loss: 0.3178699016571045, train_loss: 0.18493366241455078\n",
            "18310 val_loss: 0.318024218082428, train_loss: 0.18499897420406342\n",
            "18320 val_loss: 0.3305172622203827, train_loss: 0.18716289103031158\n",
            "18330 val_loss: 0.3250461518764496, train_loss: 0.18625709414482117\n",
            "18340 val_loss: 0.3172908127307892, train_loss: 0.18512164056301117\n",
            "18350 val_loss: 0.31582048535346985, train_loss: 0.1849527806043625\n",
            "18360 val_loss: 0.3199009895324707, train_loss: 0.18490509688854218\n",
            "18370 val_loss: 0.3186703622341156, train_loss: 0.1842467486858368\n",
            "18380 val_loss: 0.3179645538330078, train_loss: 0.18408353626728058\n",
            "18390 val_loss: 0.32222357392311096, train_loss: 0.18464674055576324\n",
            "18400 val_loss: 0.3098352253437042, train_loss: 0.18520121276378632\n",
            "18410 val_loss: 0.31961187720298767, train_loss: 0.18435898423194885\n",
            "18420 val_loss: 0.3205549418926239, train_loss: 0.18394382297992706\n",
            "18430 val_loss: 0.32706284523010254, train_loss: 0.18500250577926636\n",
            "18440 val_loss: 0.3197098672389984, train_loss: 0.1837313175201416\n",
            "18450 val_loss: 0.3141244351863861, train_loss: 0.18368631601333618\n",
            "18460 val_loss: 0.3209649622440338, train_loss: 0.18392620980739594\n",
            "18470 val_loss: 0.31988975405693054, train_loss: 0.18320275843143463\n",
            "18480 val_loss: 0.3133818209171295, train_loss: 0.18258202075958252\n",
            "18490 val_loss: 0.3157946765422821, train_loss: 0.18217328190803528\n",
            "18500 val_loss: 0.3240868151187897, train_loss: 0.18347834050655365\n",
            "18510 val_loss: 0.320319265127182, train_loss: 0.18237370252609253\n",
            "18520 val_loss: 0.3231544494628906, train_loss: 0.1833202838897705\n",
            "18530 val_loss: 0.3128422200679779, train_loss: 0.18194012343883514\n",
            "18540 val_loss: 0.31864237785339355, train_loss: 0.1821754425764084\n",
            "18550 val_loss: 0.3174692690372467, train_loss: 0.1819201111793518\n",
            "18560 val_loss: 0.32135090231895447, train_loss: 0.1824977993965149\n",
            "18570 val_loss: 0.3126041293144226, train_loss: 0.18114317953586578\n",
            "18580 val_loss: 0.31323638558387756, train_loss: 0.18127459287643433\n",
            "18590 val_loss: 0.3239152729511261, train_loss: 0.18216729164123535\n",
            "18600 val_loss: 0.3362058103084564, train_loss: 0.18524055182933807\n",
            "18610 val_loss: 0.31420573592185974, train_loss: 0.18045590817928314\n",
            "18620 val_loss: 0.31213584542274475, train_loss: 0.17964136600494385\n",
            "18630 val_loss: 0.31919726729393005, train_loss: 0.17977343499660492\n",
            "18640 val_loss: 0.3205101490020752, train_loss: 0.18000145256519318\n",
            "18650 val_loss: 0.3202849328517914, train_loss: 0.17990532517433167\n",
            "18660 val_loss: 0.3148145079612732, train_loss: 0.1793341338634491\n",
            "18670 val_loss: 0.3238564133644104, train_loss: 0.1804858297109604\n",
            "18680 val_loss: 0.31996166706085205, train_loss: 0.1800074726343155\n",
            "18690 val_loss: 0.32244154810905457, train_loss: 0.1802728772163391\n",
            "18700 val_loss: 0.3190496861934662, train_loss: 0.17907623946666718\n",
            "18710 val_loss: 0.3210398256778717, train_loss: 0.17933157086372375\n",
            "18720 val_loss: 0.3198093771934509, train_loss: 0.17864619195461273\n",
            "18730 val_loss: 0.3226664066314697, train_loss: 0.17909200489521027\n",
            "18740 val_loss: 0.31711870431900024, train_loss: 0.17854464054107666\n",
            "18750 val_loss: 0.31657087802886963, train_loss: 0.1777193695306778\n",
            "18760 val_loss: 0.32174357771873474, train_loss: 0.17797671258449554\n",
            "18770 val_loss: 0.31746700406074524, train_loss: 0.1766977459192276\n",
            "18780 val_loss: 0.3154802620410919, train_loss: 0.17655974626541138\n",
            "18790 val_loss: 0.31988680362701416, train_loss: 0.17713946104049683\n",
            "18800 val_loss: 0.3152301013469696, train_loss: 0.17626921832561493\n",
            "18810 val_loss: 0.3137829899787903, train_loss: 0.17577548325061798\n",
            "18820 val_loss: 0.31450018286705017, train_loss: 0.17591315507888794\n",
            "18830 val_loss: 0.31796231865882874, train_loss: 0.1762399524450302\n",
            "18840 val_loss: 0.3129540979862213, train_loss: 0.17579694092273712\n",
            "18850 val_loss: 0.3161450922489166, train_loss: 0.1754922866821289\n",
            "18860 val_loss: 0.315860390663147, train_loss: 0.17528916895389557\n",
            "18870 val_loss: 0.32011738419532776, train_loss: 0.17579849064350128\n",
            "18880 val_loss: 0.30947449803352356, train_loss: 0.17545185983181\n",
            "18890 val_loss: 0.31778988242149353, train_loss: 0.17480336129665375\n",
            "18900 val_loss: 0.3256949782371521, train_loss: 0.17612852156162262\n",
            "18910 val_loss: 0.3213086724281311, train_loss: 0.17457394301891327\n",
            "18920 val_loss: 0.32314813137054443, train_loss: 0.17494530975818634\n",
            "18930 val_loss: 0.3155212104320526, train_loss: 0.17356890439987183\n",
            "18940 val_loss: 0.3271327018737793, train_loss: 0.17454132437705994\n",
            "18950 val_loss: 0.32298389077186584, train_loss: 0.1740112453699112\n",
            "18960 val_loss: 0.31355249881744385, train_loss: 0.17311739921569824\n",
            "18970 val_loss: 0.311453640460968, train_loss: 0.17319342494010925\n",
            "18980 val_loss: 0.31909459829330444, train_loss: 0.17285500466823578\n",
            "18990 val_loss: 0.3172592520713806, train_loss: 0.17213396728038788\n",
            "19000 val_loss: 0.31775963306427, train_loss: 0.17189431190490723\n",
            "19010 val_loss: 0.31709957122802734, train_loss: 0.171423077583313\n",
            "19020 val_loss: 0.31231746077537537, train_loss: 0.1711725890636444\n",
            "19030 val_loss: 0.3094402551651001, train_loss: 0.17113007605075836\n",
            "19040 val_loss: 0.3098055124282837, train_loss: 0.17025971412658691\n",
            "19050 val_loss: 0.3098597824573517, train_loss: 0.169697567820549\n",
            "19060 val_loss: 0.3106295168399811, train_loss: 0.16883137822151184\n",
            "19070 val_loss: 0.3042747974395752, train_loss: 0.16892501711845398\n",
            "19080 val_loss: 0.3092358708381653, train_loss: 0.16711024940013885\n",
            "19090 val_loss: 0.30152666568756104, train_loss: 0.16686534881591797\n",
            "19100 val_loss: 0.30490100383758545, train_loss: 0.16534246504306793\n",
            "19110 val_loss: 0.30065569281578064, train_loss: 0.16425463557243347\n",
            "19120 val_loss: 0.2998979091644287, train_loss: 0.1630616933107376\n",
            "19130 val_loss: 0.2946259081363678, train_loss: 0.16259244084358215\n",
            "19140 val_loss: 0.2943035960197449, train_loss: 0.1617467999458313\n",
            "19150 val_loss: 0.2915080487728119, train_loss: 0.1608358472585678\n",
            "19160 val_loss: 0.2905753552913666, train_loss: 0.1598270833492279\n",
            "19170 val_loss: 0.2901669144630432, train_loss: 0.15910539031028748\n",
            "19180 val_loss: 0.2872423827648163, train_loss: 0.15812984108924866\n",
            "19190 val_loss: 0.28939786553382874, train_loss: 0.15794916450977325\n",
            "19200 val_loss: 0.2867947518825531, train_loss: 0.15714752674102783\n",
            "19210 val_loss: 0.28849586844444275, train_loss: 0.1566597819328308\n",
            "19220 val_loss: 0.2863888442516327, train_loss: 0.15599438548088074\n",
            "19230 val_loss: 0.28513726592063904, train_loss: 0.15553143620491028\n",
            "19240 val_loss: 0.2839718461036682, train_loss: 0.15481096506118774\n",
            "19250 val_loss: 0.28041255474090576, train_loss: 0.1544797122478485\n",
            "19260 val_loss: 0.28412336111068726, train_loss: 0.1541600227355957\n",
            "19270 val_loss: 0.2816780209541321, train_loss: 0.15288332104682922\n",
            "19280 val_loss: 0.2845960259437561, train_loss: 0.15237510204315186\n",
            "19290 val_loss: 0.28170257806777954, train_loss: 0.1518861949443817\n",
            "19300 val_loss: 0.2812277376651764, train_loss: 0.1511663794517517\n",
            "19310 val_loss: 0.279670387506485, train_loss: 0.15063941478729248\n",
            "19320 val_loss: 0.27811798453330994, train_loss: 0.15003091096878052\n",
            "19330 val_loss: 0.2785692512989044, train_loss: 0.14971913397312164\n",
            "19340 val_loss: 0.2803371548652649, train_loss: 0.149533212184906\n",
            "19350 val_loss: 0.27626481652259827, train_loss: 0.1488744020462036\n",
            "19360 val_loss: 0.2765597403049469, train_loss: 0.14826805889606476\n",
            "19370 val_loss: 0.27940434217453003, train_loss: 0.14806994795799255\n",
            "19380 val_loss: 0.28263944387435913, train_loss: 0.1485191285610199\n",
            "19390 val_loss: 0.2791144847869873, train_loss: 0.14629700779914856\n",
            "19400 val_loss: 0.28289249539375305, train_loss: 0.14638076722621918\n",
            "19410 val_loss: 0.28315994143486023, train_loss: 0.1453813761472702\n",
            "19420 val_loss: 0.281343936920166, train_loss: 0.14447124302387238\n",
            "19430 val_loss: 0.27828535437583923, train_loss: 0.14379173517227173\n",
            "19440 val_loss: 0.27969440817832947, train_loss: 0.1435004323720932\n",
            "19450 val_loss: 0.2785339653491974, train_loss: 0.14291241765022278\n",
            "19460 val_loss: 0.27437567710876465, train_loss: 0.14237986505031586\n",
            "19470 val_loss: 0.27277857065200806, train_loss: 0.14239254593849182\n",
            "19480 val_loss: 0.2742113769054413, train_loss: 0.14197444915771484\n",
            "19490 val_loss: 0.2719632685184479, train_loss: 0.1411028653383255\n",
            "19500 val_loss: 0.2731458842754364, train_loss: 0.14053715765476227\n",
            "19510 val_loss: 0.27258121967315674, train_loss: 0.1400553286075592\n",
            "19520 val_loss: 0.28580442070961, train_loss: 0.1435612589120865\n",
            "19530 val_loss: 0.2709081172943115, train_loss: 0.13918636739253998\n",
            "19540 val_loss: 0.2708858251571655, train_loss: 0.13957801461219788\n",
            "19550 val_loss: 0.2736181914806366, train_loss: 0.13971254229545593\n",
            "19560 val_loss: 0.2666029632091522, train_loss: 0.13819725811481476\n",
            "19570 val_loss: 0.27143654227256775, train_loss: 0.1375240683555603\n",
            "19580 val_loss: 0.2693583071231842, train_loss: 0.13689951598644257\n",
            "19590 val_loss: 0.2684547007083893, train_loss: 0.13662323355674744\n",
            "19600 val_loss: 0.2686612606048584, train_loss: 0.13650943338871002\n",
            "19610 val_loss: 0.2744094431400299, train_loss: 0.13630886375904083\n",
            "19620 val_loss: 0.2732055187225342, train_loss: 0.1358804553747177\n",
            "19630 val_loss: 0.271633505821228, train_loss: 0.13582460582256317\n",
            "19640 val_loss: 0.2661483585834503, train_loss: 0.1348065882921219\n",
            "19650 val_loss: 0.26443591713905334, train_loss: 0.13455790281295776\n",
            "19660 val_loss: 0.26545947790145874, train_loss: 0.1356230080127716\n",
            "19670 val_loss: 0.26344427466392517, train_loss: 0.1336347758769989\n",
            "19680 val_loss: 0.26369327306747437, train_loss: 0.13363991677761078\n",
            "19690 val_loss: 0.2632720470428467, train_loss: 0.1333869844675064\n",
            "19700 val_loss: 0.26426512002944946, train_loss: 0.13254371285438538\n",
            "19710 val_loss: 0.26490288972854614, train_loss: 0.13260066509246826\n",
            "19720 val_loss: 0.26591065526008606, train_loss: 0.13206757605075836\n",
            "19730 val_loss: 0.2639565169811249, train_loss: 0.13142362236976624\n",
            "19740 val_loss: 0.2617771327495575, train_loss: 0.1312885731458664\n",
            "19750 val_loss: 0.2696029245853424, train_loss: 0.1319955736398697\n",
            "19760 val_loss: 0.26417985558509827, train_loss: 0.1306149959564209\n",
            "19770 val_loss: 0.2609197199344635, train_loss: 0.13257797062397003\n",
            "19780 val_loss: 0.26180100440979004, train_loss: 0.1297987550497055\n",
            "19790 val_loss: 0.25977209210395813, train_loss: 0.13257934153079987\n",
            "19800 val_loss: 0.26615238189697266, train_loss: 0.12977245450019836\n",
            "19810 val_loss: 0.2583794891834259, train_loss: 0.1292787790298462\n",
            "19820 val_loss: 0.26186403632164, train_loss: 0.12905928492546082\n",
            "19830 val_loss: 0.26158633828163147, train_loss: 0.1284673660993576\n",
            "19840 val_loss: 0.26341140270233154, train_loss: 0.12877768278121948\n",
            "19850 val_loss: 0.2600732445716858, train_loss: 0.12793706357479095\n",
            "19860 val_loss: 0.25805193185806274, train_loss: 0.1273454874753952\n",
            "19870 val_loss: 0.260414719581604, train_loss: 0.12708435952663422\n",
            "19880 val_loss: 0.2651147246360779, train_loss: 0.12698198854923248\n",
            "19890 val_loss: 0.258569598197937, train_loss: 0.1264861524105072\n",
            "19900 val_loss: 0.2615211606025696, train_loss: 0.12633493542671204\n",
            "19910 val_loss: 0.2601798176765442, train_loss: 0.1257755160331726\n",
            "19920 val_loss: 0.26028403639793396, train_loss: 0.12577830255031586\n",
            "19930 val_loss: 0.2600327432155609, train_loss: 0.12491597980260849\n",
            "19940 val_loss: 0.26192641258239746, train_loss: 0.1252811700105667\n",
            "19950 val_loss: 0.2630928158760071, train_loss: 0.12758763134479523\n",
            "19960 val_loss: 0.2536783814430237, train_loss: 0.12476634979248047\n",
            "19970 val_loss: 0.25569674372673035, train_loss: 0.12745067477226257\n",
            "19980 val_loss: 0.26652559638023376, train_loss: 0.12362789362668991\n",
            "19990 val_loss: 0.25821685791015625, train_loss: 0.12325432896614075\n",
            "20000 val_loss: 0.2687126696109772, train_loss: 0.12323673069477081\n",
            "20010 val_loss: 0.2581940293312073, train_loss: 0.12243203073740005\n",
            "20020 val_loss: 0.2537000775337219, train_loss: 0.1243608370423317\n",
            "20030 val_loss: 0.256895512342453, train_loss: 0.12284177541732788\n",
            "20040 val_loss: 0.25735723972320557, train_loss: 0.122027687728405\n",
            "20050 val_loss: 0.26153242588043213, train_loss: 0.12191247195005417\n",
            "20060 val_loss: 0.26706257462501526, train_loss: 0.1225319504737854\n",
            "20070 val_loss: 0.25843483209609985, train_loss: 0.12070826441049576\n",
            "20080 val_loss: 0.2675822675228119, train_loss: 0.12242908775806427\n",
            "20090 val_loss: 0.2563299536705017, train_loss: 0.12101977318525314\n",
            "20100 val_loss: 0.2580515444278717, train_loss: 0.12048148363828659\n",
            "20110 val_loss: 0.26185575127601624, train_loss: 0.12070468813180923\n",
            "20120 val_loss: 0.25619444251060486, train_loss: 0.11978273093700409\n",
            "20130 val_loss: 0.2557329535484314, train_loss: 0.1202789843082428\n",
            "20140 val_loss: 0.2536402642726898, train_loss: 0.11966149508953094\n",
            "20150 val_loss: 0.25452008843421936, train_loss: 0.11924324184656143\n",
            "20160 val_loss: 0.2672933340072632, train_loss: 0.12203064560890198\n",
            "20170 val_loss: 0.2518247961997986, train_loss: 0.12296494841575623\n",
            "20180 val_loss: 0.25403186678886414, train_loss: 0.1188177689909935\n",
            "20190 val_loss: 0.2524462938308716, train_loss: 0.11878033727407455\n",
            "20200 val_loss: 0.26047006249427795, train_loss: 0.11815962195396423\n",
            "20210 val_loss: 0.25810518860816956, train_loss: 0.11753153055906296\n",
            "20220 val_loss: 0.2577368915081024, train_loss: 0.11771669238805771\n",
            "20230 val_loss: 0.2557049095630646, train_loss: 0.11795192956924438\n",
            "20240 val_loss: 0.2550959885120392, train_loss: 0.11733529716730118\n",
            "20250 val_loss: 0.2648356556892395, train_loss: 0.12075668573379517\n",
            "20260 val_loss: 0.25612059235572815, train_loss: 0.1173056811094284\n",
            "20270 val_loss: 0.2495225965976715, train_loss: 0.1171262338757515\n",
            "20280 val_loss: 0.2549766004085541, train_loss: 0.11634620279073715\n",
            "20290 val_loss: 0.2551054358482361, train_loss: 0.11630821973085403\n",
            "20300 val_loss: 0.2638270854949951, train_loss: 0.11874338984489441\n",
            "20310 val_loss: 0.266160249710083, train_loss: 0.11700821667909622\n",
            "20320 val_loss: 0.26079699397087097, train_loss: 0.11618045717477798\n",
            "20330 val_loss: 0.25314903259277344, train_loss: 0.11549285799264908\n",
            "20340 val_loss: 0.25283142924308777, train_loss: 0.11560636758804321\n",
            "20350 val_loss: 0.25858503580093384, train_loss: 0.1160011813044548\n",
            "20360 val_loss: 0.25418028235435486, train_loss: 0.1151777058839798\n",
            "20370 val_loss: 0.2535727024078369, train_loss: 0.11483220010995865\n",
            "20380 val_loss: 0.25247102975845337, train_loss: 0.11491359025239944\n",
            "20390 val_loss: 0.26130884885787964, train_loss: 0.11481232196092606\n",
            "20400 val_loss: 0.25432977080345154, train_loss: 0.11560802906751633\n",
            "20410 val_loss: 0.25457072257995605, train_loss: 0.11409575492143631\n",
            "20420 val_loss: 0.2566964328289032, train_loss: 0.11390362679958344\n",
            "20430 val_loss: 0.2550964951515198, train_loss: 0.11354990303516388\n",
            "20440 val_loss: 0.2527860999107361, train_loss: 0.11325068026781082\n",
            "20450 val_loss: 0.2518429160118103, train_loss: 0.11322277784347534\n",
            "20460 val_loss: 0.2580511271953583, train_loss: 0.11439096927642822\n",
            "20470 val_loss: 0.25253644585609436, train_loss: 0.11261789500713348\n",
            "20480 val_loss: 0.2562006413936615, train_loss: 0.11328300088644028\n",
            "20490 val_loss: 0.2577430009841919, train_loss: 0.11295608431100845\n",
            "20500 val_loss: 0.249266117811203, train_loss: 0.11239521205425262\n",
            "20510 val_loss: 0.253421813249588, train_loss: 0.11193285137414932\n",
            "20520 val_loss: 0.25231924653053284, train_loss: 0.11163884401321411\n",
            "20530 val_loss: 0.25350624322891235, train_loss: 0.11158855259418488\n",
            "20540 val_loss: 0.250884473323822, train_loss: 0.11272300034761429\n",
            "20550 val_loss: 0.25304439663887024, train_loss: 0.11220694333314896\n",
            "20560 val_loss: 0.2524189054965973, train_loss: 0.11110030114650726\n",
            "20570 val_loss: 0.25169044733047485, train_loss: 0.11096643656492233\n",
            "20580 val_loss: 0.25211942195892334, train_loss: 0.11170367896556854\n",
            "20590 val_loss: 0.2481721043586731, train_loss: 0.11086496710777283\n",
            "20600 val_loss: 0.25519058108329773, train_loss: 0.11091429740190506\n",
            "20610 val_loss: 0.25495827198028564, train_loss: 0.11101797968149185\n",
            "20620 val_loss: 0.24923484027385712, train_loss: 0.11047109961509705\n",
            "20630 val_loss: 0.2621355354785919, train_loss: 0.11192475259304047\n",
            "20640 val_loss: 0.24491842091083527, train_loss: 0.1144162267446518\n",
            "20650 val_loss: 0.25622615218162537, train_loss: 0.1120179072022438\n",
            "20660 val_loss: 0.2566496729850769, train_loss: 0.10927597433328629\n",
            "20670 val_loss: 0.25084400177001953, train_loss: 0.10939006507396698\n",
            "20680 val_loss: 0.2551877498626709, train_loss: 0.108615942299366\n",
            "20690 val_loss: 0.2765122652053833, train_loss: 0.11621420085430145\n",
            "20700 val_loss: 0.25135916471481323, train_loss: 0.10813245177268982\n",
            "20710 val_loss: 0.2552727162837982, train_loss: 0.10852763056755066\n",
            "20720 val_loss: 0.24819642305374146, train_loss: 0.10746704787015915\n",
            "20730 val_loss: 0.24593092501163483, train_loss: 0.10820939391851425\n",
            "20740 val_loss: 0.2501789629459381, train_loss: 0.10742119699716568\n",
            "20750 val_loss: 0.2529671788215637, train_loss: 0.1079907938838005\n",
            "20760 val_loss: 0.25504592061042786, train_loss: 0.10775914788246155\n",
            "20770 val_loss: 0.2472396194934845, train_loss: 0.107907734811306\n",
            "20780 val_loss: 0.2567910850048065, train_loss: 0.10831965506076813\n",
            "20790 val_loss: 0.24519841372966766, train_loss: 0.10712867230176926\n",
            "20800 val_loss: 0.2482149451971054, train_loss: 0.10606320947408676\n",
            "20810 val_loss: 0.25219446420669556, train_loss: 0.10624867677688599\n",
            "20820 val_loss: 0.24509379267692566, train_loss: 0.1054474264383316\n",
            "20830 val_loss: 0.2502402365207672, train_loss: 0.10522521287202835\n",
            "20840 val_loss: 0.24608416855335236, train_loss: 0.10496772825717926\n",
            "20850 val_loss: 0.2486470490694046, train_loss: 0.10469336062669754\n",
            "20860 val_loss: 0.25294268131256104, train_loss: 0.10453158617019653\n",
            "20870 val_loss: 0.25342702865600586, train_loss: 0.1045728251338005\n",
            "20880 val_loss: 0.2524254024028778, train_loss: 0.10531282424926758\n",
            "20890 val_loss: 0.2472073882818222, train_loss: 0.1038864254951477\n",
            "20900 val_loss: 0.2481435388326645, train_loss: 0.10361190885305405\n",
            "20910 val_loss: 0.2473401129245758, train_loss: 0.10343403369188309\n",
            "20920 val_loss: 0.25205495953559875, train_loss: 0.10301557928323746\n",
            "20930 val_loss: 0.24568183720111847, train_loss: 0.10311712324619293\n",
            "20940 val_loss: 0.2546543478965759, train_loss: 0.10392074286937714\n",
            "20950 val_loss: 0.26392242312431335, train_loss: 0.10783836990594864\n",
            "20960 val_loss: 0.24436278641223907, train_loss: 0.10308563709259033\n",
            "20970 val_loss: 0.25451111793518066, train_loss: 0.10300804674625397\n",
            "20980 val_loss: 0.2473095953464508, train_loss: 0.10202518850564957\n",
            "20990 val_loss: 0.25461727380752563, train_loss: 0.10102175921201706\n",
            "21000 val_loss: 0.2455846220254898, train_loss: 0.10135161131620407\n",
            "21010 val_loss: 0.24760696291923523, train_loss: 0.10112088173627853\n",
            "21020 val_loss: 0.24636727571487427, train_loss: 0.10058193653821945\n",
            "21030 val_loss: 0.2425880879163742, train_loss: 0.1017356663942337\n",
            "21040 val_loss: 0.2541271448135376, train_loss: 0.10172133892774582\n",
            "21050 val_loss: 0.24871514737606049, train_loss: 0.09954841434955597\n",
            "21060 val_loss: 0.24794837832450867, train_loss: 0.099461130797863\n",
            "21070 val_loss: 0.2501269578933716, train_loss: 0.09892778843641281\n",
            "21080 val_loss: 0.25614434480667114, train_loss: 0.09961706399917603\n",
            "21090 val_loss: 0.2543133795261383, train_loss: 0.09935315698385239\n",
            "21100 val_loss: 0.2513585090637207, train_loss: 0.09682665765285492\n",
            "21110 val_loss: 0.26026126742362976, train_loss: 0.09775254875421524\n",
            "21120 val_loss: 0.24994032084941864, train_loss: 0.09757605195045471\n",
            "21130 val_loss: 0.24783647060394287, train_loss: 0.09611450880765915\n",
            "21140 val_loss: 0.2440120428800583, train_loss: 0.09675343334674835\n",
            "21150 val_loss: 0.2530902326107025, train_loss: 0.09650483727455139\n",
            "21160 val_loss: 0.24756202101707458, train_loss: 0.09574231505393982\n",
            "21170 val_loss: 0.24933727085590363, train_loss: 0.09504178166389465\n",
            "21180 val_loss: 0.24116690456867218, train_loss: 0.09527282416820526\n",
            "21190 val_loss: 0.24857079982757568, train_loss: 0.09455662220716476\n",
            "21200 val_loss: 0.2526242136955261, train_loss: 0.09681578725576401\n",
            "21210 val_loss: 0.24758538603782654, train_loss: 0.09476716071367264\n",
            "21220 val_loss: 0.24762217700481415, train_loss: 0.09350262582302094\n",
            "21230 val_loss: 0.24629801511764526, train_loss: 0.09357987344264984\n",
            "21240 val_loss: 0.24660982191562653, train_loss: 0.09266475588083267\n",
            "21250 val_loss: 0.2524290978908539, train_loss: 0.09510785341262817\n",
            "21260 val_loss: 0.2375887930393219, train_loss: 0.09483925253152847\n",
            "21270 val_loss: 0.2469235211610794, train_loss: 0.09278688579797745\n",
            "21280 val_loss: 0.24518030881881714, train_loss: 0.09235656261444092\n",
            "21290 val_loss: 0.25256648659706116, train_loss: 0.0928402692079544\n",
            "21300 val_loss: 0.24410438537597656, train_loss: 0.09210414439439774\n",
            "21310 val_loss: 0.24568778276443481, train_loss: 0.09286469966173172\n",
            "21320 val_loss: 0.24309898912906647, train_loss: 0.09163528680801392\n",
            "21330 val_loss: 0.2443949431180954, train_loss: 0.09199848026037216\n",
            "21340 val_loss: 0.2476753443479538, train_loss: 0.09046509861946106\n",
            "21350 val_loss: 0.24764660000801086, train_loss: 0.09313446283340454\n",
            "21360 val_loss: 0.24653160572052002, train_loss: 0.09042438864707947\n",
            "21370 val_loss: 0.24838431179523468, train_loss: 0.091323621571064\n",
            "21380 val_loss: 0.2628435790538788, train_loss: 0.0979212075471878\n",
            "21390 val_loss: 0.24371983110904694, train_loss: 0.09099089354276657\n",
            "21400 val_loss: 0.2401794195175171, train_loss: 0.0916169136762619\n",
            "21410 val_loss: 0.24829280376434326, train_loss: 0.09033368527889252\n",
            "21420 val_loss: 0.24132229387760162, train_loss: 0.08979250490665436\n",
            "21430 val_loss: 0.24432511627674103, train_loss: 0.08957678079605103\n",
            "21440 val_loss: 0.24313519895076752, train_loss: 0.08921229839324951\n",
            "21450 val_loss: 0.2501351237297058, train_loss: 0.08950882405042648\n",
            "21460 val_loss: 0.2478678822517395, train_loss: 0.08870767056941986\n",
            "21470 val_loss: 0.2405710220336914, train_loss: 0.08855907618999481\n",
            "21480 val_loss: 0.2501619756221771, train_loss: 0.08822096884250641\n",
            "21490 val_loss: 0.24132563173770905, train_loss: 0.08823073655366898\n",
            "21500 val_loss: 0.24387450516223907, train_loss: 0.08803477883338928\n",
            "21510 val_loss: 0.24168309569358826, train_loss: 0.08891691267490387\n",
            "21520 val_loss: 0.245815709233284, train_loss: 0.08810432255268097\n",
            "21530 val_loss: 0.24489034712314606, train_loss: 0.08730901777744293\n",
            "21540 val_loss: 0.25016531348228455, train_loss: 0.09102204442024231\n",
            "21550 val_loss: 0.24836304783821106, train_loss: 0.08744782954454422\n",
            "21560 val_loss: 0.24417546391487122, train_loss: 0.086869977414608\n",
            "21570 val_loss: 0.25104281306266785, train_loss: 0.08778923749923706\n",
            "21580 val_loss: 0.23751288652420044, train_loss: 0.08769328892230988\n",
            "21590 val_loss: 0.24244582653045654, train_loss: 0.08628155291080475\n",
            "21600 val_loss: 0.24196961522102356, train_loss: 0.08629313111305237\n",
            "21610 val_loss: 0.2485911101102829, train_loss: 0.08692631125450134\n",
            "21620 val_loss: 0.24072547256946564, train_loss: 0.08636121451854706\n",
            "21630 val_loss: 0.24602726101875305, train_loss: 0.08564597368240356\n",
            "21640 val_loss: 0.24910952150821686, train_loss: 0.08583191782236099\n",
            "21650 val_loss: 0.2450653463602066, train_loss: 0.08517628908157349\n",
            "21660 val_loss: 0.25056028366088867, train_loss: 0.08653360605239868\n",
            "21670 val_loss: 0.23478169739246368, train_loss: 0.08683408051729202\n",
            "21680 val_loss: 0.27160704135894775, train_loss: 0.0961194559931755\n",
            "21690 val_loss: 0.2412382811307907, train_loss: 0.08507168292999268\n",
            "21700 val_loss: 0.2431114763021469, train_loss: 0.08552642911672592\n",
            "21710 val_loss: 0.2436138540506363, train_loss: 0.0847049281001091\n",
            "21720 val_loss: 0.2407994270324707, train_loss: 0.08460407704114914\n",
            "21730 val_loss: 0.24090854823589325, train_loss: 0.08456578850746155\n",
            "21740 val_loss: 0.2579204738140106, train_loss: 0.09012486785650253\n",
            "21750 val_loss: 0.24361814558506012, train_loss: 0.08343840390443802\n",
            "21760 val_loss: 0.23450899124145508, train_loss: 0.08567947149276733\n",
            "21770 val_loss: 0.23692485690116882, train_loss: 0.08729197084903717\n",
            "21780 val_loss: 0.24007923901081085, train_loss: 0.08382468670606613\n",
            "21790 val_loss: 0.24345672130584717, train_loss: 0.08385667949914932\n",
            "21800 val_loss: 0.24680030345916748, train_loss: 0.08380094915628433\n",
            "21810 val_loss: 0.23876473307609558, train_loss: 0.082654669880867\n",
            "21820 val_loss: 0.244697704911232, train_loss: 0.08225182443857193\n",
            "21830 val_loss: 0.23710834980010986, train_loss: 0.084614098072052\n",
            "21840 val_loss: 0.24542853236198425, train_loss: 0.08355595916509628\n",
            "21850 val_loss: 0.24005192518234253, train_loss: 0.08304911106824875\n",
            "21860 val_loss: 0.24234454333782196, train_loss: 0.08213894814252853\n",
            "21870 val_loss: 0.24732771515846252, train_loss: 0.08317438513040543\n",
            "21880 val_loss: 0.2385796308517456, train_loss: 0.08182837069034576\n",
            "21890 val_loss: 0.24614779651165009, train_loss: 0.08316285163164139\n",
            "21900 val_loss: 0.23883724212646484, train_loss: 0.08145792782306671\n",
            "21910 val_loss: 0.24101608991622925, train_loss: 0.0815945565700531\n",
            "21920 val_loss: 0.23715946078300476, train_loss: 0.08173512667417526\n",
            "21930 val_loss: 0.23934181034564972, train_loss: 0.08230654150247574\n",
            "21940 val_loss: 0.23821350932121277, train_loss: 0.08149035274982452\n",
            "21950 val_loss: 0.2471158355474472, train_loss: 0.08242524415254593\n",
            "21960 val_loss: 0.24665185809135437, train_loss: 0.08075620979070663\n",
            "21970 val_loss: 0.24088945984840393, train_loss: 0.08063698559999466\n",
            "21980 val_loss: 0.23773440718650818, train_loss: 0.08056677877902985\n",
            "21990 val_loss: 0.23735536634922028, train_loss: 0.08066295832395554\n",
            "22000 val_loss: 0.2411661148071289, train_loss: 0.07982407510280609\n",
            "22010 val_loss: 0.23916283249855042, train_loss: 0.08009881526231766\n",
            "22020 val_loss: 0.2407260239124298, train_loss: 0.07998140156269073\n",
            "22030 val_loss: 0.24737244844436646, train_loss: 0.08131171017885208\n",
            "22040 val_loss: 0.24535390734672546, train_loss: 0.08117279410362244\n",
            "22050 val_loss: 0.24215289950370789, train_loss: 0.07900701463222504\n",
            "22060 val_loss: 0.2521042823791504, train_loss: 0.08065825700759888\n",
            "22070 val_loss: 0.24242350459098816, train_loss: 0.07917968928813934\n",
            "22080 val_loss: 0.24628981947898865, train_loss: 0.07935500890016556\n",
            "22090 val_loss: 0.24201470613479614, train_loss: 0.07899629324674606\n",
            "22100 val_loss: 0.24623489379882812, train_loss: 0.0786905512213707\n",
            "22110 val_loss: 0.2501143515110016, train_loss: 0.07870811969041824\n",
            "22120 val_loss: 0.24484674632549286, train_loss: 0.07826629281044006\n",
            "22130 val_loss: 0.23971231281757355, train_loss: 0.07845532149076462\n",
            "22140 val_loss: 0.2617117762565613, train_loss: 0.0822114571928978\n",
            "22150 val_loss: 0.24730293452739716, train_loss: 0.07848741859197617\n",
            "22160 val_loss: 0.24710237979888916, train_loss: 0.07804416120052338\n",
            "22170 val_loss: 0.23914983868598938, train_loss: 0.07795627415180206\n",
            "22180 val_loss: 0.23884253203868866, train_loss: 0.07775741815567017\n",
            "22190 val_loss: 0.2369173765182495, train_loss: 0.07876867055892944\n",
            "22200 val_loss: 0.2431175410747528, train_loss: 0.07752154767513275\n",
            "22210 val_loss: 0.2398279309272766, train_loss: 0.07773225009441376\n",
            "22220 val_loss: 0.24965150654315948, train_loss: 0.0782540887594223\n",
            "22230 val_loss: 0.24640382826328278, train_loss: 0.07786273211240768\n",
            "22240 val_loss: 0.23986414074897766, train_loss: 0.07750964909791946\n",
            "22250 val_loss: 0.23757536709308624, train_loss: 0.0791783556342125\n",
            "22260 val_loss: 0.24388955533504486, train_loss: 0.07741916179656982\n",
            "22270 val_loss: 0.2463892102241516, train_loss: 0.07652140408754349\n",
            "22280 val_loss: 0.24866530299186707, train_loss: 0.0782768577337265\n",
            "22290 val_loss: 0.2513667643070221, train_loss: 0.07833108305931091\n",
            "22300 val_loss: 0.2461593747138977, train_loss: 0.07658883184194565\n",
            "22310 val_loss: 0.24044860899448395, train_loss: 0.07739558815956116\n",
            "22320 val_loss: 0.23541468381881714, train_loss: 0.07661299407482147\n",
            "22330 val_loss: 0.24634890258312225, train_loss: 0.07567325234413147\n",
            "22340 val_loss: 0.24003350734710693, train_loss: 0.07668235152959824\n",
            "22350 val_loss: 0.24525898694992065, train_loss: 0.07725891470909119\n",
            "22360 val_loss: 0.2338077276945114, train_loss: 0.07614267617464066\n",
            "22370 val_loss: 0.24077506363391876, train_loss: 0.07574988156557083\n",
            "22380 val_loss: 0.24994780123233795, train_loss: 0.07687937468290329\n",
            "22390 val_loss: 0.24382922053337097, train_loss: 0.07648533582687378\n",
            "22400 val_loss: 0.23796336352825165, train_loss: 0.07608311623334885\n",
            "22410 val_loss: 0.23564842343330383, train_loss: 0.07643783837556839\n",
            "22420 val_loss: 0.2448352873325348, train_loss: 0.07626619189977646\n",
            "22430 val_loss: 0.24609874188899994, train_loss: 0.07927269488573074\n",
            "22440 val_loss: 0.2337782233953476, train_loss: 0.08002634346485138\n",
            "22450 val_loss: 0.24211502075195312, train_loss: 0.0760650560259819\n",
            "22460 val_loss: 0.2403184324502945, train_loss: 0.07461757212877274\n",
            "22470 val_loss: 0.23748251795768738, train_loss: 0.07530075311660767\n",
            "22480 val_loss: 0.25604766607284546, train_loss: 0.0768161192536354\n",
            "22490 val_loss: 0.23948800563812256, train_loss: 0.07447664439678192\n",
            "22500 val_loss: 0.2402527630329132, train_loss: 0.07512237876653671\n",
            "22510 val_loss: 0.23429332673549652, train_loss: 0.0746462419629097\n",
            "22520 val_loss: 0.23926839232444763, train_loss: 0.07544200122356415\n",
            "22530 val_loss: 0.2575155198574066, train_loss: 0.07718019187450409\n",
            "22540 val_loss: 0.23829664289951324, train_loss: 0.07420163601636887\n",
            "22550 val_loss: 0.23671582341194153, train_loss: 0.07477479428052902\n",
            "22560 val_loss: 0.24172265827655792, train_loss: 0.07428083568811417\n",
            "22570 val_loss: 0.24638807773590088, train_loss: 0.07371879369020462\n",
            "22580 val_loss: 0.24264231324195862, train_loss: 0.07440271973609924\n",
            "22590 val_loss: 0.2345755398273468, train_loss: 0.07386673986911774\n",
            "22600 val_loss: 0.23791758716106415, train_loss: 0.07362477481365204\n",
            "22610 val_loss: 0.24979393184185028, train_loss: 0.07487491518259048\n",
            "22620 val_loss: 0.2534392178058624, train_loss: 0.07832677662372589\n",
            "22630 val_loss: 0.23886874318122864, train_loss: 0.07502812147140503\n",
            "22640 val_loss: 0.2412402480840683, train_loss: 0.07403325289487839\n",
            "22650 val_loss: 0.24337442219257355, train_loss: 0.07448442280292511\n",
            "22660 val_loss: 0.25423792004585266, train_loss: 0.0756835862994194\n",
            "22670 val_loss: 0.2509269118309021, train_loss: 0.07517766952514648\n",
            "22680 val_loss: 0.23967769742012024, train_loss: 0.07348015159368515\n",
            "22690 val_loss: 0.2342497557401657, train_loss: 0.07380697876214981\n",
            "22700 val_loss: 0.2477182149887085, train_loss: 0.0735895112156868\n",
            "22710 val_loss: 0.23522844910621643, train_loss: 0.07363160699605942\n",
            "22720 val_loss: 0.2382059097290039, train_loss: 0.07358197122812271\n",
            "22730 val_loss: 0.2339838594198227, train_loss: 0.07261054962873459\n",
            "22740 val_loss: 0.25001558661460876, train_loss: 0.07483178377151489\n",
            "22750 val_loss: 0.23238813877105713, train_loss: 0.07305870205163956\n",
            "22760 val_loss: 0.23177137970924377, train_loss: 0.0723043829202652\n",
            "22770 val_loss: 0.24809108674526215, train_loss: 0.0745093822479248\n",
            "22780 val_loss: 0.23409803211688995, train_loss: 0.07286768406629562\n",
            "22790 val_loss: 0.2317483276128769, train_loss: 0.07280188053846359\n",
            "22800 val_loss: 0.24277496337890625, train_loss: 0.07420430332422256\n",
            "22810 val_loss: 0.24502651393413544, train_loss: 0.07237683236598969\n",
            "22820 val_loss: 0.2387637197971344, train_loss: 0.07203013449907303\n",
            "22830 val_loss: 0.23745107650756836, train_loss: 0.07173897325992584\n",
            "22840 val_loss: 0.2374102771282196, train_loss: 0.07191120088100433\n",
            "22850 val_loss: 0.23955830931663513, train_loss: 0.0723922848701477\n",
            "22860 val_loss: 0.24016736447811127, train_loss: 0.0717909038066864\n",
            "22870 val_loss: 0.2326529175043106, train_loss: 0.0722675696015358\n",
            "22880 val_loss: 0.2340656965970993, train_loss: 0.07216239720582962\n",
            "22890 val_loss: 0.2361537665128708, train_loss: 0.07184360176324844\n",
            "22900 val_loss: 0.23996007442474365, train_loss: 0.07172421365976334\n",
            "22910 val_loss: 0.23664960265159607, train_loss: 0.07123205810785294\n",
            "22920 val_loss: 0.2354288101196289, train_loss: 0.0712001696228981\n",
            "22930 val_loss: 0.2408735454082489, train_loss: 0.07168697565793991\n",
            "22940 val_loss: 0.24730341136455536, train_loss: 0.07216925173997879\n",
            "22950 val_loss: 0.23700793087482452, train_loss: 0.0710688978433609\n",
            "22960 val_loss: 0.23865440487861633, train_loss: 0.07095833122730255\n",
            "22970 val_loss: 0.23093344271183014, train_loss: 0.07237216085195541\n",
            "22980 val_loss: 0.23283179104328156, train_loss: 0.07127723097801208\n",
            "22990 val_loss: 0.23144733905792236, train_loss: 0.07071070373058319\n",
            "23000 val_loss: 0.231889545917511, train_loss: 0.07085120677947998\n",
            "23010 val_loss: 0.23911240696907043, train_loss: 0.07161509990692139\n",
            "23020 val_loss: 0.243451327085495, train_loss: 0.07069364935159683\n",
            "23030 val_loss: 0.23533006012439728, train_loss: 0.07046488672494888\n",
            "23040 val_loss: 0.23900815844535828, train_loss: 0.07056114822626114\n",
            "23050 val_loss: 0.2396533340215683, train_loss: 0.07149739563465118\n",
            "23060 val_loss: 0.23323523998260498, train_loss: 0.07024174928665161\n",
            "23070 val_loss: 0.23284681141376495, train_loss: 0.07039394974708557\n",
            "23080 val_loss: 0.24069026112556458, train_loss: 0.0705762505531311\n",
            "23090 val_loss: 0.23725482821464539, train_loss: 0.07077577710151672\n",
            "23100 val_loss: 0.24616046249866486, train_loss: 0.07039782404899597\n",
            "23110 val_loss: 0.23471809923648834, train_loss: 0.0701524168252945\n",
            "23120 val_loss: 0.22825472056865692, train_loss: 0.07028300315141678\n",
            "23130 val_loss: 0.23531198501586914, train_loss: 0.06955038756132126\n",
            "23140 val_loss: 0.23410698771476746, train_loss: 0.06957021355628967\n",
            "23150 val_loss: 0.23568132519721985, train_loss: 0.06911927461624146\n",
            "23160 val_loss: 0.23644889891147614, train_loss: 0.06888247281312943\n",
            "23170 val_loss: 0.2333870381116867, train_loss: 0.06961993873119354\n",
            "23180 val_loss: 0.24696826934814453, train_loss: 0.06939950585365295\n",
            "23190 val_loss: 0.2326488196849823, train_loss: 0.06933130323886871\n",
            "23200 val_loss: 0.22907784581184387, train_loss: 0.06953895092010498\n",
            "23210 val_loss: 0.2354469746351242, train_loss: 0.06901709735393524\n",
            "23220 val_loss: 0.22361469268798828, train_loss: 0.06975860893726349\n",
            "23230 val_loss: 0.23147380352020264, train_loss: 0.06859774142503738\n",
            "23240 val_loss: 0.23466847836971283, train_loss: 0.06924410909414291\n",
            "23250 val_loss: 0.22674855589866638, train_loss: 0.0681542158126831\n",
            "23260 val_loss: 0.23888596892356873, train_loss: 0.06820516288280487\n",
            "23270 val_loss: 0.23595015704631805, train_loss: 0.0691935122013092\n",
            "23280 val_loss: 0.24730075895786285, train_loss: 0.06977919489145279\n",
            "23290 val_loss: 0.2266831398010254, train_loss: 0.06775286793708801\n",
            "23300 val_loss: 0.2484007626771927, train_loss: 0.0702633336186409\n",
            "23310 val_loss: 0.2327812761068344, train_loss: 0.06838372349739075\n",
            "23320 val_loss: 0.22882218658924103, train_loss: 0.06862383335828781\n",
            "23330 val_loss: 0.23605333268642426, train_loss: 0.06741702556610107\n",
            "23340 val_loss: 0.2358045130968094, train_loss: 0.06786622107028961\n",
            "23350 val_loss: 0.23470976948738098, train_loss: 0.06772282719612122\n",
            "23360 val_loss: 0.2281375378370285, train_loss: 0.07431492209434509\n",
            "23370 val_loss: 0.22630445659160614, train_loss: 0.06835709512233734\n",
            "23380 val_loss: 0.23440617322921753, train_loss: 0.06715048849582672\n",
            "23390 val_loss: 0.2299949824810028, train_loss: 0.06743007898330688\n",
            "23400 val_loss: 0.22967325150966644, train_loss: 0.06773609668016434\n",
            "23410 val_loss: 0.23451045155525208, train_loss: 0.0676405057311058\n",
            "23420 val_loss: 0.23195800185203552, train_loss: 0.06765226274728775\n",
            "23430 val_loss: 0.22774410247802734, train_loss: 0.06800346821546555\n",
            "23440 val_loss: 0.23687414824962616, train_loss: 0.06747497618198395\n",
            "23450 val_loss: 0.23449788987636566, train_loss: 0.06761293858289719\n",
            "23460 val_loss: 0.23080503940582275, train_loss: 0.06728605180978775\n",
            "23470 val_loss: 0.23012486100196838, train_loss: 0.06723328679800034\n",
            "23480 val_loss: 0.2376866638660431, train_loss: 0.06727149337530136\n",
            "23490 val_loss: 0.23776310682296753, train_loss: 0.06663065403699875\n",
            "23500 val_loss: 0.25761064887046814, train_loss: 0.06867320090532303\n",
            "23510 val_loss: 0.25104957818984985, train_loss: 0.0692339539527893\n",
            "23520 val_loss: 0.23421630263328552, train_loss: 0.06657079607248306\n",
            "23530 val_loss: 0.23662497103214264, train_loss: 0.0662030279636383\n",
            "23540 val_loss: 0.23209813237190247, train_loss: 0.06829804927110672\n",
            "23550 val_loss: 0.23109790682792664, train_loss: 0.06681352853775024\n",
            "23560 val_loss: 0.23184964060783386, train_loss: 0.06713010370731354\n",
            "23570 val_loss: 0.23353521525859833, train_loss: 0.06619668751955032\n",
            "23580 val_loss: 0.2740586996078491, train_loss: 0.07324083149433136\n",
            "23590 val_loss: 0.22669236361980438, train_loss: 0.06616031378507614\n",
            "23600 val_loss: 0.22529059648513794, train_loss: 0.06686287373304367\n",
            "23610 val_loss: 0.22841708362102509, train_loss: 0.06652561575174332\n",
            "23620 val_loss: 0.23272046446800232, train_loss: 0.06601638346910477\n",
            "23630 val_loss: 0.2365494668483734, train_loss: 0.06555527448654175\n",
            "23640 val_loss: 0.2295180708169937, train_loss: 0.06517345458269119\n",
            "23650 val_loss: 0.22868601977825165, train_loss: 0.06509120762348175\n",
            "23660 val_loss: 0.23384223878383636, train_loss: 0.06528881937265396\n",
            "23670 val_loss: 0.22458752989768982, train_loss: 0.06479940563440323\n",
            "23680 val_loss: 0.22877810895442963, train_loss: 0.06470412760972977\n",
            "23690 val_loss: 0.22495707869529724, train_loss: 0.06572441011667252\n",
            "23700 val_loss: 0.23194517195224762, train_loss: 0.06493690609931946\n",
            "23710 val_loss: 0.22755305469036102, train_loss: 0.06500500440597534\n",
            "23720 val_loss: 0.23211443424224854, train_loss: 0.06445752084255219\n",
            "23730 val_loss: 0.24139273166656494, train_loss: 0.06572513282299042\n",
            "23740 val_loss: 0.23095551133155823, train_loss: 0.06396643817424774\n",
            "23750 val_loss: 0.23826459050178528, train_loss: 0.06390724331140518\n",
            "23760 val_loss: 0.22356489300727844, train_loss: 0.06289952993392944\n",
            "23770 val_loss: 0.22338758409023285, train_loss: 0.062025558203458786\n",
            "23780 val_loss: 0.23158186674118042, train_loss: 0.06049991026520729\n",
            "23790 val_loss: 0.22537028789520264, train_loss: 0.06099919229745865\n",
            "23800 val_loss: 0.23247818648815155, train_loss: 0.06089995801448822\n",
            "23810 val_loss: 0.22427436709403992, train_loss: 0.05946706235408783\n",
            "23820 val_loss: 0.23372268676757812, train_loss: 0.059343159198760986\n",
            "23830 val_loss: 0.24205997586250305, train_loss: 0.05905931442975998\n",
            "23840 val_loss: 0.23347647488117218, train_loss: 0.05539019778370857\n",
            "23850 val_loss: 0.233910471200943, train_loss: 0.05665658041834831\n",
            "23860 val_loss: 0.22550423443317413, train_loss: 0.05534825101494789\n",
            "23870 val_loss: 0.22613070905208588, train_loss: 0.054289910942316055\n",
            "23880 val_loss: 0.22703173756599426, train_loss: 0.0536825992166996\n",
            "23890 val_loss: 0.22126440703868866, train_loss: 0.05377280339598656\n",
            "23900 val_loss: 0.222754567861557, train_loss: 0.058530695736408234\n",
            "23910 val_loss: 0.21443410217761993, train_loss: 0.05350061506032944\n",
            "23920 val_loss: 0.23037633299827576, train_loss: 0.05418132245540619\n",
            "23930 val_loss: 0.2317546159029007, train_loss: 0.054237980395555496\n",
            "23940 val_loss: 0.23060278594493866, train_loss: 0.05232395976781845\n",
            "23950 val_loss: 0.2167506217956543, train_loss: 0.05053851380944252\n",
            "23960 val_loss: 0.22667309641838074, train_loss: 0.05230070650577545\n",
            "23970 val_loss: 0.2213503122329712, train_loss: 0.051742177456617355\n",
            "23980 val_loss: 0.21502597630023956, train_loss: 0.04970923811197281\n",
            "23990 val_loss: 0.22506679594516754, train_loss: 0.05076490715146065\n",
            "24000 val_loss: 0.21919254958629608, train_loss: 0.04904568940401077\n",
            "24010 val_loss: 0.2167387753725052, train_loss: 0.05007535219192505\n",
            "24020 val_loss: 0.21006645262241364, train_loss: 0.05006493255496025\n",
            "24030 val_loss: 0.21989606320858002, train_loss: 0.04865158349275589\n",
            "24040 val_loss: 0.22741945087909698, train_loss: 0.050501104444265366\n",
            "24050 val_loss: 0.21505892276763916, train_loss: 0.048346951603889465\n",
            "24060 val_loss: 0.22664156556129456, train_loss: 0.048080313950777054\n",
            "24070 val_loss: 0.21852219104766846, train_loss: 0.04932086914777756\n",
            "24080 val_loss: 0.22208066284656525, train_loss: 0.04830895736813545\n",
            "24090 val_loss: 0.21168752014636993, train_loss: 0.04789404943585396\n",
            "24100 val_loss: 0.21838419139385223, train_loss: 0.0500699058175087\n",
            "24110 val_loss: 0.22558607161045074, train_loss: 0.04877061769366264\n",
            "24120 val_loss: 0.21235136687755585, train_loss: 0.04715617001056671\n",
            "24130 val_loss: 0.21275624632835388, train_loss: 0.0481320358812809\n",
            "24140 val_loss: 0.21922162175178528, train_loss: 0.04657263681292534\n",
            "24150 val_loss: 0.20910583436489105, train_loss: 0.04587087407708168\n",
            "24160 val_loss: 0.2220257967710495, train_loss: 0.04708217456936836\n",
            "24170 val_loss: 0.21950115263462067, train_loss: 0.0450768917798996\n",
            "24180 val_loss: 0.21393337845802307, train_loss: 0.04434781149029732\n",
            "24190 val_loss: 0.20618145167827606, train_loss: 0.045121967792510986\n",
            "24200 val_loss: 0.2156570702791214, train_loss: 0.04428189992904663\n",
            "24210 val_loss: 0.21885576844215393, train_loss: 0.046002715826034546\n",
            "24220 val_loss: 0.21376001834869385, train_loss: 0.04563559219241142\n",
            "24230 val_loss: 0.21510019898414612, train_loss: 0.04686405882239342\n",
            "24240 val_loss: 0.20718005299568176, train_loss: 0.04346391558647156\n",
            "24250 val_loss: 0.20301453769207, train_loss: 0.04460713639855385\n",
            "24260 val_loss: 0.2221817523241043, train_loss: 0.04568726569414139\n",
            "24270 val_loss: 0.21474075317382812, train_loss: 0.04401445388793945\n",
            "24280 val_loss: 0.2116599977016449, train_loss: 0.04352003335952759\n",
            "24290 val_loss: 0.21364541351795197, train_loss: 0.04261508584022522\n",
            "24300 val_loss: 0.19393478333950043, train_loss: 0.04406066983938217\n",
            "24310 val_loss: 0.21213072538375854, train_loss: 0.04367464408278465\n",
            "24320 val_loss: 0.21345146000385284, train_loss: 0.0440545454621315\n",
            "24330 val_loss: 0.20735767483711243, train_loss: 0.04500846564769745\n",
            "24340 val_loss: 0.21062807738780975, train_loss: 0.042447879910469055\n",
            "24350 val_loss: 0.21306069195270538, train_loss: 0.042621172964572906\n",
            "24360 val_loss: 0.22540156543254852, train_loss: 0.04329061880707741\n",
            "24370 val_loss: 0.20033177733421326, train_loss: 0.04351088032126427\n",
            "24380 val_loss: 0.20568729937076569, train_loss: 0.042450711131095886\n",
            "24390 val_loss: 0.20765525102615356, train_loss: 0.04281356558203697\n",
            "24400 val_loss: 0.20492853224277496, train_loss: 0.041522104293107986\n",
            "24410 val_loss: 0.21576769649982452, train_loss: 0.04270293191075325\n",
            "24420 val_loss: 0.2075597494840622, train_loss: 0.04323594272136688\n",
            "24430 val_loss: 0.2164187878370285, train_loss: 0.04048576578497887\n",
            "24440 val_loss: 0.19909007847309113, train_loss: 0.041272301226854324\n",
            "24450 val_loss: 0.20725293457508087, train_loss: 0.04074561968445778\n",
            "24460 val_loss: 0.20416978001594543, train_loss: 0.04096946120262146\n",
            "24470 val_loss: 0.21315093338489532, train_loss: 0.04086139798164368\n",
            "24480 val_loss: 0.2135886698961258, train_loss: 0.04029100760817528\n",
            "24490 val_loss: 0.21255914866924286, train_loss: 0.04138091579079628\n",
            "24500 val_loss: 0.2183622568845749, train_loss: 0.04213062673807144\n",
            "24510 val_loss: 0.2062690556049347, train_loss: 0.04007204249501228\n",
            "24520 val_loss: 0.21300390362739563, train_loss: 0.04318966343998909\n",
            "24530 val_loss: 0.2131815254688263, train_loss: 0.04014746844768524\n",
            "24540 val_loss: 0.19559629261493683, train_loss: 0.0404386967420578\n",
            "24550 val_loss: 0.21412809193134308, train_loss: 0.03887886181473732\n",
            "24560 val_loss: 0.20538781583309174, train_loss: 0.03860468789935112\n",
            "24570 val_loss: 0.2130117416381836, train_loss: 0.039305876940488815\n",
            "24580 val_loss: 0.24168053269386292, train_loss: 0.04784347116947174\n",
            "24590 val_loss: 0.20360848307609558, train_loss: 0.0393412746489048\n",
            "24600 val_loss: 0.2048061043024063, train_loss: 0.038560084998607635\n",
            "24610 val_loss: 0.19739922881126404, train_loss: 0.03905615210533142\n",
            "24620 val_loss: 0.2117127627134323, train_loss: 0.0387638621032238\n",
            "24630 val_loss: 0.2014952003955841, train_loss: 0.038271207362413406\n",
            "24640 val_loss: 0.1972099095582962, train_loss: 0.038801975548267365\n",
            "24650 val_loss: 0.20272259414196014, train_loss: 0.03862909600138664\n",
            "24660 val_loss: 0.19595234096050262, train_loss: 0.03852963075041771\n",
            "24670 val_loss: 0.20024894177913666, train_loss: 0.03804286941885948\n",
            "24680 val_loss: 0.20687709748744965, train_loss: 0.03752419725060463\n",
            "24690 val_loss: 0.212273508310318, train_loss: 0.03750535473227501\n",
            "24700 val_loss: 0.20533907413482666, train_loss: 0.03841501474380493\n",
            "24710 val_loss: 0.20620830357074738, train_loss: 0.03813078626990318\n",
            "24720 val_loss: 0.19808943569660187, train_loss: 0.037156589329242706\n",
            "24730 val_loss: 0.20942556858062744, train_loss: 0.03924127668142319\n",
            "24740 val_loss: 0.26869913935661316, train_loss: 0.057131022214889526\n",
            "24750 val_loss: 0.21249476075172424, train_loss: 0.03805210068821907\n",
            "24760 val_loss: 0.20439399778842926, train_loss: 0.03684931993484497\n",
            "24770 val_loss: 0.2119901329278946, train_loss: 0.03669898211956024\n",
            "24780 val_loss: 0.19614246487617493, train_loss: 0.03656064718961716\n",
            "24790 val_loss: 0.20566511154174805, train_loss: 0.037231702357530594\n",
            "24800 val_loss: 0.20822221040725708, train_loss: 0.03656221553683281\n",
            "24810 val_loss: 0.20680968463420868, train_loss: 0.03684716671705246\n",
            "24820 val_loss: 0.20205533504486084, train_loss: 0.03636791184544563\n",
            "24830 val_loss: 0.20480310916900635, train_loss: 0.0387970469892025\n",
            "24840 val_loss: 0.20358414947986603, train_loss: 0.036086469888687134\n",
            "24850 val_loss: 0.20887744426727295, train_loss: 0.037655215710401535\n",
            "24860 val_loss: 0.20938891172409058, train_loss: 0.036832138895988464\n",
            "24870 val_loss: 0.20682989060878754, train_loss: 0.0357942208647728\n",
            "24880 val_loss: 0.21264226734638214, train_loss: 0.03688759356737137\n",
            "24890 val_loss: 0.19739115238189697, train_loss: 0.03535447642207146\n",
            "24900 val_loss: 0.21263417601585388, train_loss: 0.03631806746125221\n",
            "24910 val_loss: 0.1973879188299179, train_loss: 0.03666745498776436\n",
            "24920 val_loss: 0.19947068393230438, train_loss: 0.035996656864881516\n",
            "24930 val_loss: 0.20681989192962646, train_loss: 0.03659665957093239\n",
            "24940 val_loss: 0.20344622433185577, train_loss: 0.03583982214331627\n",
            "24950 val_loss: 0.2102900892496109, train_loss: 0.03549522906541824\n",
            "24960 val_loss: 0.2139098048210144, train_loss: 0.040660396218299866\n",
            "24970 val_loss: 0.21192452311515808, train_loss: 0.035339828580617905\n",
            "24980 val_loss: 0.20569215714931488, train_loss: 0.03542248532176018\n",
            "24990 val_loss: 0.19701679050922394, train_loss: 0.03550305962562561\n",
            "25000 val_loss: 0.21275945007801056, train_loss: 0.03634806349873543\n",
            "25010 val_loss: 0.19936947524547577, train_loss: 0.03524412587285042\n",
            "25020 val_loss: 0.20694267749786377, train_loss: 0.034724265336990356\n",
            "25030 val_loss: 0.20567594468593597, train_loss: 0.03459269180893898\n",
            "25040 val_loss: 0.2131744623184204, train_loss: 0.034205999225378036\n",
            "25050 val_loss: 0.21020208299160004, train_loss: 0.03530789911746979\n",
            "25060 val_loss: 0.20136797428131104, train_loss: 0.03403579443693161\n",
            "25070 val_loss: 0.21371303498744965, train_loss: 0.035284966230392456\n",
            "25080 val_loss: 0.21123048663139343, train_loss: 0.037895187735557556\n",
            "25090 val_loss: 0.20098529756069183, train_loss: 0.03414540737867355\n",
            "25100 val_loss: 0.20351266860961914, train_loss: 0.03368451073765755\n",
            "25110 val_loss: 0.21162259578704834, train_loss: 0.03435872867703438\n",
            "25120 val_loss: 0.20715738832950592, train_loss: 0.035219620913267136\n",
            "25130 val_loss: 0.20977531373500824, train_loss: 0.0333021879196167\n",
            "25140 val_loss: 0.2127794474363327, train_loss: 0.03346317261457443\n",
            "25150 val_loss: 0.2151523381471634, train_loss: 0.03293614089488983\n",
            "25160 val_loss: 0.21901221573352814, train_loss: 0.035125188529491425\n",
            "25170 val_loss: 0.22344258427619934, train_loss: 0.033828672021627426\n",
            "25180 val_loss: 0.20827558636665344, train_loss: 0.032738298177719116\n",
            "25190 val_loss: 0.20676720142364502, train_loss: 0.03298459202051163\n",
            "25200 val_loss: 0.2165125608444214, train_loss: 0.032618772238492966\n",
            "25210 val_loss: 0.21287289261817932, train_loss: 0.032924775034189224\n",
            "25220 val_loss: 0.21491926908493042, train_loss: 0.032239869236946106\n",
            "25230 val_loss: 0.20812086760997772, train_loss: 0.03482870012521744\n",
            "25240 val_loss: 0.20097215473651886, train_loss: 0.03256721794605255\n",
            "25250 val_loss: 0.2090483009815216, train_loss: 0.03246806189417839\n",
            "25260 val_loss: 0.1932796984910965, train_loss: 0.03337608277797699\n",
            "25270 val_loss: 0.19958986341953278, train_loss: 0.03222569078207016\n",
            "25280 val_loss: 0.21206362545490265, train_loss: 0.03337150067090988\n",
            "25290 val_loss: 0.2005547136068344, train_loss: 0.032323773950338364\n",
            "25300 val_loss: 0.20958073437213898, train_loss: 0.032207224518060684\n",
            "25310 val_loss: 0.20867611467838287, train_loss: 0.032217126339673996\n",
            "25320 val_loss: 0.21443650126457214, train_loss: 0.03206213563680649\n",
            "25330 val_loss: 0.21050608158111572, train_loss: 0.03192711994051933\n",
            "25340 val_loss: 0.21156653761863708, train_loss: 0.031636472791433334\n",
            "25350 val_loss: 0.24273695051670074, train_loss: 0.04100463166832924\n",
            "25360 val_loss: 0.20358683168888092, train_loss: 0.03293393924832344\n",
            "25370 val_loss: 0.21651728451251984, train_loss: 0.033625390380620956\n",
            "25380 val_loss: 0.22604955732822418, train_loss: 0.032774731516838074\n",
            "25390 val_loss: 0.21142828464508057, train_loss: 0.031488630920648575\n",
            "25400 val_loss: 0.19593217968940735, train_loss: 0.03127430006861687\n",
            "25410 val_loss: 0.22533991932868958, train_loss: 0.03371378779411316\n",
            "25420 val_loss: 0.20629090070724487, train_loss: 0.033225614577531815\n",
            "25430 val_loss: 0.20846465229988098, train_loss: 0.03780093789100647\n",
            "25440 val_loss: 0.20356205105781555, train_loss: 0.03253287822008133\n",
            "25450 val_loss: 0.21158243715763092, train_loss: 0.03175225481390953\n",
            "25460 val_loss: 0.2062849998474121, train_loss: 0.03183932974934578\n",
            "25470 val_loss: 0.2219485342502594, train_loss: 0.031420912593603134\n",
            "25480 val_loss: 0.2076157182455063, train_loss: 0.03136948496103287\n",
            "25490 val_loss: 0.21656282246112823, train_loss: 0.03261983022093773\n",
            "25500 val_loss: 0.19365975260734558, train_loss: 0.03147229179739952\n",
            "25510 val_loss: 0.1944476068019867, train_loss: 0.031139502301812172\n",
            "25520 val_loss: 0.2095770388841629, train_loss: 0.030642038211226463\n",
            "25530 val_loss: 0.21457034349441528, train_loss: 0.031543079763650894\n",
            "25540 val_loss: 0.19206063449382782, train_loss: 0.03199653699994087\n",
            "25550 val_loss: 0.19644655287265778, train_loss: 0.03125949203968048\n",
            "25560 val_loss: 0.20105378329753876, train_loss: 0.030754420906305313\n",
            "25570 val_loss: 0.20284807682037354, train_loss: 0.030517449602484703\n",
            "25580 val_loss: 0.20404717326164246, train_loss: 0.03013237565755844\n",
            "25590 val_loss: 0.1937803030014038, train_loss: 0.031285159289836884\n",
            "25600 val_loss: 0.1994399130344391, train_loss: 0.030015280470252037\n",
            "25610 val_loss: 0.21001963317394257, train_loss: 0.029786445200443268\n",
            "25620 val_loss: 0.2026316374540329, train_loss: 0.02958032488822937\n",
            "25630 val_loss: 0.21183742582798004, train_loss: 0.030807770788669586\n",
            "25640 val_loss: 0.19237717986106873, train_loss: 0.030192920938134193\n",
            "25650 val_loss: 0.21621139347553253, train_loss: 0.03047017753124237\n",
            "25660 val_loss: 0.21166837215423584, train_loss: 0.030199194326996803\n",
            "25670 val_loss: 0.2129964530467987, train_loss: 0.029240449890494347\n",
            "25680 val_loss: 0.20825538039207458, train_loss: 0.02974349819123745\n",
            "25690 val_loss: 0.20830675959587097, train_loss: 0.02979467809200287\n",
            "25700 val_loss: 0.21018703281879425, train_loss: 0.029259398579597473\n",
            "25710 val_loss: 0.22641919553279877, train_loss: 0.030101696029305458\n",
            "25720 val_loss: 0.1960485726594925, train_loss: 0.028920168057084084\n",
            "25730 val_loss: 0.21354958415031433, train_loss: 0.029035942628979683\n",
            "25740 val_loss: 0.20144285261631012, train_loss: 0.029103534296154976\n",
            "25750 val_loss: 0.21577292680740356, train_loss: 0.029309354722499847\n",
            "25760 val_loss: 0.20434415340423584, train_loss: 0.0298796147108078\n",
            "25770 val_loss: 0.19883687794208527, train_loss: 0.029044872149825096\n",
            "25780 val_loss: 0.2069455087184906, train_loss: 0.02876773476600647\n",
            "25790 val_loss: 0.21105453372001648, train_loss: 0.028282059356570244\n",
            "25800 val_loss: 0.2192276567220688, train_loss: 0.03241952881217003\n",
            "25810 val_loss: 0.2209114283323288, train_loss: 0.030016377568244934\n",
            "25820 val_loss: 0.21429027616977692, train_loss: 0.029051844030618668\n",
            "25830 val_loss: 0.2003825306892395, train_loss: 0.028557492420077324\n",
            "25840 val_loss: 0.1969500035047531, train_loss: 0.028731832280755043\n",
            "25850 val_loss: 0.21011488139629364, train_loss: 0.029562553390860558\n",
            "25860 val_loss: 0.21656717360019684, train_loss: 0.028798239305615425\n",
            "25870 val_loss: 0.22058075666427612, train_loss: 0.029019411653280258\n",
            "25880 val_loss: 0.20265205204486847, train_loss: 0.02886868268251419\n",
            "25890 val_loss: 0.21149341762065887, train_loss: 0.02915208227932453\n",
            "25900 val_loss: 0.20670145750045776, train_loss: 0.028675343841314316\n",
            "25910 val_loss: 0.20433689653873444, train_loss: 0.02834763564169407\n",
            "25920 val_loss: 0.21309569478034973, train_loss: 0.0294902715831995\n",
            "25930 val_loss: 0.1938270628452301, train_loss: 0.028340008109807968\n",
            "25940 val_loss: 0.20054274797439575, train_loss: 0.028540536761283875\n",
            "25950 val_loss: 0.2000306248664856, train_loss: 0.028098970651626587\n",
            "25960 val_loss: 0.2021455019712448, train_loss: 0.028281865641474724\n",
            "25970 val_loss: 0.18930742144584656, train_loss: 0.028226930648088455\n",
            "25980 val_loss: 0.19598892331123352, train_loss: 0.02849007211625576\n",
            "25990 val_loss: 0.21468500792980194, train_loss: 0.027663474902510643\n",
            "26000 val_loss: 0.21278569102287292, train_loss: 0.02792811393737793\n",
            "26010 val_loss: 0.21263492107391357, train_loss: 0.028580011799931526\n",
            "26020 val_loss: 0.20244906842708588, train_loss: 0.02769657038152218\n",
            "26030 val_loss: 0.21853502094745636, train_loss: 0.02802889049053192\n",
            "26040 val_loss: 0.2132791131734848, train_loss: 0.02730037458240986\n",
            "26050 val_loss: 0.2039186805486679, train_loss: 0.02745940536260605\n",
            "26060 val_loss: 0.19774194061756134, train_loss: 0.027507567778229713\n",
            "26070 val_loss: 0.20825304090976715, train_loss: 0.02711835689842701\n",
            "26080 val_loss: 0.21101222932338715, train_loss: 0.026972977444529533\n",
            "26090 val_loss: 0.21100866794586182, train_loss: 0.03255382180213928\n",
            "26100 val_loss: 0.19192788004875183, train_loss: 0.027825508266687393\n",
            "26110 val_loss: 0.19421499967575073, train_loss: 0.027090126648545265\n",
            "26120 val_loss: 0.18416255712509155, train_loss: 0.028333265334367752\n",
            "26130 val_loss: 0.22237496078014374, train_loss: 0.027649754658341408\n",
            "26140 val_loss: 0.21495530009269714, train_loss: 0.027333900332450867\n",
            "26150 val_loss: 0.20836859941482544, train_loss: 0.027161993086338043\n",
            "26160 val_loss: 0.20654137432575226, train_loss: 0.026964377611875534\n",
            "26170 val_loss: 0.21686360239982605, train_loss: 0.02720663696527481\n",
            "26180 val_loss: 0.20641596615314484, train_loss: 0.026965832337737083\n",
            "26190 val_loss: 0.21529266238212585, train_loss: 0.02723010443150997\n",
            "26200 val_loss: 0.2226095199584961, train_loss: 0.028442636132240295\n",
            "26210 val_loss: 0.21428269147872925, train_loss: 0.027816183865070343\n",
            "26220 val_loss: 0.2092316448688507, train_loss: 0.026929738000035286\n",
            "26230 val_loss: 0.20352251827716827, train_loss: 0.027064092457294464\n",
            "26240 val_loss: 0.21312573552131653, train_loss: 0.026754919439554214\n",
            "26250 val_loss: 0.21787980198860168, train_loss: 0.0269427839666605\n",
            "26260 val_loss: 0.20786814391613007, train_loss: 0.02657490409910679\n",
            "26270 val_loss: 0.19917747378349304, train_loss: 0.026923619210720062\n",
            "26280 val_loss: 0.22108018398284912, train_loss: 0.026457851752638817\n",
            "26290 val_loss: 0.2123800665140152, train_loss: 0.02839767187833786\n",
            "26300 val_loss: 0.19311124086380005, train_loss: 0.02901352010667324\n",
            "26310 val_loss: 0.20910675823688507, train_loss: 0.026612374931573868\n",
            "26320 val_loss: 0.1984499990940094, train_loss: 0.027332689613103867\n",
            "26330 val_loss: 0.2019539177417755, train_loss: 0.026348326355218887\n",
            "26340 val_loss: 0.1998288333415985, train_loss: 0.02630036696791649\n",
            "26350 val_loss: 0.20201323926448822, train_loss: 0.026626992970705032\n",
            "26360 val_loss: 0.20734284818172455, train_loss: 0.026227498427033424\n",
            "26370 val_loss: 0.20065966248512268, train_loss: 0.02613324299454689\n",
            "26380 val_loss: 0.20449095964431763, train_loss: 0.026008240878582\n",
            "26390 val_loss: 0.19629427790641785, train_loss: 0.026302101090550423\n",
            "26400 val_loss: 0.21212559938430786, train_loss: 0.02587355114519596\n",
            "26410 val_loss: 0.22213447093963623, train_loss: 0.026130085811018944\n",
            "26420 val_loss: 0.21743465960025787, train_loss: 0.025993146002292633\n",
            "26430 val_loss: 0.2213483303785324, train_loss: 0.02618381939828396\n",
            "26440 val_loss: 0.2045353651046753, train_loss: 0.026255423203110695\n",
            "26450 val_loss: 0.20949701964855194, train_loss: 0.026123855262994766\n",
            "26460 val_loss: 0.20964109897613525, train_loss: 0.025862369686365128\n",
            "26470 val_loss: 0.2108728587627411, train_loss: 0.02648404985666275\n",
            "26480 val_loss: 0.2078850120306015, train_loss: 0.026154473423957825\n",
            "26490 val_loss: 0.21287286281585693, train_loss: 0.02631930634379387\n",
            "26500 val_loss: 0.22448162734508514, train_loss: 0.026252349838614464\n",
            "26510 val_loss: 0.22572898864746094, train_loss: 0.025765806436538696\n",
            "26520 val_loss: 0.22900263965129852, train_loss: 0.026286188513040543\n",
            "26530 val_loss: 0.22383278608322144, train_loss: 0.025737473741173744\n",
            "26540 val_loss: 0.20234377682209015, train_loss: 0.026089755818247795\n",
            "26550 val_loss: 0.20617572963237762, train_loss: 0.026135189458727837\n",
            "26560 val_loss: 0.20935501158237457, train_loss: 0.025354474782943726\n",
            "26570 val_loss: 0.20825287699699402, train_loss: 0.025744078680872917\n",
            "26580 val_loss: 0.2267819344997406, train_loss: 0.02637193724513054\n",
            "26590 val_loss: 0.20650656521320343, train_loss: 0.02538336254656315\n",
            "26600 val_loss: 0.23456630110740662, train_loss: 0.02764599584043026\n",
            "26610 val_loss: 0.2119356393814087, train_loss: 0.025329526513814926\n",
            "26620 val_loss: 0.20731914043426514, train_loss: 0.02518591843545437\n",
            "26630 val_loss: 0.21044397354125977, train_loss: 0.025075014680624008\n",
            "26640 val_loss: 0.21272429823875427, train_loss: 0.02545757032930851\n",
            "26650 val_loss: 0.20733854174613953, train_loss: 0.025689348578453064\n",
            "26660 val_loss: 0.2440641075372696, train_loss: 0.027800966054201126\n",
            "26670 val_loss: 0.2059730589389801, train_loss: 0.025332171469926834\n",
            "26680 val_loss: 0.20122064650058746, train_loss: 0.025981420651078224\n",
            "26690 val_loss: 0.22867368161678314, train_loss: 0.026647550985217094\n",
            "26700 val_loss: 0.2158823013305664, train_loss: 0.027617011219263077\n",
            "26710 val_loss: 0.21354489028453827, train_loss: 0.025097304955124855\n",
            "26720 val_loss: 0.2171287089586258, train_loss: 0.025168299674987793\n",
            "26730 val_loss: 0.20742911100387573, train_loss: 0.025233520194888115\n",
            "26740 val_loss: 0.22225116193294525, train_loss: 0.025309331715106964\n",
            "26750 val_loss: 0.21397148072719574, train_loss: 0.024968864396214485\n",
            "26760 val_loss: 0.21791943907737732, train_loss: 0.025814106687903404\n",
            "26770 val_loss: 0.19928905367851257, train_loss: 0.024985576048493385\n",
            "26780 val_loss: 0.19342800974845886, train_loss: 0.026273498311638832\n",
            "26790 val_loss: 0.2057226449251175, train_loss: 0.024917274713516235\n",
            "26800 val_loss: 0.26587989926338196, train_loss: 0.04260055348277092\n",
            "26810 val_loss: 0.2273755818605423, train_loss: 0.026404667645692825\n",
            "26820 val_loss: 0.1976310908794403, train_loss: 0.02507324516773224\n",
            "26830 val_loss: 0.2067294418811798, train_loss: 0.024817798286676407\n",
            "26840 val_loss: 0.2026320844888687, train_loss: 0.025142889469861984\n",
            "26850 val_loss: 0.19408763945102692, train_loss: 0.025134116411209106\n",
            "26860 val_loss: 0.20250943303108215, train_loss: 0.02501653879880905\n",
            "26870 val_loss: 0.19792872667312622, train_loss: 0.02646450698375702\n",
            "26880 val_loss: 0.2022734135389328, train_loss: 0.02482210285961628\n",
            "26890 val_loss: 0.23403319716453552, train_loss: 0.02598126046359539\n",
            "26900 val_loss: 0.22248132526874542, train_loss: 0.024803338572382927\n",
            "26910 val_loss: 0.22087958455085754, train_loss: 0.026354026049375534\n",
            "26920 val_loss: 0.250692218542099, train_loss: 0.028362860903143883\n",
            "26930 val_loss: 0.20813517272472382, train_loss: 0.02547065168619156\n",
            "26940 val_loss: 0.2140597105026245, train_loss: 0.024396168068051338\n",
            "26950 val_loss: 0.20748856663703918, train_loss: 0.024437744170427322\n",
            "26960 val_loss: 0.21425849199295044, train_loss: 0.0244152769446373\n",
            "26970 val_loss: 0.20566856861114502, train_loss: 0.025578338652849197\n",
            "26980 val_loss: 0.2211511880159378, train_loss: 0.02405724674463272\n",
            "26990 val_loss: 0.20479175448417664, train_loss: 0.024242129176855087\n",
            "27000 val_loss: 0.22215761244297028, train_loss: 0.023885542526841164\n",
            "27010 val_loss: 0.23092839121818542, train_loss: 0.024905890226364136\n",
            "27020 val_loss: 0.1987641155719757, train_loss: 0.023972107097506523\n",
            "27030 val_loss: 0.19898183643817902, train_loss: 0.02423986606299877\n",
            "27040 val_loss: 0.21454933285713196, train_loss: 0.02413453534245491\n",
            "27050 val_loss: 0.21432268619537354, train_loss: 0.02443738281726837\n",
            "27060 val_loss: 0.20389172434806824, train_loss: 0.024317264556884766\n",
            "27070 val_loss: 0.21821454167366028, train_loss: 0.024094875901937485\n",
            "27080 val_loss: 0.22492313385009766, train_loss: 0.024080965667963028\n",
            "27090 val_loss: 0.20423655211925507, train_loss: 0.02450818195939064\n",
            "27100 val_loss: 0.2035934329032898, train_loss: 0.02487124688923359\n",
            "27110 val_loss: 0.20348380506038666, train_loss: 0.025947529822587967\n",
            "27120 val_loss: 0.21574556827545166, train_loss: 0.02360895462334156\n",
            "27130 val_loss: 0.20357488095760345, train_loss: 0.023662615567445755\n",
            "27140 val_loss: 0.22014331817626953, train_loss: 0.023648666217923164\n",
            "27150 val_loss: 0.22845813632011414, train_loss: 0.025309085845947266\n",
            "27160 val_loss: 0.20593638718128204, train_loss: 0.0237120408564806\n",
            "27170 val_loss: 0.21262593567371368, train_loss: 0.023340703919529915\n",
            "27180 val_loss: 0.22936590015888214, train_loss: 0.023746972903609276\n",
            "27190 val_loss: 0.23248347640037537, train_loss: 0.025215603411197662\n",
            "27200 val_loss: 0.20626744627952576, train_loss: 0.024174878373742104\n",
            "27210 val_loss: 0.2050131857395172, train_loss: 0.023256873711943626\n",
            "27220 val_loss: 0.21344983577728271, train_loss: 0.023501157760620117\n",
            "27230 val_loss: 0.22946809232234955, train_loss: 0.023548223078250885\n",
            "27240 val_loss: 0.22639191150665283, train_loss: 0.023435404524207115\n",
            "27250 val_loss: 0.199790358543396, train_loss: 0.02370542101562023\n",
            "27260 val_loss: 0.24311771988868713, train_loss: 0.025963321328163147\n",
            "27270 val_loss: 0.21469232439994812, train_loss: 0.023379597812891006\n",
            "27280 val_loss: 0.21320606768131256, train_loss: 0.02287553809583187\n",
            "27290 val_loss: 0.21092727780342102, train_loss: 0.022856784984469414\n",
            "27300 val_loss: 0.2048189491033554, train_loss: 0.02328110672533512\n",
            "27310 val_loss: 0.1912950575351715, train_loss: 0.02391723543405533\n",
            "27320 val_loss: 0.2215423285961151, train_loss: 0.023791896179318428\n",
            "27330 val_loss: 0.20584094524383545, train_loss: 0.023372337222099304\n",
            "27340 val_loss: 0.19883888959884644, train_loss: 0.023493120446801186\n",
            "27350 val_loss: 0.20055289566516876, train_loss: 0.02596079558134079\n",
            "27360 val_loss: 0.2171919196844101, train_loss: 0.022932490333914757\n",
            "27370 val_loss: 0.20477871596813202, train_loss: 0.023040229454636574\n",
            "27380 val_loss: 0.21978656947612762, train_loss: 0.022803526371717453\n",
            "27390 val_loss: 0.23378849029541016, train_loss: 0.02361004613339901\n",
            "27400 val_loss: 0.19948582351207733, train_loss: 0.02394235134124756\n",
            "27410 val_loss: 0.2156599760055542, train_loss: 0.022933106869459152\n",
            "27420 val_loss: 0.19807812571525574, train_loss: 0.026134096086025238\n",
            "27430 val_loss: 0.21043123304843903, train_loss: 0.023398756980895996\n",
            "27440 val_loss: 0.1989157497882843, train_loss: 0.023211870342493057\n",
            "27450 val_loss: 0.22340914607048035, train_loss: 0.02293376624584198\n",
            "27460 val_loss: 0.19396305084228516, train_loss: 0.024588124826550484\n",
            "27470 val_loss: 0.22061637043952942, train_loss: 0.022432133555412292\n",
            "27480 val_loss: 0.20693501830101013, train_loss: 0.023041274398565292\n",
            "27490 val_loss: 0.2234814465045929, train_loss: 0.022396495565772057\n",
            "27500 val_loss: 0.20102497935295105, train_loss: 0.02298000641167164\n",
            "27510 val_loss: 0.22898432612419128, train_loss: 0.02336081862449646\n",
            "27520 val_loss: 0.22091343998908997, train_loss: 0.02221064642071724\n",
            "27530 val_loss: 0.20382264256477356, train_loss: 0.022442147135734558\n",
            "27540 val_loss: 0.20897839963436127, train_loss: 0.022334247827529907\n",
            "27550 val_loss: 0.20650525391101837, train_loss: 0.022619154304265976\n",
            "27560 val_loss: 0.22673742473125458, train_loss: 0.022367697209119797\n",
            "27570 val_loss: 0.22223147749900818, train_loss: 0.023153742775321007\n",
            "27580 val_loss: 0.23292867839336395, train_loss: 0.022364284843206406\n",
            "27590 val_loss: 0.21252845227718353, train_loss: 0.022186480462551117\n",
            "27600 val_loss: 0.22511829435825348, train_loss: 0.022315606474876404\n",
            "27610 val_loss: 0.22185811400413513, train_loss: 0.022713957354426384\n",
            "27620 val_loss: 0.23212788999080658, train_loss: 0.022841164842247963\n",
            "27630 val_loss: 0.2323637455701828, train_loss: 0.02273423969745636\n",
            "27640 val_loss: 0.2137424796819687, train_loss: 0.02221394143998623\n",
            "27650 val_loss: 0.20186074078083038, train_loss: 0.022338707000017166\n",
            "27660 val_loss: 0.21442919969558716, train_loss: 0.022310486063361168\n",
            "27670 val_loss: 0.21607628464698792, train_loss: 0.02239314466714859\n",
            "27680 val_loss: 0.22274789214134216, train_loss: 0.02232859656214714\n",
            "27690 val_loss: 0.23636691272258759, train_loss: 0.022267580032348633\n",
            "27700 val_loss: 0.2214302122592926, train_loss: 0.02201041951775551\n",
            "27710 val_loss: 0.1951669454574585, train_loss: 0.025066496804356575\n",
            "27720 val_loss: 0.21065646409988403, train_loss: 0.02187689207494259\n",
            "27730 val_loss: 0.2181328386068344, train_loss: 0.02224738337099552\n",
            "27740 val_loss: 0.21066097915172577, train_loss: 0.02207845076918602\n",
            "27750 val_loss: 0.21739594638347626, train_loss: 0.022518085315823555\n",
            "27760 val_loss: 0.20633848011493683, train_loss: 0.02219686657190323\n",
            "27770 val_loss: 0.21871314942836761, train_loss: 0.02196507528424263\n",
            "27780 val_loss: 0.21352526545524597, train_loss: 0.02169397473335266\n",
            "27790 val_loss: 0.219142347574234, train_loss: 0.022487634792923927\n",
            "27800 val_loss: 0.21460194885730743, train_loss: 0.021622546017169952\n",
            "27810 val_loss: 0.22832831740379333, train_loss: 0.02152818627655506\n",
            "27820 val_loss: 0.22087889909744263, train_loss: 0.022287288680672646\n",
            "27830 val_loss: 0.23783819377422333, train_loss: 0.022768409922719002\n",
            "27840 val_loss: 0.19390933215618134, train_loss: 0.022759482264518738\n",
            "27850 val_loss: 0.20807704329490662, train_loss: 0.021807843819260597\n",
            "27860 val_loss: 0.21525971591472626, train_loss: 0.021878845989704132\n",
            "27870 val_loss: 0.2172364741563797, train_loss: 0.021456286311149597\n",
            "27880 val_loss: 0.2558249831199646, train_loss: 0.02264571748673916\n",
            "27890 val_loss: 0.2172641009092331, train_loss: 0.02222193032503128\n",
            "27900 val_loss: 0.2101101130247116, train_loss: 0.02151692658662796\n",
            "27910 val_loss: 0.21099308133125305, train_loss: 0.022210702300071716\n",
            "27920 val_loss: 0.21597735583782196, train_loss: 0.021335460245609283\n",
            "27930 val_loss: 0.22846300899982452, train_loss: 0.02115565910935402\n",
            "27940 val_loss: 0.24218983948230743, train_loss: 0.023765897378325462\n",
            "27950 val_loss: 0.22230418026447296, train_loss: 0.021481966599822044\n",
            "27960 val_loss: 0.22671429812908173, train_loss: 0.021331986412405968\n",
            "27970 val_loss: 0.19954785704612732, train_loss: 0.021754782646894455\n",
            "27980 val_loss: 0.2116507589817047, train_loss: 0.021421005949378014\n",
            "27990 val_loss: 0.1976536363363266, train_loss: 0.022054163739085197\n",
            "28000 val_loss: 0.1992165744304657, train_loss: 0.0220410767942667\n",
            "28010 val_loss: 0.23766282200813293, train_loss: 0.02343805879354477\n",
            "28020 val_loss: 0.22073248028755188, train_loss: 0.02204110659658909\n",
            "28030 val_loss: 0.2503716051578522, train_loss: 0.028108883649110794\n",
            "28040 val_loss: 0.20751726627349854, train_loss: 0.022248800843954086\n",
            "28050 val_loss: 0.2195858508348465, train_loss: 0.02161583863198757\n",
            "28060 val_loss: 0.21745721995830536, train_loss: 0.021604591980576515\n",
            "28070 val_loss: 0.24489198625087738, train_loss: 0.022269267588853836\n",
            "28080 val_loss: 0.22656667232513428, train_loss: 0.02137978933751583\n",
            "28090 val_loss: 0.22551104426383972, train_loss: 0.021066762506961823\n",
            "28100 val_loss: 0.22771231830120087, train_loss: 0.021183187142014503\n",
            "28110 val_loss: 0.22119830548763275, train_loss: 0.021179065108299255\n",
            "28120 val_loss: 0.1953733116388321, train_loss: 0.02297273464500904\n",
            "28130 val_loss: 0.22077074646949768, train_loss: 0.02166547253727913\n",
            "28140 val_loss: 0.2254420667886734, train_loss: 0.02089814841747284\n",
            "28150 val_loss: 0.2208162248134613, train_loss: 0.02138976939022541\n",
            "28160 val_loss: 0.21615460515022278, train_loss: 0.021357877179980278\n",
            "28170 val_loss: 0.21745741367340088, train_loss: 0.021081821992993355\n",
            "28180 val_loss: 0.22910849750041962, train_loss: 0.021404311060905457\n",
            "28190 val_loss: 0.22450216114521027, train_loss: 0.02080591581761837\n",
            "28200 val_loss: 0.22567236423492432, train_loss: 0.02085484005510807\n",
            "28210 val_loss: 0.22880207002162933, train_loss: 0.0209882240742445\n",
            "28220 val_loss: 0.22840502858161926, train_loss: 0.02101367525756359\n",
            "28230 val_loss: 0.21677455306053162, train_loss: 0.02077907882630825\n",
            "28240 val_loss: 0.21850234270095825, train_loss: 0.020941922441124916\n",
            "28250 val_loss: 0.2138034999370575, train_loss: 0.021143095567822456\n",
            "28260 val_loss: 0.21242281794548035, train_loss: 0.020750442519783974\n",
            "28270 val_loss: 0.20419155061244965, train_loss: 0.021611252799630165\n",
            "28280 val_loss: 0.23287110030651093, train_loss: 0.0206951592117548\n",
            "28290 val_loss: 0.22535501420497894, train_loss: 0.020943989977240562\n",
            "28300 val_loss: 0.22194130718708038, train_loss: 0.0207514688372612\n",
            "28310 val_loss: 0.2175695300102234, train_loss: 0.020677397027611732\n",
            "28320 val_loss: 0.21655108034610748, train_loss: 0.020499322563409805\n",
            "28330 val_loss: 0.21857942640781403, train_loss: 0.02096709981560707\n",
            "28340 val_loss: 0.22499462962150574, train_loss: 0.020611681044101715\n",
            "28350 val_loss: 0.2295904904603958, train_loss: 0.020963499322533607\n",
            "28360 val_loss: 0.23351222276687622, train_loss: 0.0211543720215559\n",
            "28370 val_loss: 0.22525432705879211, train_loss: 0.020881619304418564\n",
            "28380 val_loss: 0.23460319638252258, train_loss: 0.020884618163108826\n",
            "28390 val_loss: 0.23070338368415833, train_loss: 0.0205684881657362\n",
            "28400 val_loss: 0.21965515613555908, train_loss: 0.02037140540778637\n",
            "28410 val_loss: 0.2090815156698227, train_loss: 0.020405637100338936\n",
            "28420 val_loss: 0.22586429119110107, train_loss: 0.020264850929379463\n",
            "28430 val_loss: 0.21472515165805817, train_loss: 0.02052771858870983\n",
            "28440 val_loss: 0.21398933231830597, train_loss: 0.02061062678694725\n",
            "28450 val_loss: 0.215995654463768, train_loss: 0.020582793280482292\n",
            "28460 val_loss: 0.21876651048660278, train_loss: 0.020314404740929604\n",
            "28470 val_loss: 0.2114984393119812, train_loss: 0.02057025581598282\n",
            "28480 val_loss: 0.21411749720573425, train_loss: 0.02047216147184372\n",
            "28490 val_loss: 0.23885808885097504, train_loss: 0.020155571401119232\n",
            "28500 val_loss: 0.21698743104934692, train_loss: 0.020276641473174095\n",
            "28510 val_loss: 0.1900981217622757, train_loss: 0.02355060540139675\n",
            "28520 val_loss: 0.22105629742145538, train_loss: 0.020109102129936218\n",
            "28530 val_loss: 0.20925414562225342, train_loss: 0.021382929757237434\n",
            "28540 val_loss: 0.20372183620929718, train_loss: 0.020476918667554855\n",
            "28550 val_loss: 0.20458753407001495, train_loss: 0.020522043108940125\n",
            "28560 val_loss: 0.2103578746318817, train_loss: 0.020089024677872658\n",
            "28570 val_loss: 0.22400438785552979, train_loss: 0.019827967509627342\n",
            "28580 val_loss: 0.26526620984077454, train_loss: 0.02338164485991001\n",
            "28590 val_loss: 0.2255024015903473, train_loss: 0.019783683121204376\n",
            "28600 val_loss: 0.24321816861629486, train_loss: 0.02076038531959057\n",
            "28610 val_loss: 0.1894090622663498, train_loss: 0.02178378403186798\n",
            "28620 val_loss: 0.21394652128219604, train_loss: 0.020042747259140015\n",
            "28630 val_loss: 0.21620069444179535, train_loss: 0.01996656507253647\n",
            "28640 val_loss: 0.2228172868490219, train_loss: 0.02336897887289524\n",
            "28650 val_loss: 0.21768955886363983, train_loss: 0.019656414166092873\n",
            "28660 val_loss: 0.2251516580581665, train_loss: 0.020085856318473816\n",
            "28670 val_loss: 0.22013162076473236, train_loss: 0.02008555456995964\n",
            "28680 val_loss: 0.22863657772541046, train_loss: 0.020036347210407257\n",
            "28690 val_loss: 0.21285703778266907, train_loss: 0.019921185448765755\n",
            "28700 val_loss: 0.218512624502182, train_loss: 0.019783340394496918\n",
            "28710 val_loss: 0.24058419466018677, train_loss: 0.020124321803450584\n",
            "28720 val_loss: 0.25221818685531616, train_loss: 0.01989840902388096\n",
            "28730 val_loss: 0.21588633954524994, train_loss: 0.01977517083287239\n",
            "28740 val_loss: 0.2372570037841797, train_loss: 0.019714882597327232\n",
            "28750 val_loss: 0.22810804843902588, train_loss: 0.019574569538235664\n",
            "28760 val_loss: 0.23572513461112976, train_loss: 0.019616423174738884\n",
            "28770 val_loss: 0.24457094073295593, train_loss: 0.021023167297244072\n",
            "28780 val_loss: 0.24021638929843903, train_loss: 0.020009510219097137\n",
            "28790 val_loss: 0.257598340511322, train_loss: 0.022339127957820892\n",
            "28800 val_loss: 0.22943015396595, train_loss: 0.019557124003767967\n",
            "28810 val_loss: 0.23227934539318085, train_loss: 0.019445136189460754\n",
            "28820 val_loss: 0.23485444486141205, train_loss: 0.019728679209947586\n",
            "28830 val_loss: 0.2253798097372055, train_loss: 0.01965579017996788\n",
            "28840 val_loss: 0.21383912861347198, train_loss: 0.020084256306290627\n",
            "28850 val_loss: 0.22333648800849915, train_loss: 0.019316110759973526\n",
            "28860 val_loss: 0.21216601133346558, train_loss: 0.019627410918474197\n",
            "28870 val_loss: 0.20413830876350403, train_loss: 0.020153766497969627\n",
            "28880 val_loss: 0.22667568922042847, train_loss: 0.02015301212668419\n",
            "28890 val_loss: 0.23219303786754608, train_loss: 0.019765600562095642\n",
            "28900 val_loss: 0.2101060301065445, train_loss: 0.019174691289663315\n",
            "28910 val_loss: 0.23721781373023987, train_loss: 0.023275107145309448\n",
            "28920 val_loss: 0.2310948371887207, train_loss: 0.019110018387436867\n",
            "28930 val_loss: 0.2170005738735199, train_loss: 0.019708970561623573\n",
            "28940 val_loss: 0.24104300141334534, train_loss: 0.020179321989417076\n",
            "28950 val_loss: 0.21458542346954346, train_loss: 0.019915098324418068\n",
            "28960 val_loss: 0.22946205735206604, train_loss: 0.019392941147089005\n",
            "28970 val_loss: 0.230101078748703, train_loss: 0.019325019791722298\n",
            "28980 val_loss: 0.2153826653957367, train_loss: 0.0191744826734066\n",
            "28990 val_loss: 0.22276131808757782, train_loss: 0.018863389268517494\n",
            "29000 val_loss: 0.2352098524570465, train_loss: 0.01913534104824066\n",
            "29010 val_loss: 0.24034391343593597, train_loss: 0.018812106922268867\n",
            "29020 val_loss: 0.2228562980890274, train_loss: 0.018844764679670334\n",
            "29030 val_loss: 0.23636847734451294, train_loss: 0.018695276230573654\n",
            "29040 val_loss: 0.22491124272346497, train_loss: 0.01864835061132908\n",
            "29050 val_loss: 0.20838022232055664, train_loss: 0.01890852116048336\n",
            "29060 val_loss: 0.22221916913986206, train_loss: 0.018686357885599136\n",
            "29070 val_loss: 0.220438614487648, train_loss: 0.018724694848060608\n",
            "29080 val_loss: 0.21602527797222137, train_loss: 0.01855410635471344\n",
            "29090 val_loss: 0.22168037295341492, train_loss: 0.018601272255182266\n",
            "29100 val_loss: 0.21770477294921875, train_loss: 0.019038166850805283\n",
            "29110 val_loss: 0.2423543483018875, train_loss: 0.018582314252853394\n",
            "29120 val_loss: 0.2226959615945816, train_loss: 0.01858600229024887\n",
            "29130 val_loss: 0.2682933807373047, train_loss: 0.01850087009370327\n",
            "29140 val_loss: 0.2386263906955719, train_loss: 0.019347339868545532\n",
            "29150 val_loss: 0.2382093220949173, train_loss: 0.018345907330513\n",
            "29160 val_loss: 0.22411301732063293, train_loss: 0.01838691532611847\n",
            "29170 val_loss: 0.22196505963802338, train_loss: 0.018948078155517578\n",
            "29180 val_loss: 0.22587017714977264, train_loss: 0.018265334889292717\n",
            "29190 val_loss: 0.23351891338825226, train_loss: 0.018494050949811935\n",
            "29200 val_loss: 0.23825275897979736, train_loss: 0.01843532919883728\n",
            "29210 val_loss: 0.2407836616039276, train_loss: 0.01812640205025673\n",
            "29220 val_loss: 0.2243727743625641, train_loss: 0.01804284192621708\n",
            "29230 val_loss: 0.2120053917169571, train_loss: 0.01866777613759041\n",
            "29240 val_loss: 0.23903396725654602, train_loss: 0.01838807947933674\n",
            "29250 val_loss: 0.23405325412750244, train_loss: 0.022310243919491768\n",
            "29260 val_loss: 0.22075489163398743, train_loss: 0.018244335427880287\n",
            "29270 val_loss: 0.23707249760627747, train_loss: 0.018367258831858635\n",
            "29280 val_loss: 0.23076677322387695, train_loss: 0.01822114922106266\n",
            "29290 val_loss: 0.2345181405544281, train_loss: 0.01820388063788414\n",
            "29300 val_loss: 0.2134641855955124, train_loss: 0.01893957331776619\n",
            "29310 val_loss: 0.2368556261062622, train_loss: 0.02503042481839657\n",
            "29320 val_loss: 0.2241702526807785, train_loss: 0.018313396722078323\n",
            "29330 val_loss: 0.19201834499835968, train_loss: 0.01943531259894371\n",
            "29340 val_loss: 0.2148839235305786, train_loss: 0.019853992387652397\n",
            "29350 val_loss: 0.2373945415019989, train_loss: 0.018248319625854492\n",
            "29360 val_loss: 0.22577936947345734, train_loss: 0.01844894327223301\n",
            "29370 val_loss: 0.23442360758781433, train_loss: 0.018439162522554398\n",
            "29380 val_loss: 0.24234530329704285, train_loss: 0.017977405339479446\n",
            "29390 val_loss: 0.21114417910575867, train_loss: 0.018228065222501755\n",
            "29400 val_loss: 0.21924768388271332, train_loss: 0.018128257244825363\n",
            "29410 val_loss: 0.2145944982767105, train_loss: 0.017997635528445244\n",
            "29420 val_loss: 0.22436383366584778, train_loss: 0.01887267269194126\n",
            "29430 val_loss: 0.2171211540699005, train_loss: 0.018474943935871124\n",
            "29440 val_loss: 0.22568820416927338, train_loss: 0.018118102103471756\n",
            "29450 val_loss: 0.20902758836746216, train_loss: 0.019082751125097275\n",
            "29460 val_loss: 0.21896927058696747, train_loss: 0.01898207888007164\n",
            "29470 val_loss: 0.22649189829826355, train_loss: 0.01792220026254654\n",
            "29480 val_loss: 0.24348457157611847, train_loss: 0.018388669937849045\n",
            "29490 val_loss: 0.21434824168682098, train_loss: 0.018837498500943184\n",
            "29500 val_loss: 0.225246861577034, train_loss: 0.01792898029088974\n",
            "29510 val_loss: 0.25135311484336853, train_loss: 0.0184564758092165\n",
            "29520 val_loss: 0.23553358018398285, train_loss: 0.017865927889943123\n",
            "29530 val_loss: 0.2186187505722046, train_loss: 0.018058808520436287\n",
            "29540 val_loss: 0.25643807649612427, train_loss: 0.02316197380423546\n",
            "29550 val_loss: 0.23894457519054413, train_loss: 0.018085401505231857\n",
            "29560 val_loss: 0.2171168029308319, train_loss: 0.01804039441049099\n",
            "29570 val_loss: 0.19822363555431366, train_loss: 0.020161805674433708\n",
            "29580 val_loss: 0.2366989254951477, train_loss: 0.018218550831079483\n",
            "29590 val_loss: 0.2271546572446823, train_loss: 0.018268560990691185\n",
            "29600 val_loss: 0.2061198353767395, train_loss: 0.019324716180562973\n",
            "29610 val_loss: 0.22952105104923248, train_loss: 0.017986351624131203\n",
            "29620 val_loss: 0.24973934888839722, train_loss: 0.018445169553160667\n",
            "29630 val_loss: 0.23009707033634186, train_loss: 0.017862344160676003\n",
            "29640 val_loss: 0.24473167955875397, train_loss: 0.018458167091012\n",
            "29650 val_loss: 0.22666090726852417, train_loss: 0.0178228672593832\n",
            "29660 val_loss: 0.20901823043823242, train_loss: 0.019119277596473694\n",
            "29670 val_loss: 0.2252611219882965, train_loss: 0.01817788928747177\n",
            "29680 val_loss: 0.23774035274982452, train_loss: 0.0180065855383873\n",
            "29690 val_loss: 0.2565355896949768, train_loss: 0.018206005915999413\n",
            "29700 val_loss: 0.226253941655159, train_loss: 0.018989086151123047\n",
            "29710 val_loss: 0.22843746840953827, train_loss: 0.0180214811116457\n",
            "29720 val_loss: 0.21725068986415863, train_loss: 0.01970476470887661\n",
            "29730 val_loss: 0.21913966536521912, train_loss: 0.018814148381352425\n",
            "29740 val_loss: 0.2203586846590042, train_loss: 0.018338728696107864\n",
            "29750 val_loss: 0.22425894439220428, train_loss: 0.018734361976385117\n",
            "29760 val_loss: 0.23006340861320496, train_loss: 0.01786206103861332\n",
            "29770 val_loss: 0.21770504117012024, train_loss: 0.018010972067713737\n",
            "29780 val_loss: 0.233979731798172, train_loss: 0.01792209967970848\n",
            "29790 val_loss: 0.2188061773777008, train_loss: 0.018349213525652885\n",
            "29800 val_loss: 0.21263262629508972, train_loss: 0.018395893275737762\n",
            "29810 val_loss: 0.20289841294288635, train_loss: 0.018465138971805573\n",
            "29820 val_loss: 0.21524810791015625, train_loss: 0.019047625362873077\n",
            "29830 val_loss: 0.2462913691997528, train_loss: 0.01776120997965336\n",
            "29840 val_loss: 0.2268882691860199, train_loss: 0.017833620309829712\n",
            "29850 val_loss: 0.20941969752311707, train_loss: 0.0183166041970253\n",
            "29860 val_loss: 0.23391221463680267, train_loss: 0.01792917586863041\n",
            "29870 val_loss: 0.21709568798542023, train_loss: 0.018529679626226425\n",
            "29880 val_loss: 0.21832630038261414, train_loss: 0.017698680981993675\n",
            "29890 val_loss: 0.22327527403831482, train_loss: 0.017394768074154854\n",
            "29900 val_loss: 0.23799346387386322, train_loss: 0.01739824377000332\n",
            "29910 val_loss: 0.24724432826042175, train_loss: 0.025393597781658173\n",
            "29920 val_loss: 0.2425474226474762, train_loss: 0.01734316535294056\n",
            "29930 val_loss: 0.20541012287139893, train_loss: 0.01928720995783806\n",
            "29940 val_loss: 0.2492952048778534, train_loss: 0.017302315682172775\n",
            "29950 val_loss: 0.24749056994915009, train_loss: 0.0172526016831398\n",
            "29960 val_loss: 0.23709408938884735, train_loss: 0.01722756214439869\n",
            "29970 val_loss: 0.22756870090961456, train_loss: 0.0172249898314476\n",
            "29980 val_loss: 0.2180643528699875, train_loss: 0.0173462126404047\n",
            "29990 val_loss: 0.23745189607143402, train_loss: 0.01709122397005558\n",
            "30000 val_loss: 0.2441989630460739, train_loss: 0.01720641739666462\n",
            "30010 val_loss: 0.2249862104654312, train_loss: 0.01900850422680378\n",
            "30020 val_loss: 0.22623975574970245, train_loss: 0.017345992848277092\n",
            "30030 val_loss: 0.24380992352962494, train_loss: 0.017271479591727257\n",
            "30040 val_loss: 0.22731997072696686, train_loss: 0.017753519117832184\n",
            "30050 val_loss: 0.23839439451694489, train_loss: 0.01702967658638954\n",
            "30060 val_loss: 0.22610409557819366, train_loss: 0.01716780848801136\n",
            "30070 val_loss: 0.2270517796278, train_loss: 0.017716657370328903\n",
            "30080 val_loss: 0.2643412947654724, train_loss: 0.017420543357729912\n",
            "30090 val_loss: 0.24450935423374176, train_loss: 0.017004843801259995\n",
            "30100 val_loss: 0.2408360093832016, train_loss: 0.017027512192726135\n",
            "30110 val_loss: 0.24977819621562958, train_loss: 0.017579838633537292\n",
            "30120 val_loss: 0.30006757378578186, train_loss: 0.02421087957918644\n",
            "30130 val_loss: 0.24708282947540283, train_loss: 0.01728895865380764\n",
            "30140 val_loss: 0.26862236857414246, train_loss: 0.017811499536037445\n",
            "30150 val_loss: 0.22734464704990387, train_loss: 0.017030881717801094\n",
            "30160 val_loss: 0.265193372964859, train_loss: 0.016714021563529968\n",
            "30170 val_loss: 0.2437269538640976, train_loss: 0.016732702031731606\n",
            "30180 val_loss: 0.23869866132736206, train_loss: 0.0166780948638916\n",
            "30190 val_loss: 0.23757441341876984, train_loss: 0.017159514129161835\n",
            "30200 val_loss: 0.2483869045972824, train_loss: 0.01710006780922413\n",
            "30210 val_loss: 0.24568049609661102, train_loss: 0.016554953530430794\n",
            "30220 val_loss: 0.243095263838768, train_loss: 0.01663581281900406\n",
            "30230 val_loss: 0.2407422512769699, train_loss: 0.01665329374372959\n",
            "30240 val_loss: 0.29464107751846313, train_loss: 0.026735376566648483\n",
            "30250 val_loss: 0.2643894851207733, train_loss: 0.017037078738212585\n",
            "30260 val_loss: 0.23193536698818207, train_loss: 0.016675593331456184\n",
            "30270 val_loss: 0.23551100492477417, train_loss: 0.01644262857735157\n",
            "30280 val_loss: 0.22754418849945068, train_loss: 0.01723889634013176\n",
            "30290 val_loss: 0.23919379711151123, train_loss: 0.016582338139414787\n",
            "30300 val_loss: 0.24678945541381836, train_loss: 0.01646992191672325\n",
            "30310 val_loss: 0.2459452748298645, train_loss: 0.016328830271959305\n",
            "30320 val_loss: 0.22509123384952545, train_loss: 0.016560357064008713\n",
            "30330 val_loss: 0.2305537462234497, train_loss: 0.01664738915860653\n",
            "30340 val_loss: 0.24079687893390656, train_loss: 0.01646779105067253\n",
            "30350 val_loss: 0.23971201479434967, train_loss: 0.016722774133086205\n",
            "30360 val_loss: 0.2513142228126526, train_loss: 0.01650504767894745\n",
            "30370 val_loss: 0.2554389536380768, train_loss: 0.016530225053429604\n",
            "30380 val_loss: 0.25194936990737915, train_loss: 0.016512148082256317\n",
            "30390 val_loss: 0.22265823185443878, train_loss: 0.01760733686387539\n",
            "30400 val_loss: 0.2310841828584671, train_loss: 0.016381917521357536\n",
            "30410 val_loss: 0.2402278631925583, train_loss: 0.01615617424249649\n",
            "30420 val_loss: 0.23229867219924927, train_loss: 0.01626177318394184\n",
            "30430 val_loss: 0.2588050365447998, train_loss: 0.017305510118603706\n",
            "30440 val_loss: 0.23467695713043213, train_loss: 0.0164793748408556\n",
            "30450 val_loss: 0.22297294437885284, train_loss: 0.016797007992863655\n",
            "30460 val_loss: 0.22172416746616364, train_loss: 0.01691843941807747\n",
            "30470 val_loss: 0.22529366612434387, train_loss: 0.016781479120254517\n",
            "30480 val_loss: 0.24026818573474884, train_loss: 0.016345448791980743\n",
            "30490 val_loss: 0.24320507049560547, train_loss: 0.016279537230730057\n",
            "30500 val_loss: 0.22256267070770264, train_loss: 0.016859978437423706\n",
            "30510 val_loss: 0.25475311279296875, train_loss: 0.016776029020547867\n",
            "30520 val_loss: 0.22837795317173004, train_loss: 0.016775080934166908\n",
            "30530 val_loss: 0.25326618552207947, train_loss: 0.017053889110684395\n",
            "30540 val_loss: 0.24787843227386475, train_loss: 0.016468148678541183\n",
            "30550 val_loss: 0.24270547926425934, train_loss: 0.016368335112929344\n",
            "30560 val_loss: 0.24285542964935303, train_loss: 0.016270574182271957\n",
            "30570 val_loss: 0.2614595890045166, train_loss: 0.016211627051234245\n",
            "30580 val_loss: 0.24072478711605072, train_loss: 0.016158249229192734\n",
            "30590 val_loss: 0.2404453605413437, train_loss: 0.01604585349559784\n",
            "30600 val_loss: 0.23629605770111084, train_loss: 0.016162527725100517\n",
            "30610 val_loss: 0.24088071286678314, train_loss: 0.016124660149216652\n",
            "30620 val_loss: 0.23735184967517853, train_loss: 0.016264941543340683\n",
            "30630 val_loss: 0.23335903882980347, train_loss: 0.016699137166142464\n",
            "30640 val_loss: 0.26340776681900024, train_loss: 0.017135323956608772\n",
            "30650 val_loss: 0.23863385617733002, train_loss: 0.016440454870462418\n",
            "30660 val_loss: 0.26087889075279236, train_loss: 0.016307653859257698\n",
            "30670 val_loss: 0.26926860213279724, train_loss: 0.01774001680314541\n",
            "30680 val_loss: 0.24422433972358704, train_loss: 0.01621359959244728\n",
            "30690 val_loss: 0.24398526549339294, train_loss: 0.01624137908220291\n",
            "30700 val_loss: 0.20592203736305237, train_loss: 0.017632436007261276\n",
            "30710 val_loss: 0.24923397600650787, train_loss: 0.01658511534333229\n",
            "30720 val_loss: 0.28889432549476624, train_loss: 0.01956131123006344\n",
            "30730 val_loss: 0.24169008433818817, train_loss: 0.016238315030932426\n",
            "30740 val_loss: 0.23329274356365204, train_loss: 0.0163996834307909\n",
            "30750 val_loss: 0.24582946300506592, train_loss: 0.016171934083104134\n",
            "30760 val_loss: 0.24880097806453705, train_loss: 0.016333984211087227\n",
            "30770 val_loss: 0.2527317702770233, train_loss: 0.016246668994426727\n",
            "30780 val_loss: 0.24830766022205353, train_loss: 0.015997303649783134\n",
            "30790 val_loss: 0.24448367953300476, train_loss: 0.01584315486252308\n",
            "30800 val_loss: 0.24631643295288086, train_loss: 0.015670225024223328\n",
            "30810 val_loss: 0.23701728880405426, train_loss: 0.017513412982225418\n",
            "30820 val_loss: 0.2697426378726959, train_loss: 0.01823519356548786\n",
            "30830 val_loss: 0.26052170991897583, train_loss: 0.016787802800536156\n",
            "30840 val_loss: 0.23192782700061798, train_loss: 0.015905821695923805\n",
            "30850 val_loss: 0.2338961660861969, train_loss: 0.01602267287671566\n",
            "30860 val_loss: 0.24788014590740204, train_loss: 0.01576368324458599\n",
            "30870 val_loss: 0.19059592485427856, train_loss: 0.018914170563220978\n",
            "30880 val_loss: 0.22042062878608704, train_loss: 0.016038808971643448\n",
            "30890 val_loss: 0.2430669665336609, train_loss: 0.015774160623550415\n",
            "30900 val_loss: 0.2415795773267746, train_loss: 0.01605808176100254\n",
            "30910 val_loss: 0.24988213181495667, train_loss: 0.01896592788398266\n",
            "30920 val_loss: 0.22355806827545166, train_loss: 0.015909772366285324\n",
            "30930 val_loss: 0.23957686126232147, train_loss: 0.015541620552539825\n",
            "30940 val_loss: 0.2238808125257492, train_loss: 0.01626932993531227\n",
            "30950 val_loss: 0.23732618987560272, train_loss: 0.015679998323321342\n",
            "30960 val_loss: 0.23479855060577393, train_loss: 0.015721028670668602\n",
            "30970 val_loss: 0.22157235443592072, train_loss: 0.016747133806347847\n",
            "30980 val_loss: 0.263550341129303, train_loss: 0.01558486558496952\n",
            "30990 val_loss: 0.23469647765159607, train_loss: 0.01635807380080223\n",
            "31000 val_loss: 0.2559351325035095, train_loss: 0.015470811165869236\n",
            "31010 val_loss: 0.2532956302165985, train_loss: 0.015311058610677719\n",
            "31020 val_loss: 0.22417563199996948, train_loss: 0.016367308795452118\n",
            "31030 val_loss: 0.26744532585144043, train_loss: 0.016499964520335197\n",
            "31040 val_loss: 0.25406548380851746, train_loss: 0.015231331810355186\n",
            "31050 val_loss: 0.24082785844802856, train_loss: 0.015167693607509136\n",
            "31060 val_loss: 0.23093318939208984, train_loss: 0.015541465021669865\n",
            "31070 val_loss: 0.2652800381183624, train_loss: 0.015331577509641647\n",
            "31080 val_loss: 0.2454107105731964, train_loss: 0.01530551165342331\n",
            "31090 val_loss: 0.25292444229125977, train_loss: 0.01573919504880905\n",
            "31100 val_loss: 0.24939598143100739, train_loss: 0.016688531264662743\n",
            "31110 val_loss: 0.2450086921453476, train_loss: 0.015836721286177635\n",
            "31120 val_loss: 0.2691156268119812, train_loss: 0.015771768987178802\n",
            "31130 val_loss: 0.30578944087028503, train_loss: 0.021920587867498398\n",
            "31140 val_loss: 0.22417134046554565, train_loss: 0.016549834981560707\n",
            "31150 val_loss: 0.24153448641300201, train_loss: 0.016494739800691605\n",
            "31160 val_loss: 0.23327195644378662, train_loss: 0.016200218349695206\n",
            "31170 val_loss: 0.25955063104629517, train_loss: 0.01573380082845688\n",
            "31180 val_loss: 0.22919698059558868, train_loss: 0.01581018790602684\n",
            "31190 val_loss: 0.24102404713630676, train_loss: 0.0157120693475008\n",
            "31200 val_loss: 0.23415477573871613, train_loss: 0.016807006672024727\n",
            "31210 val_loss: 0.23145239055156708, train_loss: 0.01599704474210739\n",
            "31220 val_loss: 0.2569735050201416, train_loss: 0.015651416033506393\n",
            "31230 val_loss: 0.24860091507434845, train_loss: 0.01544851902872324\n",
            "31240 val_loss: 0.2621103525161743, train_loss: 0.015224294736981392\n",
            "31250 val_loss: 0.3018695116043091, train_loss: 0.021196840330958366\n",
            "31260 val_loss: 0.26782503724098206, train_loss: 0.016017071902751923\n",
            "31270 val_loss: 0.2389144003391266, train_loss: 0.015380765311419964\n",
            "31280 val_loss: 0.23428791761398315, train_loss: 0.015423305332660675\n",
            "31290 val_loss: 0.2540367543697357, train_loss: 0.015353735536336899\n",
            "31300 val_loss: 0.23589779436588287, train_loss: 0.015373513102531433\n",
            "31310 val_loss: 0.23734715580940247, train_loss: 0.01530403271317482\n",
            "31320 val_loss: 0.2679402232170105, train_loss: 0.01586013473570347\n",
            "31330 val_loss: 0.2540907561779022, train_loss: 0.015182852745056152\n",
            "31340 val_loss: 0.2743479311466217, train_loss: 0.016071757301688194\n",
            "31350 val_loss: 0.22981154918670654, train_loss: 0.015295879915356636\n",
            "31360 val_loss: 0.258057177066803, train_loss: 0.014935695566236973\n",
            "31370 val_loss: 0.2344616949558258, train_loss: 0.015338309109210968\n",
            "31380 val_loss: 0.22446148097515106, train_loss: 0.015505628660321236\n",
            "31390 val_loss: 0.22123877704143524, train_loss: 0.01604037545621395\n",
            "31400 val_loss: 0.2576315402984619, train_loss: 0.015211917459964752\n",
            "31410 val_loss: 0.2595612108707428, train_loss: 0.015040929429233074\n",
            "31420 val_loss: 0.29752469062805176, train_loss: 0.019111208617687225\n",
            "31430 val_loss: 0.2622534930706024, train_loss: 0.015053054317831993\n",
            "31440 val_loss: 0.2654449939727783, train_loss: 0.014938288368284702\n",
            "31450 val_loss: 0.2707459330558777, train_loss: 0.015627101063728333\n",
            "31460 val_loss: 0.22333674132823944, train_loss: 0.01655801758170128\n",
            "31470 val_loss: 0.22523346543312073, train_loss: 0.015653081238269806\n",
            "31480 val_loss: 0.27378252148628235, train_loss: 0.015701087191700935\n",
            "31490 val_loss: 0.2574516832828522, train_loss: 0.014993632212281227\n",
            "31500 val_loss: 0.26905664801597595, train_loss: 0.01502569206058979\n",
            "31510 val_loss: 0.2669807970523834, train_loss: 0.01487746275961399\n",
            "31520 val_loss: 0.26250186562538147, train_loss: 0.015096052549779415\n",
            "31530 val_loss: 0.2504271864891052, train_loss: 0.015390590764582157\n",
            "31540 val_loss: 0.23230959475040436, train_loss: 0.015848232433199883\n",
            "31550 val_loss: 0.2546253800392151, train_loss: 0.015056523494422436\n",
            "31560 val_loss: 0.24838551878929138, train_loss: 0.014939483255147934\n",
            "31570 val_loss: 0.2671670615673065, train_loss: 0.014952068217098713\n",
            "31580 val_loss: 0.24693363904953003, train_loss: 0.01562440861016512\n",
            "31590 val_loss: 0.27650144696235657, train_loss: 0.015338044613599777\n",
            "31600 val_loss: 0.2534830570220947, train_loss: 0.014712799340486526\n",
            "31610 val_loss: 0.2478228509426117, train_loss: 0.01475735567510128\n",
            "31620 val_loss: 0.2736751139163971, train_loss: 0.014663150534033775\n",
            "31630 val_loss: 0.24221661686897278, train_loss: 0.014983991160988808\n",
            "31640 val_loss: 0.25030672550201416, train_loss: 0.014920513145625591\n",
            "31650 val_loss: 0.250352144241333, train_loss: 0.014868403784930706\n",
            "31660 val_loss: 0.2641362249851227, train_loss: 0.01490285899490118\n",
            "31670 val_loss: 0.26107287406921387, train_loss: 0.014923418872058392\n",
            "31680 val_loss: 0.26428115367889404, train_loss: 0.01509945373982191\n",
            "31690 val_loss: 0.23585207760334015, train_loss: 0.015475526452064514\n",
            "31700 val_loss: 0.23947511613368988, train_loss: 0.014948543161153793\n",
            "31710 val_loss: 0.23797836899757385, train_loss: 0.015115593560039997\n",
            "31720 val_loss: 0.29660719633102417, train_loss: 0.016015488654375076\n",
            "31730 val_loss: 0.2795065641403198, train_loss: 0.01472939271479845\n",
            "31740 val_loss: 0.2540755569934845, train_loss: 0.01599165052175522\n",
            "31750 val_loss: 0.23876801133155823, train_loss: 0.015116730704903603\n",
            "31760 val_loss: 0.26177623867988586, train_loss: 0.015023883432149887\n",
            "31770 val_loss: 0.24253228306770325, train_loss: 0.01487476285547018\n",
            "31780 val_loss: 0.24882328510284424, train_loss: 0.014675385318696499\n",
            "31790 val_loss: 0.2422959953546524, train_loss: 0.014905945397913456\n",
            "31800 val_loss: 0.24693189561367035, train_loss: 0.014710279181599617\n",
            "31810 val_loss: 0.2514128088951111, train_loss: 0.015658212825655937\n",
            "31820 val_loss: 0.2561323046684265, train_loss: 0.01458886917680502\n",
            "31830 val_loss: 0.2426268458366394, train_loss: 0.014783671125769615\n",
            "31840 val_loss: 0.17153164744377136, train_loss: 0.021997420117259026\n",
            "31850 val_loss: 0.23795750737190247, train_loss: 0.01488578226417303\n",
            "31860 val_loss: 0.23416739702224731, train_loss: 0.014858531765639782\n",
            "31870 val_loss: 0.2544032335281372, train_loss: 0.014654377475380898\n",
            "31880 val_loss: 0.2683078348636627, train_loss: 0.015006463043391705\n",
            "31890 val_loss: 0.2531706392765045, train_loss: 0.014735105447471142\n",
            "31900 val_loss: 0.22886225581169128, train_loss: 0.015109446831047535\n",
            "31910 val_loss: 0.2478274405002594, train_loss: 0.014419777318835258\n",
            "31920 val_loss: 0.2444869726896286, train_loss: 0.014585228636860847\n",
            "31930 val_loss: 0.27136674523353577, train_loss: 0.014389172196388245\n",
            "31940 val_loss: 0.25528496503829956, train_loss: 0.014298692345619202\n",
            "31950 val_loss: 0.2606852054595947, train_loss: 0.01422867737710476\n",
            "31960 val_loss: 0.26472803950309753, train_loss: 0.014450747519731522\n",
            "31970 val_loss: 0.2451278567314148, train_loss: 0.014089907519519329\n",
            "31980 val_loss: 0.244399756193161, train_loss: 0.014209173619747162\n",
            "31990 val_loss: 0.2509356141090393, train_loss: 0.014101671054959297\n",
            "32000 val_loss: 0.27126577496528625, train_loss: 0.014197546988725662\n",
            "32010 val_loss: 0.272009015083313, train_loss: 0.01395691279321909\n",
            "32020 val_loss: 0.2285485565662384, train_loss: 0.014475922100245953\n",
            "32030 val_loss: 0.22203974425792694, train_loss: 0.01698717661201954\n",
            "32040 val_loss: 0.2467917501926422, train_loss: 0.014088142663240433\n",
            "32050 val_loss: 0.24659974873065948, train_loss: 0.014245080761611462\n",
            "32060 val_loss: 0.22605469822883606, train_loss: 0.015134075656533241\n",
            "32070 val_loss: 0.23487313091754913, train_loss: 0.014714478515088558\n",
            "32080 val_loss: 0.2819191813468933, train_loss: 0.014157778583467007\n",
            "32090 val_loss: 0.25704216957092285, train_loss: 0.013970800675451756\n",
            "32100 val_loss: 0.23258234560489655, train_loss: 0.014782564714550972\n",
            "32110 val_loss: 0.27897635102272034, train_loss: 0.015156328678131104\n",
            "32120 val_loss: 0.2493862807750702, train_loss: 0.014115718193352222\n",
            "32130 val_loss: 0.2591302692890167, train_loss: 0.014293872751295567\n",
            "32140 val_loss: 0.25085246562957764, train_loss: 0.014249099418520927\n",
            "32150 val_loss: 0.25597530603408813, train_loss: 0.014122708700597286\n",
            "32160 val_loss: 0.25860682129859924, train_loss: 0.014089185744524002\n",
            "32170 val_loss: 0.25101780891418457, train_loss: 0.014519898220896721\n",
            "32180 val_loss: 0.25966840982437134, train_loss: 0.01661161333322525\n",
            "32190 val_loss: 0.269111692905426, train_loss: 0.0144045976921916\n",
            "32200 val_loss: 0.22775714099407196, train_loss: 0.014801359735429287\n",
            "32210 val_loss: 0.2494174689054489, train_loss: 0.014344342984259129\n",
            "32220 val_loss: 0.24828681349754333, train_loss: 0.014030278660356998\n",
            "32230 val_loss: 0.2752334773540497, train_loss: 0.014490319415926933\n",
            "32240 val_loss: 0.25809338688850403, train_loss: 0.014017309062182903\n",
            "32250 val_loss: 0.243088498711586, train_loss: 0.01418360136449337\n",
            "32260 val_loss: 0.25768551230430603, train_loss: 0.014143281616270542\n",
            "32270 val_loss: 0.25331979990005493, train_loss: 0.014069891534745693\n",
            "32280 val_loss: 0.2887689471244812, train_loss: 0.016107864677906036\n",
            "32290 val_loss: 0.24952414631843567, train_loss: 0.014042372815310955\n",
            "32300 val_loss: 0.25396502017974854, train_loss: 0.014048716053366661\n",
            "32310 val_loss: 0.25915512442588806, train_loss: 0.013938692398369312\n",
            "32320 val_loss: 0.24435193836688995, train_loss: 0.017390690743923187\n",
            "32330 val_loss: 0.26109156012535095, train_loss: 0.013728510588407516\n",
            "32340 val_loss: 0.25150126218795776, train_loss: 0.013678141869604588\n",
            "32350 val_loss: 0.2663765847682953, train_loss: 0.013559374026954174\n",
            "32360 val_loss: 0.25019797682762146, train_loss: 0.014008605852723122\n",
            "32370 val_loss: 0.24228008091449738, train_loss: 0.014227129518985748\n",
            "32380 val_loss: 0.2569084167480469, train_loss: 0.01342686451971531\n",
            "32390 val_loss: 0.2545463442802429, train_loss: 0.013241020031273365\n",
            "32400 val_loss: 0.2519388794898987, train_loss: 0.013342117890715599\n",
            "32410 val_loss: 0.25120508670806885, train_loss: 0.013217255473136902\n",
            "32420 val_loss: 0.24857814610004425, train_loss: 0.013128876686096191\n",
            "32430 val_loss: 0.22971966862678528, train_loss: 0.013874126598238945\n",
            "32440 val_loss: 0.24209880828857422, train_loss: 0.013064154423773289\n",
            "32450 val_loss: 0.2827565371990204, train_loss: 0.014477767050266266\n",
            "32460 val_loss: 0.24721089005470276, train_loss: 0.012765159830451012\n",
            "32470 val_loss: 0.2693200409412384, train_loss: 0.012811141088604927\n",
            "32480 val_loss: 0.26148098707199097, train_loss: 0.01293383538722992\n",
            "32490 val_loss: 0.24055741727352142, train_loss: 0.01272925827652216\n",
            "32500 val_loss: 0.24299655854701996, train_loss: 0.012896045111119747\n",
            "32510 val_loss: 0.23334309458732605, train_loss: 0.012878979556262493\n",
            "32520 val_loss: 0.26747265458106995, train_loss: 0.013324519619345665\n",
            "32530 val_loss: 0.2678689658641815, train_loss: 0.012734645046293736\n",
            "32540 val_loss: 0.24686171114444733, train_loss: 0.014096450991928577\n",
            "32550 val_loss: 0.25667500495910645, train_loss: 0.01262248307466507\n",
            "32560 val_loss: 0.23189708590507507, train_loss: 0.012681592255830765\n",
            "32570 val_loss: 0.25027400255203247, train_loss: 0.01244886964559555\n",
            "32580 val_loss: 0.21270258724689484, train_loss: 0.014306532219052315\n",
            "32590 val_loss: 0.2593875527381897, train_loss: 0.0129098417237401\n",
            "32600 val_loss: 0.3304951786994934, train_loss: 0.02002330869436264\n",
            "32610 val_loss: 0.2323944866657257, train_loss: 0.013385681435465813\n",
            "32620 val_loss: 0.24501632153987885, train_loss: 0.012877360917627811\n",
            "32630 val_loss: 0.2840288579463959, train_loss: 0.013948328793048859\n",
            "32640 val_loss: 0.2569383382797241, train_loss: 0.012925906106829643\n",
            "32650 val_loss: 0.2536177635192871, train_loss: 0.01258358545601368\n",
            "32660 val_loss: 0.24759122729301453, train_loss: 0.01281300000846386\n",
            "32670 val_loss: 0.26697543263435364, train_loss: 0.01572255976498127\n",
            "32680 val_loss: 0.2832035720348358, train_loss: 0.012885061092674732\n",
            "32690 val_loss: 0.24475882947444916, train_loss: 0.012481339275836945\n",
            "32700 val_loss: 0.27329036593437195, train_loss: 0.013685317710042\n",
            "32710 val_loss: 0.27545255422592163, train_loss: 0.013595875352621078\n",
            "32720 val_loss: 0.23297198116779327, train_loss: 0.015194720588624477\n",
            "32730 val_loss: 0.24772599339485168, train_loss: 0.01223397720605135\n",
            "32740 val_loss: 0.24856966733932495, train_loss: 0.012247307226061821\n",
            "32750 val_loss: 0.2570076882839203, train_loss: 0.012155194766819477\n",
            "32760 val_loss: 0.257307767868042, train_loss: 0.012119761668145657\n",
            "32770 val_loss: 0.24467343091964722, train_loss: 0.012346883304417133\n",
            "32780 val_loss: 0.2372395247220993, train_loss: 0.01263824850320816\n",
            "32790 val_loss: 0.2735481858253479, train_loss: 0.01359602715820074\n",
            "32800 val_loss: 0.2586536705493927, train_loss: 0.012636334635317326\n",
            "32810 val_loss: 0.23246844112873077, train_loss: 0.01263256836682558\n",
            "32820 val_loss: 0.25896501541137695, train_loss: 0.012760508805513382\n",
            "32830 val_loss: 0.26141858100891113, train_loss: 0.012709497474133968\n",
            "32840 val_loss: 0.24233046174049377, train_loss: 0.012113320641219616\n",
            "32850 val_loss: 0.23575744032859802, train_loss: 0.012098092585802078\n",
            "32860 val_loss: 0.23653143644332886, train_loss: 0.012404581531882286\n",
            "32870 val_loss: 0.24379289150238037, train_loss: 0.01193021610379219\n",
            "32880 val_loss: 0.2656187415122986, train_loss: 0.012142237275838852\n",
            "32890 val_loss: 0.24102966487407684, train_loss: 0.011928458698093891\n",
            "32900 val_loss: 0.2745426595211029, train_loss: 0.012697966769337654\n",
            "32910 val_loss: 0.25012779235839844, train_loss: 0.011769725009799004\n",
            "32920 val_loss: 0.2871638238430023, train_loss: 0.013345149345695972\n",
            "32930 val_loss: 0.25142690539360046, train_loss: 0.011695449240505695\n",
            "32940 val_loss: 0.23574554920196533, train_loss: 0.011959126219153404\n",
            "32950 val_loss: 0.23117217421531677, train_loss: 0.013193663209676743\n",
            "32960 val_loss: 0.24482037127017975, train_loss: 0.011679044924676418\n",
            "32970 val_loss: 0.24972525238990784, train_loss: 0.011777566745877266\n",
            "32980 val_loss: 0.24331015348434448, train_loss: 0.011711305938661098\n",
            "32990 val_loss: 0.2646561563014984, train_loss: 0.011674586683511734\n",
            "33000 val_loss: 0.24683775007724762, train_loss: 0.011374379508197308\n",
            "33010 val_loss: 0.22869643568992615, train_loss: 0.011998208239674568\n",
            "33020 val_loss: 0.23809698224067688, train_loss: 0.012436981312930584\n",
            "33030 val_loss: 0.25862812995910645, train_loss: 0.011490230448544025\n",
            "33040 val_loss: 0.2576937973499298, train_loss: 0.01191163994371891\n",
            "33050 val_loss: 0.2491045892238617, train_loss: 0.011845112778246403\n",
            "33060 val_loss: 0.24840997159481049, train_loss: 0.011640259064733982\n",
            "33070 val_loss: 0.25139319896698, train_loss: 0.011399713344871998\n",
            "33080 val_loss: 0.25940823554992676, train_loss: 0.011241714470088482\n",
            "33090 val_loss: 0.25251489877700806, train_loss: 0.011590881273150444\n",
            "33100 val_loss: 0.23815873265266418, train_loss: 0.01144080888479948\n",
            "33110 val_loss: 0.24888364970684052, train_loss: 0.011215373873710632\n",
            "33120 val_loss: 0.2557864189147949, train_loss: 0.011706148274242878\n",
            "33130 val_loss: 0.2566893398761749, train_loss: 0.011380327865481377\n",
            "33140 val_loss: 0.26425525546073914, train_loss: 0.011290079914033413\n",
            "33150 val_loss: 0.25506484508514404, train_loss: 0.011328734457492828\n",
            "33160 val_loss: 0.24565576016902924, train_loss: 0.01228116825222969\n",
            "33170 val_loss: 0.26022952795028687, train_loss: 0.01144302636384964\n",
            "33180 val_loss: 0.27283966541290283, train_loss: 0.011808824725449085\n",
            "33190 val_loss: 0.2880334258079529, train_loss: 0.011472969315946102\n",
            "33200 val_loss: 0.2665204405784607, train_loss: 0.011928227730095387\n",
            "33210 val_loss: 0.22766560316085815, train_loss: 0.012970766052603722\n",
            "33220 val_loss: 0.23990869522094727, train_loss: 0.011483890935778618\n",
            "33230 val_loss: 0.24915282428264618, train_loss: 0.011101409792900085\n",
            "33240 val_loss: 0.2701580822467804, train_loss: 0.011860914528369904\n",
            "33250 val_loss: 0.24375803768634796, train_loss: 0.01173659972846508\n",
            "33260 val_loss: 0.26116156578063965, train_loss: 0.011895950883626938\n",
            "33270 val_loss: 0.2383939027786255, train_loss: 0.01203194446861744\n",
            "33280 val_loss: 0.2614037096500397, train_loss: 0.011512266471982002\n",
            "33290 val_loss: 0.282332181930542, train_loss: 0.01106599997729063\n",
            "33300 val_loss: 0.239556223154068, train_loss: 0.011542734690010548\n",
            "33310 val_loss: 0.2563178539276123, train_loss: 0.012098835781216621\n",
            "33320 val_loss: 0.2649741768836975, train_loss: 0.011164350435137749\n",
            "33330 val_loss: 0.29234176874160767, train_loss: 0.012568602338433266\n",
            "33340 val_loss: 0.2537519335746765, train_loss: 0.011186845600605011\n",
            "33350 val_loss: 0.2731146216392517, train_loss: 0.01096717081964016\n",
            "33360 val_loss: 0.30943915247917175, train_loss: 0.012801006436347961\n",
            "33370 val_loss: 0.24766583740711212, train_loss: 0.011318800039589405\n",
            "33380 val_loss: 0.2643575072288513, train_loss: 0.012744930572807789\n",
            "33390 val_loss: 0.2578267455101013, train_loss: 0.011554115451872349\n",
            "33400 val_loss: 0.24345159530639648, train_loss: 0.011000466533005238\n",
            "33410 val_loss: 0.2804122865200043, train_loss: 0.011286027729511261\n",
            "33420 val_loss: 0.2618076503276825, train_loss: 0.011701913550496101\n",
            "33430 val_loss: 0.24410222470760345, train_loss: 0.01060883142054081\n",
            "33440 val_loss: 0.3050786852836609, train_loss: 0.010748653672635555\n",
            "33450 val_loss: 0.2611621618270874, train_loss: 0.010217699222266674\n",
            "33460 val_loss: 0.25537827610969543, train_loss: 0.010192416608333588\n",
            "33470 val_loss: 0.23212750256061554, train_loss: 0.010347877629101276\n",
            "33480 val_loss: 0.24515515565872192, train_loss: 0.009873810224235058\n",
            "33490 val_loss: 0.25635722279548645, train_loss: 0.009899691678583622\n",
            "33500 val_loss: 0.24800807237625122, train_loss: 0.0097091319039464\n",
            "33510 val_loss: 0.258698970079422, train_loss: 0.01008789986371994\n",
            "33520 val_loss: 0.24015626311302185, train_loss: 0.009742915630340576\n",
            "33530 val_loss: 0.269686758518219, train_loss: 0.010615365579724312\n",
            "33540 val_loss: 0.23323093354701996, train_loss: 0.009713825769722462\n",
            "33550 val_loss: 0.27575546503067017, train_loss: 0.009328745305538177\n",
            "33560 val_loss: 0.24934187531471252, train_loss: 0.009848262183368206\n",
            "33570 val_loss: 0.2543579638004303, train_loss: 0.0093269357457757\n",
            "33580 val_loss: 0.2616921067237854, train_loss: 0.009886411018669605\n",
            "33590 val_loss: 0.24951714277267456, train_loss: 0.009658032096922398\n",
            "33600 val_loss: 0.2559433877468109, train_loss: 0.009745215065777302\n",
            "33610 val_loss: 0.24588683247566223, train_loss: 0.009859765879809856\n",
            "33620 val_loss: 0.2532450258731842, train_loss: 0.009361709468066692\n",
            "33630 val_loss: 0.24917936325073242, train_loss: 0.008957847952842712\n",
            "33640 val_loss: 0.2592700719833374, train_loss: 0.009154180064797401\n",
            "33650 val_loss: 0.24248898029327393, train_loss: 0.009191353805363178\n",
            "33660 val_loss: 0.2621347904205322, train_loss: 0.00877427589148283\n",
            "33670 val_loss: 0.2599823474884033, train_loss: 0.008758116513490677\n",
            "33680 val_loss: 0.25777068734169006, train_loss: 0.008541722781956196\n",
            "33690 val_loss: 0.2560529112815857, train_loss: 0.008400439284741879\n",
            "33700 val_loss: 0.2525912821292877, train_loss: 0.00833432748913765\n",
            "33710 val_loss: 0.2854340076446533, train_loss: 0.009082824923098087\n",
            "33720 val_loss: 0.2740659713745117, train_loss: 0.009053870104253292\n",
            "33730 val_loss: 0.28039202094078064, train_loss: 0.008600115776062012\n",
            "33740 val_loss: 0.27391284704208374, train_loss: 0.008865113370120525\n",
            "33750 val_loss: 0.26345452666282654, train_loss: 0.008463999256491661\n",
            "33760 val_loss: 0.25827622413635254, train_loss: 0.008172172121703625\n",
            "33770 val_loss: 0.27315789461135864, train_loss: 0.008077858947217464\n",
            "33780 val_loss: 0.24100013077259064, train_loss: 0.008470154367387295\n",
            "33790 val_loss: 0.2507023811340332, train_loss: 0.00814423430711031\n",
            "33800 val_loss: 0.25812509655952454, train_loss: 0.008302239701151848\n",
            "33810 val_loss: 0.24153341352939606, train_loss: 0.008347313851118088\n",
            "33820 val_loss: 0.25243476033210754, train_loss: 0.00839561689645052\n",
            "33830 val_loss: 0.2516729533672333, train_loss: 0.008399699814617634\n",
            "33840 val_loss: 0.2614358365535736, train_loss: 0.00823039561510086\n",
            "33850 val_loss: 0.2496182769536972, train_loss: 0.008541330695152283\n",
            "33860 val_loss: 0.24256138503551483, train_loss: 0.008465450257062912\n",
            "33870 val_loss: 0.2368636131286621, train_loss: 0.008578560315072536\n",
            "33880 val_loss: 0.2601502537727356, train_loss: 0.008720421232283115\n",
            "33890 val_loss: 0.25365474820137024, train_loss: 0.008487124927341938\n",
            "33900 val_loss: 0.2606530487537384, train_loss: 0.008270765654742718\n",
            "33910 val_loss: 0.3055192232131958, train_loss: 0.008956224657595158\n",
            "33920 val_loss: 0.25972819328308105, train_loss: 0.008238807320594788\n",
            "33930 val_loss: 0.2850295305252075, train_loss: 0.009290026500821114\n",
            "33940 val_loss: 0.26749539375305176, train_loss: 0.008147160522639751\n",
            "33950 val_loss: 0.2931610643863678, train_loss: 0.009758273139595985\n",
            "33960 val_loss: 0.2869337797164917, train_loss: 0.008871047757565975\n",
            "33970 val_loss: 0.2644217908382416, train_loss: 0.008209694176912308\n",
            "33980 val_loss: 0.2569565176963806, train_loss: 0.007607899606227875\n",
            "33990 val_loss: 0.258069783449173, train_loss: 0.007889865897595882\n",
            "34000 val_loss: 0.2538055181503296, train_loss: 0.007628397550433874\n",
            "34010 val_loss: 0.25922074913978577, train_loss: 0.00773101719096303\n",
            "34020 val_loss: 0.2622435688972473, train_loss: 0.007839880883693695\n",
            "34030 val_loss: 0.2767086923122406, train_loss: 0.007761917542666197\n",
            "34040 val_loss: 0.2623431980609894, train_loss: 0.0075927539728581905\n",
            "34050 val_loss: 0.27462899684906006, train_loss: 0.008435916155576706\n",
            "34060 val_loss: 0.26563015580177307, train_loss: 0.007887010462582111\n",
            "34070 val_loss: 0.2607369124889374, train_loss: 0.0075302873738110065\n",
            "34080 val_loss: 0.25031206011772156, train_loss: 0.008546154946088791\n",
            "34090 val_loss: 0.277836412191391, train_loss: 0.00820233579725027\n",
            "34100 val_loss: 0.2635062634944916, train_loss: 0.007913244888186455\n",
            "34110 val_loss: 0.26666584610939026, train_loss: 0.007620579097419977\n",
            "34120 val_loss: 0.2372199296951294, train_loss: 0.007876367308199406\n",
            "34130 val_loss: 0.24299739301204681, train_loss: 0.0076817674562335014\n",
            "34140 val_loss: 0.2776244580745697, train_loss: 0.008126743137836456\n",
            "34150 val_loss: 0.25387948751449585, train_loss: 0.007874689996242523\n",
            "34160 val_loss: 0.2487415373325348, train_loss: 0.00779299670830369\n",
            "34170 val_loss: 0.2558356821537018, train_loss: 0.007824081927537918\n",
            "34180 val_loss: 0.2835184335708618, train_loss: 0.007599714677780867\n",
            "34190 val_loss: 0.2730681598186493, train_loss: 0.008039992302656174\n",
            "34200 val_loss: 0.2971567213535309, train_loss: 0.007733128033578396\n",
            "34210 val_loss: 0.2628908157348633, train_loss: 0.007649320177733898\n",
            "34220 val_loss: 0.271932452917099, train_loss: 0.008182413876056671\n",
            "34230 val_loss: 0.2841617166996002, train_loss: 0.007456389721482992\n",
            "34240 val_loss: 0.2946056127548218, train_loss: 0.007679267320781946\n",
            "34250 val_loss: 0.2511110305786133, train_loss: 0.007329791318625212\n",
            "34260 val_loss: 0.24500544369220734, train_loss: 0.007707004435360432\n",
            "34270 val_loss: 0.2516091465950012, train_loss: 0.007378098089247942\n",
            "34280 val_loss: 0.2507590055465698, train_loss: 0.0074696228839457035\n",
            "34290 val_loss: 0.2748139798641205, train_loss: 0.007878812029957771\n",
            "34300 val_loss: 0.3022903501987457, train_loss: 0.009459352120757103\n",
            "34310 val_loss: 0.26121312379837036, train_loss: 0.0072301528416574\n",
            "34320 val_loss: 0.26143795251846313, train_loss: 0.007193460129201412\n",
            "34330 val_loss: 0.2713232934474945, train_loss: 0.007204681169241667\n",
            "34340 val_loss: 0.27079540491104126, train_loss: 0.0069027189165353775\n",
            "34350 val_loss: 0.26787132024765015, train_loss: 0.007044373080134392\n",
            "34360 val_loss: 0.2634923756122589, train_loss: 0.0070497156120836735\n",
            "34370 val_loss: 0.2643340826034546, train_loss: 0.0069196526892483234\n",
            "34380 val_loss: 0.2372298687696457, train_loss: 0.007613384164869785\n",
            "34390 val_loss: 0.2515611946582794, train_loss: 0.0070997728034853935\n",
            "34400 val_loss: 0.24186693131923676, train_loss: 0.007482430897653103\n",
            "34410 val_loss: 0.2817208170890808, train_loss: 0.007652364205569029\n",
            "34420 val_loss: 0.2535041570663452, train_loss: 0.007130501326173544\n",
            "34430 val_loss: 0.25848835706710815, train_loss: 0.007316771894693375\n",
            "34440 val_loss: 0.2589646279811859, train_loss: 0.0071686566807329655\n",
            "34450 val_loss: 0.28027716279029846, train_loss: 0.007475988008081913\n",
            "34460 val_loss: 0.2700994908809662, train_loss: 0.007343373727053404\n",
            "34470 val_loss: 0.26090207695961, train_loss: 0.0071538034826517105\n",
            "34480 val_loss: 0.3460097908973694, train_loss: 0.013669105246663094\n",
            "34490 val_loss: 0.26260823011398315, train_loss: 0.007332070730626583\n",
            "34500 val_loss: 0.2782413363456726, train_loss: 0.007785568945109844\n",
            "34510 val_loss: 0.2652055323123932, train_loss: 0.007087239995598793\n",
            "34520 val_loss: 0.27598074078559875, train_loss: 0.007090813014656305\n",
            "34530 val_loss: 0.36741727590560913, train_loss: 0.01702994853258133\n",
            "34540 val_loss: 0.27753007411956787, train_loss: 0.007429778575897217\n",
            "34550 val_loss: 0.2848421335220337, train_loss: 0.007292591966688633\n",
            "34560 val_loss: 0.25864002108573914, train_loss: 0.007117153611034155\n",
            "34570 val_loss: 0.29035407304763794, train_loss: 0.00790512003004551\n",
            "34580 val_loss: 0.2532403767108917, train_loss: 0.007191181182861328\n",
            "34590 val_loss: 0.27563220262527466, train_loss: 0.007406852673739195\n",
            "34600 val_loss: 0.30810439586639404, train_loss: 0.007387503050267696\n",
            "34610 val_loss: 0.2687586843967438, train_loss: 0.006857999134808779\n",
            "34620 val_loss: 0.27240920066833496, train_loss: 0.006814424879848957\n",
            "34630 val_loss: 0.2749796509742737, train_loss: 0.006913304328918457\n",
            "34640 val_loss: 0.27234411239624023, train_loss: 0.007281616795808077\n",
            "34650 val_loss: 0.2675211429595947, train_loss: 0.0070464955642819405\n",
            "34660 val_loss: 0.28582072257995605, train_loss: 0.007829941809177399\n",
            "34670 val_loss: 0.25537362694740295, train_loss: 0.006982950959354639\n",
            "34680 val_loss: 0.27144429087638855, train_loss: 0.00705992616713047\n",
            "34690 val_loss: 0.2945379912853241, train_loss: 0.007620251737535\n",
            "34700 val_loss: 0.2853612005710602, train_loss: 0.007786056958138943\n",
            "34710 val_loss: 0.2555006146430969, train_loss: 0.00711430236697197\n",
            "34720 val_loss: 0.264239102602005, train_loss: 0.007379213813692331\n",
            "34730 val_loss: 0.26048746705055237, train_loss: 0.007154863327741623\n",
            "34740 val_loss: 0.25870633125305176, train_loss: 0.007218342740088701\n",
            "34750 val_loss: 0.29476386308670044, train_loss: 0.00794914085417986\n",
            "34760 val_loss: 0.2796579897403717, train_loss: 0.00707503966987133\n",
            "34770 val_loss: 0.2913976311683655, train_loss: 0.007714816369116306\n",
            "34780 val_loss: 0.2647496461868286, train_loss: 0.0069288890808820724\n",
            "34790 val_loss: 0.23766863346099854, train_loss: 0.007704004179686308\n",
            "34800 val_loss: 0.28011852502822876, train_loss: 0.00727706728503108\n",
            "34810 val_loss: 0.29498735070228577, train_loss: 0.00710113812237978\n",
            "34820 val_loss: 0.2848129868507385, train_loss: 0.007042175158858299\n",
            "34830 val_loss: 0.27008047699928284, train_loss: 0.006837226450443268\n",
            "34840 val_loss: 0.26465654373168945, train_loss: 0.006970612797886133\n",
            "34850 val_loss: 0.279543399810791, train_loss: 0.007075394503772259\n",
            "34860 val_loss: 0.34305477142333984, train_loss: 0.009312452748417854\n",
            "34870 val_loss: 0.28960028290748596, train_loss: 0.006915607489645481\n",
            "34880 val_loss: 0.28504297137260437, train_loss: 0.00693875178694725\n",
            "34890 val_loss: 0.2663823068141937, train_loss: 0.007010436616837978\n",
            "34900 val_loss: 0.2873925566673279, train_loss: 0.007449917029589415\n",
            "34910 val_loss: 0.28689512610435486, train_loss: 0.007339127361774445\n",
            "34920 val_loss: 0.296687513589859, train_loss: 0.007811135146766901\n",
            "34930 val_loss: 0.25846028327941895, train_loss: 0.0070463973097503185\n",
            "34940 val_loss: 0.25437092781066895, train_loss: 0.007124100811779499\n",
            "34950 val_loss: 0.2986759841442108, train_loss: 0.006955788936465979\n",
            "34960 val_loss: 0.30321985483169556, train_loss: 0.008023934438824654\n",
            "34970 val_loss: 0.270807147026062, train_loss: 0.006647936999797821\n",
            "34980 val_loss: 0.2481519877910614, train_loss: 0.008178492076694965\n",
            "34990 val_loss: 0.2954632639884949, train_loss: 0.006498547736555338\n",
            "35000 val_loss: 0.2713356614112854, train_loss: 0.006518810521811247\n",
            "35010 val_loss: 0.2709292769432068, train_loss: 0.006870067212730646\n",
            "35020 val_loss: 0.2901870310306549, train_loss: 0.0071599348448216915\n",
            "35030 val_loss: 0.2605253756046295, train_loss: 0.006896370556205511\n",
            "35040 val_loss: 0.2574487626552582, train_loss: 0.006985451560467482\n",
            "35050 val_loss: 0.3160569667816162, train_loss: 0.007399733178317547\n",
            "35060 val_loss: 0.27285972237586975, train_loss: 0.007022791542112827\n",
            "35070 val_loss: 0.2844543755054474, train_loss: 0.007314836606383324\n",
            "35080 val_loss: 0.2952966094017029, train_loss: 0.0077016097493469715\n",
            "35090 val_loss: 0.2552909553050995, train_loss: 0.0067644002847373486\n",
            "35100 val_loss: 0.2701098620891571, train_loss: 0.006670183036476374\n",
            "35110 val_loss: 0.27908855676651, train_loss: 0.006585279479622841\n",
            "35120 val_loss: 0.248590350151062, train_loss: 0.007033768575638533\n",
            "35130 val_loss: 0.2771826982498169, train_loss: 0.00655423803254962\n",
            "35140 val_loss: 0.34722545742988586, train_loss: 0.011149522848427296\n",
            "35150 val_loss: 0.2711285650730133, train_loss: 0.006460554897785187\n",
            "35160 val_loss: 0.27352482080459595, train_loss: 0.006192059721797705\n",
            "35170 val_loss: 0.28340867161750793, train_loss: 0.0064774188213050365\n",
            "35180 val_loss: 0.3029143512248993, train_loss: 0.01048371009528637\n",
            "35190 val_loss: 0.26695773005485535, train_loss: 0.006364000495523214\n",
            "35200 val_loss: 0.2836671471595764, train_loss: 0.006259749177843332\n",
            "35210 val_loss: 0.2852928936481476, train_loss: 0.0064322915859520435\n",
            "35220 val_loss: 0.2650667428970337, train_loss: 0.0059107341803610325\n",
            "35230 val_loss: 0.302350252866745, train_loss: 0.006847119424492121\n",
            "35240 val_loss: 0.26769280433654785, train_loss: 0.00583567563444376\n",
            "35250 val_loss: 0.2919832766056061, train_loss: 0.005907795391976833\n",
            "35260 val_loss: 0.2911219894886017, train_loss: 0.00614564772695303\n",
            "35270 val_loss: 0.26871150732040405, train_loss: 0.005793331190943718\n",
            "35280 val_loss: 0.275450736284256, train_loss: 0.005559784825891256\n",
            "35290 val_loss: 0.2349429875612259, train_loss: 0.0072954753413796425\n",
            "35300 val_loss: 0.49634766578674316, train_loss: 0.09169530123472214\n",
            "35310 val_loss: 0.2651851177215576, train_loss: 0.0060305241495370865\n",
            "35320 val_loss: 0.2576155364513397, train_loss: 0.006963085848838091\n",
            "35330 val_loss: 0.24344436824321747, train_loss: 0.00703027518466115\n",
            "35340 val_loss: 0.26015859842300415, train_loss: 0.006484470330178738\n",
            "35350 val_loss: 0.2466294914484024, train_loss: 0.007203016895800829\n",
            "35360 val_loss: 0.26339203119277954, train_loss: 0.006375305354595184\n",
            "35370 val_loss: 0.2677309811115265, train_loss: 0.006249533034861088\n",
            "35380 val_loss: 0.2747367322444916, train_loss: 0.006059012375771999\n",
            "35390 val_loss: 0.2736174166202545, train_loss: 0.005942893214523792\n",
            "35400 val_loss: 0.26868996024131775, train_loss: 0.005736601073294878\n",
            "35410 val_loss: 0.30010056495666504, train_loss: 0.006394704803824425\n",
            "35420 val_loss: 0.2776910662651062, train_loss: 0.005827311426401138\n",
            "35430 val_loss: 0.27267521619796753, train_loss: 0.005885461810976267\n",
            "35440 val_loss: 0.2975601553916931, train_loss: 0.005804212298244238\n",
            "35450 val_loss: 0.2993682622909546, train_loss: 0.00573330745100975\n",
            "35460 val_loss: 0.3094247579574585, train_loss: 0.005803963169455528\n",
            "35470 val_loss: 0.2973194718360901, train_loss: 0.0057205562479794025\n",
            "35480 val_loss: 0.283368319272995, train_loss: 0.005736695136874914\n",
            "35490 val_loss: 0.27326154708862305, train_loss: 0.005611403379589319\n",
            "35500 val_loss: 0.2943783104419708, train_loss: 0.005980847403407097\n",
            "35510 val_loss: 0.2741241157054901, train_loss: 0.005335698835551739\n",
            "35520 val_loss: 0.29086461663246155, train_loss: 0.0054923356510698795\n",
            "35530 val_loss: 0.3233322501182556, train_loss: 0.007797879166901112\n",
            "35540 val_loss: 0.29623761773109436, train_loss: 0.005720899906009436\n",
            "35550 val_loss: 0.29286274313926697, train_loss: 0.00558902882039547\n",
            "35560 val_loss: 0.2718349099159241, train_loss: 0.005913843866437674\n",
            "35570 val_loss: 0.27091139554977417, train_loss: 0.005697542801499367\n",
            "35580 val_loss: 0.270458459854126, train_loss: 0.005656213033944368\n",
            "35590 val_loss: 0.27033209800720215, train_loss: 0.005742515437304974\n",
            "35600 val_loss: 0.32172876596450806, train_loss: 0.007366013713181019\n",
            "35610 val_loss: 0.28698253631591797, train_loss: 0.005421459209173918\n",
            "35620 val_loss: 0.2865447998046875, train_loss: 0.00546856177970767\n",
            "35630 val_loss: 0.3180343806743622, train_loss: 0.0059775905683636665\n",
            "35640 val_loss: 0.302537202835083, train_loss: 0.005235821474343538\n",
            "35650 val_loss: 0.27971771359443665, train_loss: 0.005540747195482254\n",
            "35660 val_loss: 0.2763095498085022, train_loss: 0.005438053980469704\n",
            "35670 val_loss: 0.28082460165023804, train_loss: 0.005344634875655174\n",
            "35680 val_loss: 0.28297004103660583, train_loss: 0.0050849043764173985\n",
            "35690 val_loss: 0.30835476517677307, train_loss: 0.005265074782073498\n",
            "35700 val_loss: 0.2802974283695221, train_loss: 0.005381242837756872\n",
            "35710 val_loss: 0.3064381182193756, train_loss: 0.005784309469163418\n",
            "35720 val_loss: 0.27799689769744873, train_loss: 0.005374380387365818\n",
            "35730 val_loss: 0.2702030539512634, train_loss: 0.0053580282256007195\n",
            "35740 val_loss: 0.27066999673843384, train_loss: 0.005421116482466459\n",
            "35750 val_loss: 0.30485135316848755, train_loss: 0.00594967370852828\n",
            "35760 val_loss: 0.27664726972579956, train_loss: 0.005151850171387196\n",
            "35770 val_loss: 0.2775135040283203, train_loss: 0.005139015149325132\n",
            "35780 val_loss: 0.2757625877857208, train_loss: 0.005386087577790022\n",
            "35790 val_loss: 0.3080884516239166, train_loss: 0.0061823781579732895\n",
            "35800 val_loss: 0.27540212869644165, train_loss: 0.005292570684105158\n",
            "35810 val_loss: 0.278217613697052, train_loss: 0.005155086517333984\n",
            "35820 val_loss: 0.2830809950828552, train_loss: 0.00515720434486866\n",
            "35830 val_loss: 0.282733678817749, train_loss: 0.005267830565571785\n",
            "35840 val_loss: 0.30293935537338257, train_loss: 0.005202170927077532\n",
            "35850 val_loss: 0.2905700206756592, train_loss: 0.007394380867481232\n",
            "35860 val_loss: 0.2849538326263428, train_loss: 0.005308198276907206\n",
            "35870 val_loss: 0.2840200960636139, train_loss: 0.005254689604043961\n",
            "35880 val_loss: 0.2937127351760864, train_loss: 0.005271931178867817\n",
            "35890 val_loss: 0.2950609028339386, train_loss: 0.005134852137416601\n",
            "35900 val_loss: 0.3156050741672516, train_loss: 0.005108310375362635\n",
            "35910 val_loss: 0.3076350688934326, train_loss: 0.005038744769990444\n",
            "35920 val_loss: 0.2958865463733673, train_loss: 0.005353925283998251\n",
            "35930 val_loss: 0.2645610570907593, train_loss: 0.005380724556744099\n",
            "35940 val_loss: 0.29175087809562683, train_loss: 0.00512575451284647\n",
            "35950 val_loss: 0.34854641556739807, train_loss: 0.005304898601025343\n",
            "35960 val_loss: 0.2855839133262634, train_loss: 0.00554842222481966\n",
            "35970 val_loss: 0.27760249376296997, train_loss: 0.005441687535494566\n",
            "35980 val_loss: 0.2909029424190521, train_loss: 0.005340351723134518\n",
            "35990 val_loss: 0.28278204798698425, train_loss: 0.005089903250336647\n",
            "36000 val_loss: 0.2934471368789673, train_loss: 0.005331175867468119\n",
            "36010 val_loss: 0.28318139910697937, train_loss: 0.005001640412956476\n",
            "36020 val_loss: 0.27297669649124146, train_loss: 0.0051073916256427765\n",
            "36030 val_loss: 0.2922036051750183, train_loss: 0.005068420898169279\n",
            "36040 val_loss: 0.3068772256374359, train_loss: 0.004859610926359892\n",
            "36050 val_loss: 0.2862991690635681, train_loss: 0.004988742060959339\n",
            "36060 val_loss: 0.2882916331291199, train_loss: 0.0050106896087527275\n",
            "36070 val_loss: 0.29632219672203064, train_loss: 0.0051547507755458355\n",
            "36080 val_loss: 0.2927200496196747, train_loss: 0.005106927827000618\n",
            "36090 val_loss: 0.284554660320282, train_loss: 0.0049097901210188866\n",
            "36100 val_loss: 0.27346837520599365, train_loss: 0.0049482304602861404\n",
            "36110 val_loss: 0.28094854950904846, train_loss: 0.004930230788886547\n",
            "36120 val_loss: 0.3015598654747009, train_loss: 0.0050859651528298855\n",
            "36130 val_loss: 0.32582584023475647, train_loss: 0.006258912850171328\n",
            "36140 val_loss: 0.288696825504303, train_loss: 0.0052063241600990295\n",
            "36150 val_loss: 0.2893960773944855, train_loss: 0.005132215563207865\n",
            "36160 val_loss: 0.27050772309303284, train_loss: 0.005228163208812475\n",
            "36170 val_loss: 0.2859409749507904, train_loss: 0.01244291476905346\n",
            "36180 val_loss: 0.3019285798072815, train_loss: 0.005553683266043663\n",
            "36190 val_loss: 0.274905264377594, train_loss: 0.006951323710381985\n",
            "36200 val_loss: 0.2740490138530731, train_loss: 0.005458424333482981\n",
            "36210 val_loss: 0.2931540906429291, train_loss: 0.004912672098726034\n",
            "36220 val_loss: 0.32386353611946106, train_loss: 0.004835222382098436\n",
            "36230 val_loss: 0.3304370045661926, train_loss: 0.005586978048086166\n",
            "36240 val_loss: 0.2974638342857361, train_loss: 0.005134228616952896\n",
            "36250 val_loss: 0.26777806878089905, train_loss: 0.005230328068137169\n",
            "36260 val_loss: 0.3005881607532501, train_loss: 0.0052679008804261684\n",
            "36270 val_loss: 0.28310662508010864, train_loss: 0.004998458083719015\n",
            "36280 val_loss: 0.2952529788017273, train_loss: 0.00515617523342371\n",
            "36290 val_loss: 0.2945033013820648, train_loss: 0.005071483086794615\n",
            "36300 val_loss: 0.2863197326660156, train_loss: 0.0049154190346598625\n",
            "36310 val_loss: 0.27885186672210693, train_loss: 0.005162487272173166\n",
            "36320 val_loss: 0.28103938698768616, train_loss: 0.004802348557859659\n",
            "36330 val_loss: 0.26374372839927673, train_loss: 0.005276443902403116\n",
            "36340 val_loss: 0.26564621925354004, train_loss: 0.005038989242166281\n",
            "36350 val_loss: 0.2587769329547882, train_loss: 0.006192674860358238\n",
            "36360 val_loss: 0.2942592203617096, train_loss: 0.004637389909476042\n",
            "36370 val_loss: 0.3554181754589081, train_loss: 0.0080376286059618\n",
            "36380 val_loss: 0.2756030261516571, train_loss: 0.00473821721971035\n",
            "36390 val_loss: 0.30505165457725525, train_loss: 0.004785641096532345\n",
            "36400 val_loss: 0.2919747233390808, train_loss: 0.004948496352881193\n",
            "36410 val_loss: 0.2994779646396637, train_loss: 0.004851834382861853\n",
            "36420 val_loss: 0.2740194797515869, train_loss: 0.0049933078698813915\n",
            "36430 val_loss: 0.3352418541908264, train_loss: 0.006761129945516586\n",
            "36440 val_loss: 0.282166063785553, train_loss: 0.004811454098671675\n",
            "36450 val_loss: 0.2866875231266022, train_loss: 0.004834396298974752\n",
            "36460 val_loss: 0.28929978609085083, train_loss: 0.004749364219605923\n",
            "36470 val_loss: 0.35696864128112793, train_loss: 0.005708588752895594\n",
            "36480 val_loss: 0.29500308632850647, train_loss: 0.004635351710021496\n",
            "36490 val_loss: 0.29330945014953613, train_loss: 0.004555074498057365\n",
            "36500 val_loss: 0.32671934366226196, train_loss: 0.005457302089780569\n",
            "36510 val_loss: 0.30269306898117065, train_loss: 0.004797955043613911\n",
            "36520 val_loss: 0.28897824883461, train_loss: 0.004753760527819395\n",
            "36530 val_loss: 0.2945123314857483, train_loss: 0.004696520045399666\n",
            "36540 val_loss: 0.30816444754600525, train_loss: 0.004814911633729935\n",
            "36550 val_loss: 0.28450995683670044, train_loss: 0.00511391693726182\n",
            "36560 val_loss: 0.3416672945022583, train_loss: 0.0044236755929887295\n",
            "36570 val_loss: 0.31280821561813354, train_loss: 0.0046238042414188385\n",
            "36580 val_loss: 0.2925015091896057, train_loss: 0.004393938463181257\n",
            "36590 val_loss: 0.2961066961288452, train_loss: 0.004498420748859644\n",
            "36600 val_loss: 0.2950906455516815, train_loss: 0.004839761648327112\n",
            "36610 val_loss: 0.2891741096973419, train_loss: 0.004803814925253391\n",
            "36620 val_loss: 0.35918593406677246, train_loss: 0.008081370033323765\n",
            "36630 val_loss: 0.29056963324546814, train_loss: 0.004654678050428629\n",
            "36640 val_loss: 0.3200814127922058, train_loss: 0.005338104907423258\n",
            "36650 val_loss: 0.3333257734775543, train_loss: 0.0055795712396502495\n",
            "36660 val_loss: 0.29900720715522766, train_loss: 0.004745441488921642\n",
            "36670 val_loss: 0.3082022964954376, train_loss: 0.005787468049675226\n",
            "36680 val_loss: 0.29577261209487915, train_loss: 0.004875239450484514\n",
            "36690 val_loss: 0.32437294721603394, train_loss: 0.0053579071536660194\n",
            "36700 val_loss: 0.28712400794029236, train_loss: 0.004961511120200157\n",
            "36710 val_loss: 0.33497413992881775, train_loss: 0.004566344432532787\n",
            "36720 val_loss: 0.309727281332016, train_loss: 0.004634349141269922\n",
            "36730 val_loss: 0.2866077125072479, train_loss: 0.004464565310627222\n",
            "36740 val_loss: 0.32596030831336975, train_loss: 0.00572138000279665\n",
            "36750 val_loss: 0.2892279028892517, train_loss: 0.004505564458668232\n",
            "36760 val_loss: 0.30104348063468933, train_loss: 0.004432946443557739\n",
            "36770 val_loss: 0.2911297380924225, train_loss: 0.0042719547636806965\n",
            "36780 val_loss: 0.3020423352718353, train_loss: 0.004522769246250391\n",
            "36790 val_loss: 0.2999281883239746, train_loss: 0.004396549426019192\n",
            "36800 val_loss: 0.3356095552444458, train_loss: 0.005265919957309961\n",
            "36810 val_loss: 0.29519203305244446, train_loss: 0.004952038638293743\n",
            "36820 val_loss: 0.3083309531211853, train_loss: 0.004637023434042931\n",
            "36830 val_loss: 0.3247910141944885, train_loss: 0.005226037465035915\n",
            "36840 val_loss: 0.3126835823059082, train_loss: 0.0043428028002381325\n",
            "36850 val_loss: 0.31935736536979675, train_loss: 0.004649955779314041\n",
            "36860 val_loss: 0.31035280227661133, train_loss: 0.004355438984930515\n",
            "36870 val_loss: 0.29644134640693665, train_loss: 0.004455244168639183\n",
            "36880 val_loss: 0.3231934607028961, train_loss: 0.004474259912967682\n",
            "36890 val_loss: 0.3108590543270111, train_loss: 0.004891569260507822\n",
            "36900 val_loss: 0.30231449007987976, train_loss: 0.004498675931245089\n",
            "36910 val_loss: 0.30623379349708557, train_loss: 0.004528756719082594\n",
            "36920 val_loss: 0.30367398262023926, train_loss: 0.004381861537694931\n",
            "36930 val_loss: 0.3127938210964203, train_loss: 0.0045122322626411915\n",
            "36940 val_loss: 0.2890174388885498, train_loss: 0.0042055631056427956\n",
            "36950 val_loss: 0.3307613432407379, train_loss: 0.0044971732422709465\n",
            "36960 val_loss: 0.32019656896591187, train_loss: 0.004805344622582197\n",
            "36970 val_loss: 0.30222782492637634, train_loss: 0.004365346394479275\n",
            "36980 val_loss: 0.32876914739608765, train_loss: 0.005440836772322655\n",
            "36990 val_loss: 0.28718361258506775, train_loss: 0.00455137062817812\n",
            "37000 val_loss: 0.3087975084781647, train_loss: 0.004561907146126032\n",
            "37010 val_loss: 0.2995019555091858, train_loss: 0.004420985002070665\n",
            "37020 val_loss: 0.2957330346107483, train_loss: 0.004513879306614399\n",
            "37030 val_loss: 0.31293749809265137, train_loss: 0.004623612388968468\n",
            "37040 val_loss: 0.2995789051055908, train_loss: 0.0045204199850559235\n",
            "37050 val_loss: 0.3022157847881317, train_loss: 0.004457511939108372\n",
            "37060 val_loss: 0.29818323254585266, train_loss: 0.004311536904424429\n",
            "37070 val_loss: 0.3072797358036041, train_loss: 0.0044288113713264465\n",
            "37080 val_loss: 0.2841360569000244, train_loss: 0.0046183825470507145\n",
            "37090 val_loss: 0.2980249226093292, train_loss: 0.004238275811076164\n",
            "37100 val_loss: 0.30515819787979126, train_loss: 0.004308722913265228\n",
            "37110 val_loss: 0.34211838245391846, train_loss: 0.004186875652521849\n",
            "37120 val_loss: 0.41148802638053894, train_loss: 0.004213626030832529\n",
            "37130 val_loss: 0.3563234210014343, train_loss: 0.00426584854722023\n",
            "37140 val_loss: 0.2900627553462982, train_loss: 0.004491001833230257\n",
            "37150 val_loss: 0.2879319190979004, train_loss: 0.0043886261992156506\n",
            "37160 val_loss: 0.30298465490341187, train_loss: 0.004329853691160679\n",
            "37170 val_loss: 0.29491421580314636, train_loss: 0.004314116667956114\n",
            "37180 val_loss: 0.30188533663749695, train_loss: 0.004190356936305761\n",
            "37190 val_loss: 0.3035922944545746, train_loss: 0.004127598833292723\n",
            "37200 val_loss: 0.3198931813240051, train_loss: 0.004135310649871826\n",
            "37210 val_loss: 0.30374303460121155, train_loss: 0.004284730646759272\n",
            "37220 val_loss: 0.32998138666152954, train_loss: 0.004247176926583052\n",
            "37230 val_loss: 0.34279564023017883, train_loss: 0.004913794342428446\n",
            "37240 val_loss: 0.3124881386756897, train_loss: 0.004376367665827274\n",
            "37250 val_loss: 0.3207234740257263, train_loss: 0.004376220051199198\n",
            "37260 val_loss: 0.31059274077415466, train_loss: 0.004387082066386938\n",
            "37270 val_loss: 0.29745885729789734, train_loss: 0.004476141184568405\n",
            "37280 val_loss: 0.2916278839111328, train_loss: 0.004502128344029188\n",
            "37290 val_loss: 0.3070594370365143, train_loss: 0.004559609107673168\n",
            "37300 val_loss: 0.27129122614860535, train_loss: 0.00525284418836236\n",
            "37310 val_loss: 0.3322444260120392, train_loss: 0.00570128345862031\n",
            "37320 val_loss: 0.301638126373291, train_loss: 0.004399149678647518\n",
            "37330 val_loss: 0.31150174140930176, train_loss: 0.004505684599280357\n",
            "37340 val_loss: 0.306497722864151, train_loss: 0.004570141900330782\n",
            "37350 val_loss: 0.3002840578556061, train_loss: 0.004304253961890936\n",
            "37360 val_loss: 0.310718834400177, train_loss: 0.004518131259828806\n",
            "37370 val_loss: 0.2962315082550049, train_loss: 0.004213806241750717\n",
            "37380 val_loss: 0.2828880548477173, train_loss: 0.004396029282361269\n",
            "37390 val_loss: 0.2862960398197174, train_loss: 0.004268518649041653\n",
            "37400 val_loss: 0.32967767119407654, train_loss: 0.004957660101354122\n",
            "37410 val_loss: 0.3168484568595886, train_loss: 0.004428714979439974\n",
            "37420 val_loss: 0.29278650879859924, train_loss: 0.004336273763328791\n",
            "37430 val_loss: 0.27487489581108093, train_loss: 0.00465170806273818\n",
            "37440 val_loss: 0.3067438304424286, train_loss: 0.004298354499042034\n",
            "37450 val_loss: 0.3209884762763977, train_loss: 0.004515280947089195\n",
            "37460 val_loss: 0.32771188020706177, train_loss: 0.004760900977998972\n",
            "37470 val_loss: 0.3259417414665222, train_loss: 0.004771983716636896\n",
            "37480 val_loss: 0.30449992418289185, train_loss: 0.004330944735556841\n",
            "37490 val_loss: 0.32930052280426025, train_loss: 0.004821151960641146\n",
            "37500 val_loss: 0.3422057330608368, train_loss: 0.004893647972494364\n",
            "37510 val_loss: 0.2966863214969635, train_loss: 0.004188232589513063\n",
            "37520 val_loss: 0.3092178404331207, train_loss: 0.004129801411181688\n",
            "37530 val_loss: 0.36045318841934204, train_loss: 0.0044863284565508366\n",
            "37540 val_loss: 0.302920937538147, train_loss: 0.004301147069782019\n",
            "37550 val_loss: 0.2922396957874298, train_loss: 0.004255302250385284\n",
            "37560 val_loss: 0.2973061501979828, train_loss: 0.0041814809665083885\n",
            "37570 val_loss: 0.30841925740242004, train_loss: 0.004388018045574427\n",
            "37580 val_loss: 0.3176007568836212, train_loss: 0.004526360426098108\n",
            "37590 val_loss: 0.30571508407592773, train_loss: 0.004222127143293619\n",
            "37600 val_loss: 0.3186935782432556, train_loss: 0.005887850187718868\n",
            "37610 val_loss: 0.3147820830345154, train_loss: 0.004246624652296305\n",
            "37620 val_loss: 0.31368643045425415, train_loss: 0.004328081849962473\n",
            "37630 val_loss: 0.3102177381515503, train_loss: 0.004208741709589958\n",
            "37640 val_loss: 0.3209846317768097, train_loss: 0.00471922243013978\n",
            "37650 val_loss: 0.29422470927238464, train_loss: 0.004275711253285408\n",
            "37660 val_loss: 0.3007409870624542, train_loss: 0.0041068787686526775\n",
            "37670 val_loss: 0.2998964190483093, train_loss: 0.004133328329771757\n",
            "37680 val_loss: 0.2962923049926758, train_loss: 0.004143653903156519\n",
            "37690 val_loss: 0.29770323634147644, train_loss: 0.004531524609774351\n",
            "37700 val_loss: 0.2959449887275696, train_loss: 0.004189115483313799\n",
            "37710 val_loss: 0.2826656103134155, train_loss: 0.004353498108685017\n",
            "37720 val_loss: 0.33791080117225647, train_loss: 0.005082100164145231\n",
            "37730 val_loss: 0.3281385898590088, train_loss: 0.004720860160887241\n",
            "37740 val_loss: 0.28205543756484985, train_loss: 0.004361127968877554\n",
            "37750 val_loss: 0.354941189289093, train_loss: 0.004831592086702585\n",
            "37760 val_loss: 0.3090694546699524, train_loss: 0.004137425217777491\n",
            "37770 val_loss: 0.3224015533924103, train_loss: 0.004503043368458748\n",
            "37780 val_loss: 0.3144482672214508, train_loss: 0.004227795638144016\n",
            "37790 val_loss: 0.347272664308548, train_loss: 0.004039389081299305\n",
            "37800 val_loss: 0.3161919116973877, train_loss: 0.004124223720282316\n",
            "37810 val_loss: 0.3455847203731537, train_loss: 0.004466763231903315\n",
            "37820 val_loss: 0.3195194602012634, train_loss: 0.00420837476849556\n",
            "37830 val_loss: 0.31806784868240356, train_loss: 0.004201611969619989\n",
            "37840 val_loss: 0.3150271475315094, train_loss: 0.00404728576540947\n",
            "37850 val_loss: 0.3264164328575134, train_loss: 0.003940938506275415\n",
            "37860 val_loss: 0.3588010370731354, train_loss: 0.0056887660175561905\n",
            "37870 val_loss: 0.3167523145675659, train_loss: 0.003937737550586462\n",
            "37880 val_loss: 0.2939630448818207, train_loss: 0.004742864519357681\n",
            "37890 val_loss: 0.3320905268192291, train_loss: 0.004474712070077658\n",
            "37900 val_loss: 0.2967463433742523, train_loss: 0.004004878923296928\n",
            "37910 val_loss: 0.3272181451320648, train_loss: 0.0039886729791760445\n",
            "37920 val_loss: 0.32201874256134033, train_loss: 0.004091587848961353\n",
            "37930 val_loss: 0.31793704628944397, train_loss: 0.003906642086803913\n",
            "37940 val_loss: 0.3234627842903137, train_loss: 0.003858119249343872\n",
            "37950 val_loss: 0.3032824397087097, train_loss: 0.004403091035783291\n",
            "37960 val_loss: 0.3331788182258606, train_loss: 0.004021333064883947\n",
            "37970 val_loss: 0.3653765022754669, train_loss: 0.006058708298951387\n",
            "37980 val_loss: 0.3015514314174652, train_loss: 0.004046380985528231\n",
            "37990 val_loss: 0.3150700330734253, train_loss: 0.0039570820517838\n",
            "38000 val_loss: 0.31933122873306274, train_loss: 0.004109714645892382\n",
            "38010 val_loss: 0.3018662929534912, train_loss: 0.004249727353453636\n",
            "38020 val_loss: 0.2536046504974365, train_loss: 0.005485793575644493\n",
            "38030 val_loss: 0.30434080958366394, train_loss: 0.004314483143389225\n",
            "38040 val_loss: 0.3138974606990814, train_loss: 0.004094122443348169\n",
            "38050 val_loss: 0.2975219190120697, train_loss: 0.004135599359869957\n",
            "38060 val_loss: 0.3023183345794678, train_loss: 0.0041036843322217464\n",
            "38070 val_loss: 0.3283965587615967, train_loss: 0.004291162826120853\n",
            "38080 val_loss: 0.31702059507369995, train_loss: 0.004075545351952314\n",
            "38090 val_loss: 0.3116927146911621, train_loss: 0.004074272233992815\n",
            "38100 val_loss: 0.2988688051700592, train_loss: 0.004314071033149958\n",
            "38110 val_loss: 0.32034388184547424, train_loss: 0.004016546066850424\n",
            "38120 val_loss: 0.3119297921657562, train_loss: 0.004014918580651283\n",
            "38130 val_loss: 0.3202647864818573, train_loss: 0.0039885095320641994\n",
            "38140 val_loss: 0.32583707571029663, train_loss: 0.004155248869210482\n",
            "38150 val_loss: 0.3088620603084564, train_loss: 0.0038716287817806005\n",
            "38160 val_loss: 0.31759586930274963, train_loss: 0.0038936224300414324\n",
            "38170 val_loss: 0.3282124400138855, train_loss: 0.004024463705718517\n",
            "38180 val_loss: 0.34717684984207153, train_loss: 0.003841707017272711\n",
            "38190 val_loss: 0.3427176773548126, train_loss: 0.004152047447860241\n",
            "38200 val_loss: 0.3013761341571808, train_loss: 0.003748552640900016\n",
            "38210 val_loss: 0.3271128833293915, train_loss: 0.0039200857281684875\n",
            "38220 val_loss: 0.3376941382884979, train_loss: 0.004184782039374113\n",
            "38230 val_loss: 0.37496450543403625, train_loss: 0.0065490491688251495\n",
            "38240 val_loss: 0.30862078070640564, train_loss: 0.0038863280788064003\n",
            "38250 val_loss: 0.3154347240924835, train_loss: 0.0038021295331418514\n",
            "38260 val_loss: 0.3004245162010193, train_loss: 0.004021899309009314\n",
            "38270 val_loss: 0.3100534975528717, train_loss: 0.00364677794277668\n",
            "38280 val_loss: 0.3514731228351593, train_loss: 0.005042106844484806\n",
            "38290 val_loss: 0.3595416843891144, train_loss: 0.005305506754666567\n",
            "38300 val_loss: 0.36808842420578003, train_loss: 0.004250822588801384\n",
            "38310 val_loss: 0.31154388189315796, train_loss: 0.004160724580287933\n",
            "38320 val_loss: 0.346123069524765, train_loss: 0.004781526513397694\n",
            "38330 val_loss: 0.3227832615375519, train_loss: 0.004023132845759392\n",
            "38340 val_loss: 0.3250589668750763, train_loss: 0.0038390967529267073\n",
            "38350 val_loss: 0.33407074213027954, train_loss: 0.003966779448091984\n",
            "38360 val_loss: 0.3246021866798401, train_loss: 0.003726914757862687\n",
            "38370 val_loss: 0.308447003364563, train_loss: 0.0037872325628995895\n",
            "38380 val_loss: 0.3087298572063446, train_loss: 0.004112344700843096\n",
            "38390 val_loss: 0.31418275833129883, train_loss: 0.003618103452026844\n",
            "38400 val_loss: 0.3151634633541107, train_loss: 0.0037219147197902203\n",
            "38410 val_loss: 0.3355690538883209, train_loss: 0.004043004475533962\n",
            "38420 val_loss: 0.3377087116241455, train_loss: 0.0039144521579146385\n",
            "38430 val_loss: 0.3245835304260254, train_loss: 0.0036366931162774563\n",
            "38440 val_loss: 0.326030433177948, train_loss: 0.003703181864693761\n",
            "38450 val_loss: 0.35338228940963745, train_loss: 0.005024198908358812\n",
            "38460 val_loss: 0.3361677825450897, train_loss: 0.003728830488398671\n",
            "38470 val_loss: 0.3188287317752838, train_loss: 0.0037012752145528793\n",
            "38480 val_loss: 0.3149927854537964, train_loss: 0.003713221289217472\n",
            "38490 val_loss: 0.3113367557525635, train_loss: 0.004797058645635843\n",
            "38500 val_loss: 0.3560723662376404, train_loss: 0.003763543674722314\n",
            "38510 val_loss: 0.3327656388282776, train_loss: 0.003793884301558137\n",
            "38520 val_loss: 0.3589971959590912, train_loss: 0.004831386264413595\n",
            "38530 val_loss: 0.31338563561439514, train_loss: 0.003584061749279499\n",
            "38540 val_loss: 0.3191603422164917, train_loss: 0.0035854352172464132\n",
            "38550 val_loss: 0.32520174980163574, train_loss: 0.0035125527065247297\n",
            "38560 val_loss: 0.30167755484580994, train_loss: 0.0036572327371686697\n",
            "38570 val_loss: 0.3215809464454651, train_loss: 0.0036487344186753035\n",
            "38580 val_loss: 0.30728086829185486, train_loss: 0.0035813667345792055\n",
            "38590 val_loss: 0.38515669107437134, train_loss: 0.006550092250108719\n",
            "38600 val_loss: 0.37764716148376465, train_loss: 0.0059289950877428055\n",
            "38610 val_loss: 0.3199067711830139, train_loss: 0.003693368984386325\n",
            "38620 val_loss: 0.335985004901886, train_loss: 0.004012921825051308\n",
            "38630 val_loss: 0.3545669913291931, train_loss: 0.012871337123215199\n",
            "38640 val_loss: 0.3376310467720032, train_loss: 0.003777777310460806\n",
            "38650 val_loss: 0.34167903661727905, train_loss: 0.004148972220718861\n",
            "38660 val_loss: 0.3239005506038666, train_loss: 0.0037948053795844316\n",
            "38670 val_loss: 0.33761829137802124, train_loss: 0.004164898302406073\n",
            "38680 val_loss: 0.3123062252998352, train_loss: 0.003784977598115802\n",
            "38690 val_loss: 0.3260871469974518, train_loss: 0.003738563973456621\n",
            "38700 val_loss: 0.34941649436950684, train_loss: 0.003874019021168351\n",
            "38710 val_loss: 0.32057756185531616, train_loss: 0.003825678490102291\n",
            "38720 val_loss: 0.3490222990512848, train_loss: 0.004244104493409395\n",
            "38730 val_loss: 0.33031466603279114, train_loss: 0.0038318922743201256\n",
            "38740 val_loss: 0.353060781955719, train_loss: 0.004562360234558582\n",
            "38750 val_loss: 0.3036912679672241, train_loss: 0.0038318138103932142\n",
            "38760 val_loss: 0.3356488347053528, train_loss: 0.00407013762742281\n",
            "38770 val_loss: 0.31879597902297974, train_loss: 0.003953952342271805\n",
            "38780 val_loss: 0.3629003167152405, train_loss: 0.004021758679300547\n",
            "38790 val_loss: 0.348219096660614, train_loss: 0.004110608249902725\n",
            "38800 val_loss: 0.33309775590896606, train_loss: 0.003758960170671344\n",
            "38810 val_loss: 0.3171084225177765, train_loss: 0.0038126669824123383\n",
            "38820 val_loss: 0.3456436097621918, train_loss: 0.004386281128972769\n",
            "38830 val_loss: 0.3494934141635895, train_loss: 0.0044243610464036465\n",
            "38840 val_loss: 0.34848853945732117, train_loss: 0.004501319956034422\n",
            "38850 val_loss: 0.31883561611175537, train_loss: 0.004213300999253988\n",
            "38860 val_loss: 0.32958945631980896, train_loss: 0.003638460300862789\n",
            "38870 val_loss: 0.3054122030735016, train_loss: 0.0037820925936102867\n",
            "38880 val_loss: 0.32146307826042175, train_loss: 0.0035362662747502327\n",
            "38890 val_loss: 0.39222413301467896, train_loss: 0.006728491745889187\n",
            "38900 val_loss: 0.3499157428741455, train_loss: 0.004283562768250704\n",
            "38910 val_loss: 0.3330906629562378, train_loss: 0.0037418438587337732\n",
            "38920 val_loss: 0.34105613827705383, train_loss: 0.0037907790392637253\n",
            "38930 val_loss: 0.34376996755599976, train_loss: 0.004362700041383505\n",
            "38940 val_loss: 0.3085944950580597, train_loss: 0.003778499085456133\n",
            "38950 val_loss: 0.3106381893157959, train_loss: 0.0037372857332229614\n",
            "38960 val_loss: 0.34540167450904846, train_loss: 0.00397774251177907\n",
            "38970 val_loss: 0.32599249482154846, train_loss: 0.003656393149867654\n",
            "38980 val_loss: 0.32624346017837524, train_loss: 0.0036230813711881638\n",
            "38990 val_loss: 0.32978537678718567, train_loss: 0.0037236465141177177\n",
            "39000 val_loss: 0.329925537109375, train_loss: 0.0036830175668001175\n",
            "39010 val_loss: 0.32072558999061584, train_loss: 0.003561001271009445\n",
            "39020 val_loss: 0.33410292863845825, train_loss: 0.003742025000974536\n",
            "39030 val_loss: 0.2920803725719452, train_loss: 0.0038660219870507717\n",
            "39040 val_loss: 0.31375566124916077, train_loss: 0.0034736841917037964\n",
            "39050 val_loss: 0.32117143273353577, train_loss: 0.003489127615466714\n",
            "39060 val_loss: 0.3230222761631012, train_loss: 0.0035198975820094347\n",
            "39070 val_loss: 0.3125402629375458, train_loss: 0.004423811100423336\n",
            "39080 val_loss: 0.3339843451976776, train_loss: 0.003450597170740366\n",
            "39090 val_loss: 0.30758246779441833, train_loss: 0.0035150579642504454\n",
            "39100 val_loss: 0.37188324332237244, train_loss: 0.003414772218093276\n",
            "39110 val_loss: 0.36346933245658875, train_loss: 0.003380407579243183\n",
            "39120 val_loss: 0.31716009974479675, train_loss: 0.0033948910422623158\n",
            "39130 val_loss: 0.3295247554779053, train_loss: 0.0034765577875077724\n",
            "39140 val_loss: 0.32918912172317505, train_loss: 0.0034642971586436033\n",
            "39150 val_loss: 0.32528501749038696, train_loss: 0.00349069619551301\n",
            "39160 val_loss: 0.34507572650909424, train_loss: 0.003652108833193779\n",
            "39170 val_loss: 0.37248486280441284, train_loss: 0.004960277117788792\n",
            "39180 val_loss: 0.30241361260414124, train_loss: 0.003537734504789114\n",
            "39190 val_loss: 0.3019331097602844, train_loss: 0.00354902446269989\n",
            "39200 val_loss: 0.34832140803337097, train_loss: 0.003965866286307573\n",
            "39210 val_loss: 0.34263375401496887, train_loss: 0.00396343506872654\n",
            "39220 val_loss: 0.3157890737056732, train_loss: 0.0035558915697038174\n",
            "39230 val_loss: 0.3143586218357086, train_loss: 0.003506873967126012\n",
            "39240 val_loss: 0.32090938091278076, train_loss: 0.0035213737282902002\n",
            "39250 val_loss: 0.31834885478019714, train_loss: 0.003488673595711589\n",
            "39260 val_loss: 0.3374991714954376, train_loss: 0.0034368422348052263\n",
            "39270 val_loss: 0.33199191093444824, train_loss: 0.0035567674785852432\n",
            "39280 val_loss: 0.3562823534011841, train_loss: 0.004202266689389944\n",
            "39290 val_loss: 0.3186792731285095, train_loss: 0.003463191445916891\n",
            "39300 val_loss: 0.35972753167152405, train_loss: 0.0043637617491185665\n",
            "39310 val_loss: 0.30179551243782043, train_loss: 0.0037270993925631046\n",
            "39320 val_loss: 0.3491863012313843, train_loss: 0.003529125824570656\n",
            "39330 val_loss: 0.3391699194908142, train_loss: 0.003554479917511344\n",
            "39340 val_loss: 0.3273671865463257, train_loss: 0.0034400224685668945\n",
            "39350 val_loss: 0.3305057883262634, train_loss: 0.0034025409258902073\n",
            "39360 val_loss: 0.3493689298629761, train_loss: 0.003749464638531208\n",
            "39370 val_loss: 0.33195844292640686, train_loss: 0.0034612284507602453\n",
            "39380 val_loss: 0.3255608081817627, train_loss: 0.0034411789383739233\n",
            "39390 val_loss: 0.3438095450401306, train_loss: 0.0037008721847087145\n",
            "39400 val_loss: 0.3363902270793915, train_loss: 0.003544479375705123\n",
            "39410 val_loss: 0.3281944692134857, train_loss: 0.003430344630032778\n",
            "39420 val_loss: 0.34198614954948425, train_loss: 0.0033778606448322535\n",
            "39430 val_loss: 0.33795419335365295, train_loss: 0.003304342506453395\n",
            "39440 val_loss: 0.31720882654190063, train_loss: 0.003411599900573492\n",
            "39450 val_loss: 0.3259636461734772, train_loss: 0.003356359200552106\n",
            "39460 val_loss: 0.33018016815185547, train_loss: 0.0033282972872257233\n",
            "39470 val_loss: 0.3362692892551422, train_loss: 0.003360163187608123\n",
            "39480 val_loss: 0.3217887282371521, train_loss: 0.003294431371614337\n",
            "39490 val_loss: 0.342825323343277, train_loss: 0.003627752885222435\n",
            "39500 val_loss: 0.3461371660232544, train_loss: 0.0038329383824020624\n",
            "39510 val_loss: 0.3265140950679779, train_loss: 0.003314112313091755\n",
            "39520 val_loss: 0.4627102017402649, train_loss: 0.017230920493602753\n",
            "39530 val_loss: 0.31388115882873535, train_loss: 0.003474203636869788\n",
            "39540 val_loss: 0.3329785168170929, train_loss: 0.003596732160076499\n",
            "39550 val_loss: 0.32380810379981995, train_loss: 0.0033215340226888657\n",
            "39560 val_loss: 0.3490574359893799, train_loss: 0.0034104695077985525\n",
            "39570 val_loss: 0.33568018674850464, train_loss: 0.0032576909288764\n",
            "39580 val_loss: 0.3000732958316803, train_loss: 0.0034557003527879715\n",
            "39590 val_loss: 0.37638843059539795, train_loss: 0.004831661470234394\n",
            "39600 val_loss: 0.3220961093902588, train_loss: 0.0032680530566722155\n",
            "39610 val_loss: 0.3447757959365845, train_loss: 0.003307657316327095\n",
            "39620 val_loss: 0.36029210686683655, train_loss: 0.003728898474946618\n",
            "39630 val_loss: 0.3425981104373932, train_loss: 0.003174697980284691\n",
            "39640 val_loss: 0.32700905203819275, train_loss: 0.0031960508786141872\n",
            "39650 val_loss: 0.33647048473358154, train_loss: 0.0031612240709364414\n",
            "39660 val_loss: 0.4145869314670563, train_loss: 0.0032646525651216507\n",
            "39670 val_loss: 0.3499728739261627, train_loss: 0.003312536748126149\n",
            "39680 val_loss: 0.33397239446640015, train_loss: 0.0029972016345709562\n",
            "39690 val_loss: 0.34701305627822876, train_loss: 0.003360446309670806\n",
            "39700 val_loss: 0.31595906615257263, train_loss: 0.0038588731549680233\n",
            "39710 val_loss: 0.34321165084838867, train_loss: 0.003065873170271516\n",
            "39720 val_loss: 0.3432641327381134, train_loss: 0.003114399267360568\n",
            "39730 val_loss: 0.3151053190231323, train_loss: 0.003038234543055296\n",
            "39740 val_loss: 0.2980721890926361, train_loss: 0.003383589442819357\n",
            "39750 val_loss: 0.3431377410888672, train_loss: 0.003311971900984645\n",
            "39760 val_loss: 0.29714614152908325, train_loss: 0.0033562611788511276\n",
            "39770 val_loss: 0.3086385130882263, train_loss: 0.0032320681493729353\n",
            "39780 val_loss: 0.31893980503082275, train_loss: 0.003142628353089094\n",
            "39790 val_loss: 0.3293320834636688, train_loss: 0.003088768105953932\n",
            "39800 val_loss: 0.3278510570526123, train_loss: 0.0030664235819131136\n",
            "39810 val_loss: 0.30809715390205383, train_loss: 0.0032604497391730547\n",
            "39820 val_loss: 0.3626357614994049, train_loss: 0.003630557330325246\n",
            "39830 val_loss: 0.3375928997993469, train_loss: 0.0031756949611008167\n",
            "39840 val_loss: 0.32457712292671204, train_loss: 0.0031970301643013954\n",
            "39850 val_loss: 0.3181811273097992, train_loss: 0.003309267805889249\n",
            "39860 val_loss: 0.34098246693611145, train_loss: 0.0032555279321968555\n",
            "39870 val_loss: 0.3373965919017792, train_loss: 0.0033240593038499355\n",
            "39880 val_loss: 0.350084513425827, train_loss: 0.003780568949878216\n",
            "39890 val_loss: 0.2897490859031677, train_loss: 0.00391792505979538\n",
            "39900 val_loss: 0.3511294722557068, train_loss: 0.0033978354185819626\n",
            "39910 val_loss: 0.34907129406929016, train_loss: 0.003492926713079214\n",
            "39920 val_loss: 0.3508530259132385, train_loss: 0.0035386066883802414\n",
            "39930 val_loss: 0.3326569199562073, train_loss: 0.00347306951880455\n",
            "39940 val_loss: 0.32606369256973267, train_loss: 0.0034234430640935898\n",
            "39950 val_loss: 0.35834354162216187, train_loss: 0.003910182509571314\n",
            "39960 val_loss: 0.3204473853111267, train_loss: 0.0033315392211079597\n",
            "39970 val_loss: 0.3432543873786926, train_loss: 0.003574403002858162\n",
            "39980 val_loss: 0.33987435698509216, train_loss: 0.003398945787921548\n",
            "39990 val_loss: 0.32714003324508667, train_loss: 0.0033865117002278566\n",
            "40000 val_loss: 0.3743325471878052, train_loss: 0.004693423397839069\n",
            "40010 val_loss: 0.3282581865787506, train_loss: 0.0033104033209383488\n",
            "40020 val_loss: 0.3348056375980377, train_loss: 0.003855515504255891\n",
            "40030 val_loss: 0.32899248600006104, train_loss: 0.0033634393475949764\n",
            "40040 val_loss: 0.3532853126525879, train_loss: 0.0037157319020479918\n",
            "40050 val_loss: 0.33807173371315, train_loss: 0.0034276254009455442\n",
            "40060 val_loss: 0.3374637961387634, train_loss: 0.0035388160031288862\n",
            "40070 val_loss: 0.36806949973106384, train_loss: 0.004239758942276239\n",
            "40080 val_loss: 0.3168540298938751, train_loss: 0.003520759055390954\n",
            "40090 val_loss: 0.3445700407028198, train_loss: 0.0033150920644402504\n",
            "40100 val_loss: 0.3412817716598511, train_loss: 0.00338403950445354\n",
            "40110 val_loss: 0.32739418745040894, train_loss: 0.0032797546591609716\n",
            "40120 val_loss: 0.32776522636413574, train_loss: 0.0032457541674375534\n",
            "40130 val_loss: 0.3416271209716797, train_loss: 0.0031348606571555138\n",
            "40140 val_loss: 0.3396405875682831, train_loss: 0.0030614910647273064\n",
            "40150 val_loss: 0.33254534006118774, train_loss: 0.003118858439847827\n",
            "40160 val_loss: 0.34468957781791687, train_loss: 0.0032284690532833338\n",
            "40170 val_loss: 0.3354128301143646, train_loss: 0.003083742456510663\n",
            "40180 val_loss: 0.3222108483314514, train_loss: 0.003106720047071576\n",
            "40190 val_loss: 0.328418105840683, train_loss: 0.0031212551984936\n",
            "40200 val_loss: 0.3441274166107178, train_loss: 0.003045459510758519\n",
            "40210 val_loss: 0.3499739170074463, train_loss: 0.004285894799977541\n",
            "40220 val_loss: 0.34480446577072144, train_loss: 0.0031311851926147938\n",
            "40230 val_loss: 0.31917327642440796, train_loss: 0.0031537385657429695\n",
            "40240 val_loss: 0.35117459297180176, train_loss: 0.0034769405610859394\n",
            "40250 val_loss: 0.35410088300704956, train_loss: 0.003216338809579611\n",
            "40260 val_loss: 0.34585967659950256, train_loss: 0.0033053825609385967\n",
            "40270 val_loss: 0.3276308476924896, train_loss: 0.003225891850888729\n",
            "40280 val_loss: 0.3332866430282593, train_loss: 0.003188219852745533\n",
            "40290 val_loss: 0.3462730348110199, train_loss: 0.0032293193507939577\n",
            "40300 val_loss: 0.3241136074066162, train_loss: 0.0030977202113717794\n",
            "40310 val_loss: 0.3457995057106018, train_loss: 0.0032537532970309258\n",
            "40320 val_loss: 0.3524438440799713, train_loss: 0.0032289933878928423\n",
            "40330 val_loss: 0.3402720093727112, train_loss: 0.003080840688198805\n",
            "40340 val_loss: 0.3285026550292969, train_loss: 0.0031505206134170294\n",
            "40350 val_loss: 0.33468884229660034, train_loss: 0.0031139072962105274\n",
            "40360 val_loss: 0.341415673494339, train_loss: 0.0030975998379290104\n",
            "40370 val_loss: 0.3491889536380768, train_loss: 0.003102086018770933\n",
            "40380 val_loss: 0.3418746292591095, train_loss: 0.003098998451605439\n",
            "40390 val_loss: 0.3763226568698883, train_loss: 0.0030070680659264326\n",
            "40400 val_loss: 0.3774690628051758, train_loss: 0.004128214903175831\n",
            "40410 val_loss: 0.3520008623600006, train_loss: 0.0033831200562417507\n",
            "40420 val_loss: 0.32586151361465454, train_loss: 0.0031249800231307745\n",
            "40430 val_loss: 0.32707780599594116, train_loss: 0.003032171633094549\n",
            "40440 val_loss: 0.3327104151248932, train_loss: 0.0032732803374528885\n",
            "40450 val_loss: 0.31370311975479126, train_loss: 0.0031974997837096453\n",
            "40460 val_loss: 0.3249233365058899, train_loss: 0.0032189846970140934\n",
            "40470 val_loss: 0.32720550894737244, train_loss: 0.003244675463065505\n",
            "40480 val_loss: 0.346752792596817, train_loss: 0.0033962775487452745\n",
            "40490 val_loss: 0.3291928768157959, train_loss: 0.003205939196050167\n",
            "40500 val_loss: 0.3664081394672394, train_loss: 0.0031945693772286177\n",
            "40510 val_loss: 0.32160869240760803, train_loss: 0.0033498750999569893\n",
            "40520 val_loss: 0.33115649223327637, train_loss: 0.0032713175751268864\n",
            "40530 val_loss: 0.3504793047904968, train_loss: 0.004283506888896227\n",
            "40540 val_loss: 0.3339168429374695, train_loss: 0.0030782066751271486\n",
            "40550 val_loss: 0.3405979573726654, train_loss: 0.0030424247961491346\n",
            "40560 val_loss: 0.33648771047592163, train_loss: 0.0031013754196465015\n",
            "40570 val_loss: 0.3216680884361267, train_loss: 0.0031256594229489565\n",
            "40580 val_loss: 0.3606277108192444, train_loss: 0.0035061915405094624\n",
            "40590 val_loss: 0.33744820952415466, train_loss: 0.0030539201106876135\n",
            "40600 val_loss: 0.3310492932796478, train_loss: 0.0030947434715926647\n",
            "40610 val_loss: 0.3449486494064331, train_loss: 0.0029645429458469152\n",
            "40620 val_loss: 0.3495590388774872, train_loss: 0.0029065904673188925\n",
            "40630 val_loss: 0.3468701243400574, train_loss: 0.002882017521187663\n",
            "40640 val_loss: 0.3371535539627075, train_loss: 0.0029235861729830503\n",
            "40650 val_loss: 0.35838088393211365, train_loss: 0.0032803385984152555\n",
            "40660 val_loss: 0.40371182560920715, train_loss: 0.003050042549148202\n",
            "40670 val_loss: 0.34332212805747986, train_loss: 0.0030588137451559305\n",
            "40680 val_loss: 0.34388676285743713, train_loss: 0.0032929496373981237\n",
            "40690 val_loss: 0.3222328722476959, train_loss: 0.0031349111814051867\n",
            "40700 val_loss: 0.34855762124061584, train_loss: 0.0032194850500673056\n",
            "40710 val_loss: 0.33962351083755493, train_loss: 0.0032324769999831915\n",
            "40720 val_loss: 0.32655033469200134, train_loss: 0.0032328914385288954\n",
            "40730 val_loss: 0.3308153450489044, train_loss: 0.0032092968467622995\n",
            "40740 val_loss: 0.3250712752342224, train_loss: 0.003197919577360153\n",
            "40750 val_loss: 0.3534955084323883, train_loss: 0.003115049796178937\n",
            "40760 val_loss: 0.3756197690963745, train_loss: 0.003162863664329052\n",
            "40770 val_loss: 0.3640395402908325, train_loss: 0.0030984394252300262\n",
            "40780 val_loss: 0.34912940859794617, train_loss: 0.00302620860747993\n",
            "40790 val_loss: 0.33559492230415344, train_loss: 0.0029473723843693733\n",
            "40800 val_loss: 0.33118700981140137, train_loss: 0.002909813541918993\n",
            "40810 val_loss: 0.37100258469581604, train_loss: 0.002874912926927209\n",
            "40820 val_loss: 0.36172017455101013, train_loss: 0.0028853523544967175\n",
            "40830 val_loss: 0.3725637197494507, train_loss: 0.003716286038979888\n",
            "40840 val_loss: 0.3506631553173065, train_loss: 0.0028404633048921824\n",
            "40850 val_loss: 0.34353965520858765, train_loss: 0.0029561417177319527\n",
            "40860 val_loss: 0.40022096037864685, train_loss: 0.003330657724291086\n",
            "40870 val_loss: 0.3429678678512573, train_loss: 0.0028714649379253387\n",
            "40880 val_loss: 0.38680776953697205, train_loss: 0.004056905396282673\n",
            "40890 val_loss: 0.42147669196128845, train_loss: 0.007683209143579006\n",
            "40900 val_loss: 0.35899657011032104, train_loss: 0.0032989501487463713\n",
            "40910 val_loss: 0.34868207573890686, train_loss: 0.0029744706116616726\n",
            "40920 val_loss: 0.3317878246307373, train_loss: 0.0032787532545626163\n",
            "40930 val_loss: 0.34146419167518616, train_loss: 0.002931673312559724\n",
            "40940 val_loss: 0.34114447236061096, train_loss: 0.002934914780780673\n",
            "40950 val_loss: 0.33756813406944275, train_loss: 0.0028833828400820494\n",
            "40960 val_loss: 0.32866716384887695, train_loss: 0.0028954939916729927\n",
            "40970 val_loss: 0.33011212944984436, train_loss: 0.0028535276651382446\n",
            "40980 val_loss: 0.3284892141819, train_loss: 0.0031615030020475388\n",
            "40990 val_loss: 0.40678706765174866, train_loss: 0.005156827624887228\n",
            "41000 val_loss: 0.3438006043434143, train_loss: 0.0028541486244648695\n",
            "41010 val_loss: 0.3429761528968811, train_loss: 0.0028495939914137125\n",
            "41020 val_loss: 0.3240315914154053, train_loss: 0.0029776832088828087\n",
            "41030 val_loss: 0.3896443843841553, train_loss: 0.004210963379591703\n",
            "41040 val_loss: 0.34803470969200134, train_loss: 0.00294749834574759\n",
            "41050 val_loss: 0.3483985960483551, train_loss: 0.002919459715485573\n",
            "41060 val_loss: 0.34743836522102356, train_loss: 0.0028778803534805775\n",
            "41070 val_loss: 0.3589407503604889, train_loss: 0.0031772691290825605\n",
            "41080 val_loss: 0.34532588720321655, train_loss: 0.002983733080327511\n",
            "41090 val_loss: 0.3979392647743225, train_loss: 0.004682029597461224\n",
            "41100 val_loss: 0.35896873474121094, train_loss: 0.0031785378232598305\n",
            "41110 val_loss: 0.3254590630531311, train_loss: 0.003082426730543375\n",
            "41120 val_loss: 0.35926157236099243, train_loss: 0.0030564414337277412\n",
            "41130 val_loss: 0.3992216885089874, train_loss: 0.00445755198597908\n",
            "41140 val_loss: 0.3611387312412262, train_loss: 0.0032491646707057953\n",
            "41150 val_loss: 0.3891252279281616, train_loss: 0.004251414909958839\n",
            "41160 val_loss: 0.34803110361099243, train_loss: 0.003134586615487933\n",
            "41170 val_loss: 0.34554681181907654, train_loss: 0.0030563590116798878\n",
            "41180 val_loss: 0.34444528818130493, train_loss: 0.0030751186423003674\n",
            "41190 val_loss: 0.32951807975769043, train_loss: 0.002992916852235794\n",
            "41200 val_loss: 0.3798486590385437, train_loss: 0.002986301900818944\n",
            "41210 val_loss: 0.3796822130680084, train_loss: 0.0030118359718471766\n",
            "41220 val_loss: 0.3877737522125244, train_loss: 0.0032900297082960606\n",
            "41230 val_loss: 0.34525883197784424, train_loss: 0.004669929388910532\n",
            "41240 val_loss: 0.34982118010520935, train_loss: 0.0032328939996659756\n",
            "41250 val_loss: 0.3395405411720276, train_loss: 0.0031348096672445536\n",
            "41260 val_loss: 0.35005882382392883, train_loss: 0.0032304294873028994\n",
            "41270 val_loss: 0.3545917570590973, train_loss: 0.003113475162535906\n",
            "41280 val_loss: 0.3595360815525055, train_loss: 0.003209833288565278\n",
            "41290 val_loss: 0.3585417568683624, train_loss: 0.003052102169021964\n",
            "41300 val_loss: 0.35104233026504517, train_loss: 0.0030411190818995237\n",
            "41310 val_loss: 0.3340867757797241, train_loss: 0.0030226747039705515\n",
            "41320 val_loss: 0.33820223808288574, train_loss: 0.0030022829305380583\n",
            "41330 val_loss: 0.45453542470932007, train_loss: 0.01278054341673851\n",
            "41340 val_loss: 0.36856263875961304, train_loss: 0.0032917282078415155\n",
            "41350 val_loss: 0.342120885848999, train_loss: 0.0031342662405222654\n",
            "41360 val_loss: 0.34207606315612793, train_loss: 0.0031877662986516953\n",
            "41370 val_loss: 0.33318576216697693, train_loss: 0.003071861108765006\n",
            "41380 val_loss: 0.33991312980651855, train_loss: 0.0030671744607388973\n",
            "41390 val_loss: 0.3317084014415741, train_loss: 0.00304481852799654\n",
            "41400 val_loss: 0.3429097533226013, train_loss: 0.003048989921808243\n",
            "41410 val_loss: 0.3503791391849518, train_loss: 0.0030072347726672888\n",
            "41420 val_loss: 0.3450063169002533, train_loss: 0.002955969888716936\n",
            "41430 val_loss: 0.3482469916343689, train_loss: 0.0029413821175694466\n",
            "41440 val_loss: 0.3292199969291687, train_loss: 0.0031018992885947227\n",
            "41450 val_loss: 0.3568439185619354, train_loss: 0.003063003998249769\n",
            "41460 val_loss: 0.3546680212020874, train_loss: 0.002948100445792079\n",
            "41470 val_loss: 0.35043731331825256, train_loss: 0.002944193547591567\n",
            "41480 val_loss: 0.3318260908126831, train_loss: 0.003198800375685096\n",
            "41490 val_loss: 0.3762793242931366, train_loss: 0.0031044157221913338\n",
            "41500 val_loss: 0.3192175626754761, train_loss: 0.003151176730170846\n",
            "41510 val_loss: 0.3375388979911804, train_loss: 0.0030294028110802174\n",
            "41520 val_loss: 0.39238935708999634, train_loss: 0.0034727377351373434\n",
            "41530 val_loss: 0.40099388360977173, train_loss: 0.004591355565935373\n",
            "41540 val_loss: 0.34227582812309265, train_loss: 0.0030855201184749603\n",
            "41550 val_loss: 0.3223723769187927, train_loss: 0.0036148957442492247\n",
            "41560 val_loss: 0.36071762442588806, train_loss: 0.0032906602136790752\n",
            "41570 val_loss: 0.3321227431297302, train_loss: 0.0030576076824218035\n",
            "41580 val_loss: 0.34355300664901733, train_loss: 0.0030062186997383833\n",
            "41590 val_loss: 0.3648703992366791, train_loss: 0.003096767235547304\n",
            "41600 val_loss: 0.35370808839797974, train_loss: 0.002899518935009837\n",
            "41610 val_loss: 0.3266594409942627, train_loss: 0.003829399822279811\n",
            "41620 val_loss: 0.3806913495063782, train_loss: 0.0033430559560656548\n",
            "41630 val_loss: 0.35842230916023254, train_loss: 0.0029860821086913347\n",
            "41640 val_loss: 0.3390422463417053, train_loss: 0.0028007279615849257\n",
            "41650 val_loss: 0.3360285758972168, train_loss: 0.002807282144203782\n",
            "41660 val_loss: 0.3307058811187744, train_loss: 0.0029666516929864883\n",
            "41670 val_loss: 0.36424100399017334, train_loss: 0.0030663926154375076\n",
            "41680 val_loss: 0.31930550932884216, train_loss: 0.002996171824634075\n",
            "41690 val_loss: 0.34661296010017395, train_loss: 0.002910145092755556\n",
            "41700 val_loss: 0.3710677921772003, train_loss: 0.003353009931743145\n",
            "41710 val_loss: 0.34947970509529114, train_loss: 0.0028836303390562534\n",
            "41720 val_loss: 0.31827396154403687, train_loss: 0.002956288168206811\n",
            "41730 val_loss: 0.3418424427509308, train_loss: 0.002944525331258774\n",
            "41740 val_loss: 0.36463606357574463, train_loss: 0.0030939572025090456\n",
            "41750 val_loss: 0.3242087960243225, train_loss: 0.0029478652868419886\n",
            "41760 val_loss: 0.3884669840335846, train_loss: 0.0038941255770623684\n",
            "41770 val_loss: 0.3398553431034088, train_loss: 0.002897377824410796\n",
            "41780 val_loss: 0.3467446565628052, train_loss: 0.002916237572208047\n",
            "41790 val_loss: 0.37500834465026855, train_loss: 0.002965139225125313\n",
            "41800 val_loss: 0.3763531744480133, train_loss: 0.0030626999214291573\n",
            "41810 val_loss: 0.37922176718711853, train_loss: 0.0030394329223781824\n",
            "41820 val_loss: 0.36587244272232056, train_loss: 0.002936325268819928\n",
            "41830 val_loss: 0.37239423394203186, train_loss: 0.0029899985529482365\n",
            "41840 val_loss: 0.35681042075157166, train_loss: 0.002831885125488043\n",
            "41850 val_loss: 0.3526926338672638, train_loss: 0.002792217070236802\n",
            "41860 val_loss: 0.42064571380615234, train_loss: 0.005225728265941143\n",
            "41870 val_loss: 0.3725018799304962, train_loss: 0.002752074273303151\n",
            "41880 val_loss: 0.33468931913375854, train_loss: 0.0028794119134545326\n",
            "41890 val_loss: 0.34856414794921875, train_loss: 0.0028292883653193712\n",
            "41900 val_loss: 0.3357807993888855, train_loss: 0.0027178360614925623\n",
            "41910 val_loss: 0.36645185947418213, train_loss: 0.0027512279339134693\n",
            "41920 val_loss: 0.3624492585659027, train_loss: 0.0028235174249857664\n",
            "41930 val_loss: 0.3564974367618561, train_loss: 0.0027456001844257116\n",
            "41940 val_loss: 0.3402048647403717, train_loss: 0.0027978599537163973\n",
            "41950 val_loss: 0.3556063771247864, train_loss: 0.0028155327308923006\n",
            "41960 val_loss: 0.36487001180648804, train_loss: 0.0027941616717725992\n",
            "41970 val_loss: 0.32914745807647705, train_loss: 0.0028701932169497013\n",
            "41980 val_loss: 0.3585740923881531, train_loss: 0.0026899853255599737\n",
            "41990 val_loss: 0.3524063527584076, train_loss: 0.0027017684187740088\n",
            "42000 val_loss: 0.3497430384159088, train_loss: 0.0027098271530121565\n",
            "42010 val_loss: 0.3740689754486084, train_loss: 0.002915710909292102\n",
            "42020 val_loss: 0.3174274265766144, train_loss: 0.002975654788315296\n",
            "42030 val_loss: 0.3392481505870819, train_loss: 0.0027035127859562635\n",
            "42040 val_loss: 0.3556688129901886, train_loss: 0.0027196414303034544\n",
            "42050 val_loss: 0.4115028977394104, train_loss: 0.003889022395014763\n",
            "42060 val_loss: 0.34673288464546204, train_loss: 0.0027235555462539196\n",
            "42070 val_loss: 0.3574615716934204, train_loss: 0.002765507670119405\n",
            "42080 val_loss: 0.378277450799942, train_loss: 0.0030934959650039673\n",
            "42090 val_loss: 0.33024802803993225, train_loss: 0.003264873055741191\n",
            "42100 val_loss: 0.33780771493911743, train_loss: 0.0027296722400933504\n",
            "42110 val_loss: 0.36869293451309204, train_loss: 0.0027873823419213295\n",
            "42120 val_loss: 0.37755629420280457, train_loss: 0.0026817091275006533\n",
            "42130 val_loss: 0.35913434624671936, train_loss: 0.0026301282923668623\n",
            "42140 val_loss: 0.3445580005645752, train_loss: 0.0028793730307370424\n",
            "42150 val_loss: 0.38372430205345154, train_loss: 0.0028688639868050814\n",
            "42160 val_loss: 0.37926986813545227, train_loss: 0.0027677593752741814\n",
            "42170 val_loss: 0.3612123131752014, train_loss: 0.0026354840956628323\n",
            "42180 val_loss: 0.3241737186908722, train_loss: 0.0028262052219361067\n",
            "42190 val_loss: 0.36840885877609253, train_loss: 0.003009170526638627\n",
            "42200 val_loss: 0.38521549105644226, train_loss: 0.0031144923996180296\n",
            "42210 val_loss: 0.3768843710422516, train_loss: 0.002929781563580036\n",
            "42220 val_loss: 0.3682340085506439, train_loss: 0.002687009982764721\n",
            "42230 val_loss: 0.35804620385169983, train_loss: 0.002641467610374093\n",
            "42240 val_loss: 0.37763506174087524, train_loss: 0.0031230472959578037\n",
            "42250 val_loss: 0.35646212100982666, train_loss: 0.002710453001782298\n",
            "42260 val_loss: 0.33002132177352905, train_loss: 0.0026610391214489937\n",
            "42270 val_loss: 0.3624088764190674, train_loss: 0.0026572502683848143\n",
            "42280 val_loss: 0.3744134306907654, train_loss: 0.0028467189986258745\n",
            "42290 val_loss: 0.3533647954463959, train_loss: 0.002669613342732191\n",
            "42300 val_loss: 0.39436495304107666, train_loss: 0.0031524053774774075\n",
            "42310 val_loss: 0.3633766770362854, train_loss: 0.0026742308400571346\n",
            "42320 val_loss: 0.35768094658851624, train_loss: 0.0026186166796833277\n",
            "42330 val_loss: 0.3601680397987366, train_loss: 0.0026904952246695757\n",
            "42340 val_loss: 0.36249443888664246, train_loss: 0.002679976634681225\n",
            "42350 val_loss: 0.3904205858707428, train_loss: 0.0032300674356520176\n",
            "42360 val_loss: 0.3487718999385834, train_loss: 0.0026678312569856644\n",
            "42370 val_loss: 0.3632901608943939, train_loss: 0.0027071661315858364\n",
            "42380 val_loss: 0.3446075916290283, train_loss: 0.0026872518938034773\n",
            "42390 val_loss: 0.3590278625488281, train_loss: 0.0025765718892216682\n",
            "42400 val_loss: 0.37602028250694275, train_loss: 0.0028135040774941444\n",
            "42410 val_loss: 0.34827721118927, train_loss: 0.0027962978929281235\n",
            "42420 val_loss: 0.338504821062088, train_loss: 0.002809162950143218\n",
            "42430 val_loss: 0.35988202691078186, train_loss: 0.0027998615987598896\n",
            "42440 val_loss: 0.360725075006485, train_loss: 0.0026996831875294447\n",
            "42450 val_loss: 0.3735922873020172, train_loss: 0.0027252822183072567\n",
            "42460 val_loss: 0.3748508095741272, train_loss: 0.002704468322917819\n",
            "42470 val_loss: 0.36982181668281555, train_loss: 0.002736844588071108\n",
            "42480 val_loss: 0.32798486948013306, train_loss: 0.0027003970462828875\n",
            "42490 val_loss: 0.3571924865245819, train_loss: 0.002664963249117136\n",
            "42500 val_loss: 0.33252379298210144, train_loss: 0.002700526500120759\n",
            "42510 val_loss: 0.3595854938030243, train_loss: 0.0028365442994982004\n",
            "42520 val_loss: 0.3514554500579834, train_loss: 0.002884653862565756\n",
            "42530 val_loss: 0.35225000977516174, train_loss: 0.0028363524470478296\n",
            "42540 val_loss: 0.35534048080444336, train_loss: 0.0027746567502617836\n",
            "42550 val_loss: 0.35888975858688354, train_loss: 0.0027309153228998184\n",
            "42560 val_loss: 0.36356204748153687, train_loss: 0.0027954010292887688\n",
            "42570 val_loss: 0.37866225838661194, train_loss: 0.0029402237851172686\n",
            "42580 val_loss: 0.36467012763023376, train_loss: 0.002692741807550192\n",
            "42590 val_loss: 0.3586715757846832, train_loss: 0.002680559642612934\n",
            "42600 val_loss: 0.3689998388290405, train_loss: 0.002768463222309947\n",
            "42610 val_loss: 0.3688756823539734, train_loss: 0.002587303752079606\n",
            "42620 val_loss: 0.36576953530311584, train_loss: 0.002563152927905321\n",
            "42630 val_loss: 0.3971654772758484, train_loss: 0.0033892225474119186\n",
            "42640 val_loss: 0.36148756742477417, train_loss: 0.002738598268479109\n",
            "42650 val_loss: 0.37437111139297485, train_loss: 0.0027422960847616196\n",
            "42660 val_loss: 0.3849605321884155, train_loss: 0.0029163528233766556\n",
            "42670 val_loss: 0.3415701687335968, train_loss: 0.0028029773384332657\n",
            "42680 val_loss: 0.34888768196105957, train_loss: 0.0027890880592167377\n",
            "42690 val_loss: 0.36035609245300293, train_loss: 0.002774222521111369\n",
            "42700 val_loss: 0.35398048162460327, train_loss: 0.0026736257132142782\n",
            "42710 val_loss: 0.3578203022480011, train_loss: 0.0026699563022702932\n",
            "42720 val_loss: 0.3649403750896454, train_loss: 0.002642750972881913\n",
            "42730 val_loss: 0.3732977509498596, train_loss: 0.0027587683871388435\n",
            "42740 val_loss: 0.35684487223625183, train_loss: 0.0025460177566856146\n",
            "42750 val_loss: 0.36368587613105774, train_loss: 0.0025267712771892548\n",
            "42760 val_loss: 0.40570876002311707, train_loss: 0.0027902284637093544\n",
            "42770 val_loss: 0.40148085355758667, train_loss: 0.002631839830428362\n",
            "42780 val_loss: 0.36720696091651917, train_loss: 0.0025692402850836515\n",
            "42790 val_loss: 0.37829285860061646, train_loss: 0.002869706368073821\n",
            "42800 val_loss: 0.35317739844322205, train_loss: 0.0026074887719005346\n",
            "42810 val_loss: 0.372119277715683, train_loss: 0.002572447992861271\n",
            "42820 val_loss: 0.3503280580043793, train_loss: 0.002558761974796653\n",
            "42830 val_loss: 0.3679659366607666, train_loss: 0.0030298226047307253\n",
            "42840 val_loss: 0.35571303963661194, train_loss: 0.0028011389076709747\n",
            "42850 val_loss: 0.36151114106178284, train_loss: 0.0027356543578207493\n",
            "42860 val_loss: 0.34787067770957947, train_loss: 0.002688213950023055\n",
            "42870 val_loss: 0.3587852120399475, train_loss: 0.0027006648015230894\n",
            "42880 val_loss: 0.38545626401901245, train_loss: 0.002616981975734234\n",
            "42890 val_loss: 0.3793693780899048, train_loss: 0.002670093672350049\n",
            "42900 val_loss: 0.3633759021759033, train_loss: 0.0026113581843674183\n",
            "42910 val_loss: 0.3623327314853668, train_loss: 0.002563642570748925\n",
            "42920 val_loss: 0.36273065209388733, train_loss: 0.0025049401447176933\n",
            "42930 val_loss: 0.34330683946609497, train_loss: 0.0024644348304718733\n",
            "42940 val_loss: 0.41005465388298035, train_loss: 0.003623630851507187\n",
            "42950 val_loss: 0.33947986364364624, train_loss: 0.0026373942382633686\n",
            "42960 val_loss: 0.36674684286117554, train_loss: 0.002614351222291589\n",
            "42970 val_loss: 0.3692032992839813, train_loss: 0.002473338507115841\n",
            "42980 val_loss: 0.39442121982574463, train_loss: 0.003229013178497553\n",
            "42990 val_loss: 0.3661932349205017, train_loss: 0.0027264736127108335\n",
            "43000 val_loss: 0.3378015160560608, train_loss: 0.0025368991773575544\n",
            "43010 val_loss: 0.3748210072517395, train_loss: 0.002510430058464408\n",
            "43020 val_loss: 0.3748432695865631, train_loss: 0.0025565377436578274\n",
            "43030 val_loss: 0.3544987738132477, train_loss: 0.0027651898562908173\n",
            "43040 val_loss: 0.36287105083465576, train_loss: 0.002436660462990403\n",
            "43050 val_loss: 0.3645433485507965, train_loss: 0.0024134917184710503\n",
            "43060 val_loss: 0.3614717721939087, train_loss: 0.002402055775746703\n",
            "43070 val_loss: 0.3720547556877136, train_loss: 0.0025297056417912245\n",
            "43080 val_loss: 0.36982953548431396, train_loss: 0.002391842193901539\n",
            "43090 val_loss: 0.3738148808479309, train_loss: 0.002476244932040572\n",
            "43100 val_loss: 0.36638230085372925, train_loss: 0.002439991571009159\n",
            "43110 val_loss: 0.367399126291275, train_loss: 0.0024960641749203205\n",
            "43120 val_loss: 0.3459750711917877, train_loss: 0.0025445783976465464\n",
            "43130 val_loss: 0.2808930277824402, train_loss: 0.0312674418091774\n",
            "43140 val_loss: 0.29991963505744934, train_loss: 0.008791990578174591\n",
            "43150 val_loss: 0.32241883873939514, train_loss: 0.008190997876226902\n",
            "43160 val_loss: 0.3353593051433563, train_loss: 0.007523050531744957\n",
            "43170 val_loss: 0.345308780670166, train_loss: 0.007134238723665476\n",
            "43180 val_loss: 0.3235916793346405, train_loss: 0.006473594810813665\n",
            "43190 val_loss: 0.33525434136390686, train_loss: 0.00621839752420783\n",
            "43200 val_loss: 0.34821954369544983, train_loss: 0.006066786590963602\n",
            "43210 val_loss: 0.3359978199005127, train_loss: 0.005532636307179928\n",
            "43220 val_loss: 0.35281822085380554, train_loss: 0.005384194198995829\n",
            "43230 val_loss: 0.3397683799266815, train_loss: 0.00483702914789319\n",
            "43240 val_loss: 0.3570966422557831, train_loss: 0.004580542910844088\n",
            "43250 val_loss: 0.33542031049728394, train_loss: 0.004075137432664633\n",
            "43260 val_loss: 0.3567397892475128, train_loss: 0.004058728460222483\n",
            "43270 val_loss: 0.35666435956954956, train_loss: 0.0038197676185518503\n",
            "43280 val_loss: 0.31518977880477905, train_loss: 0.0037028819788247347\n",
            "43290 val_loss: 0.3466030955314636, train_loss: 0.003627839032560587\n",
            "43300 val_loss: 0.34451550245285034, train_loss: 0.0036824296694248915\n",
            "43310 val_loss: 0.3778975307941437, train_loss: 0.004007265437394381\n",
            "43320 val_loss: 0.34873050451278687, train_loss: 0.0033079558052122593\n",
            "43330 val_loss: 0.35780373215675354, train_loss: 0.003309532068669796\n",
            "43340 val_loss: 0.3573850095272064, train_loss: 0.003184901550412178\n",
            "43350 val_loss: 0.3773435056209564, train_loss: 0.003417096333578229\n",
            "43360 val_loss: 0.35688358545303345, train_loss: 0.003058824222534895\n",
            "43370 val_loss: 0.3706679046154022, train_loss: 0.003192311618477106\n",
            "43380 val_loss: 0.3698440194129944, train_loss: 0.0031205567065626383\n",
            "43390 val_loss: 0.34101641178131104, train_loss: 0.0029107159934937954\n",
            "43400 val_loss: 0.3488563001155853, train_loss: 0.002840283792465925\n",
            "43410 val_loss: 0.35875263810157776, train_loss: 0.002784161129966378\n",
            "43420 val_loss: 0.32684066891670227, train_loss: 0.002779618604108691\n",
            "43430 val_loss: 0.3366367816925049, train_loss: 0.0027409298345446587\n",
            "43440 val_loss: 0.35712337493896484, train_loss: 0.002697734162211418\n",
            "43450 val_loss: 0.3375936448574066, train_loss: 0.00261564739048481\n",
            "43460 val_loss: 0.35797521471977234, train_loss: 0.00259369146078825\n",
            "43470 val_loss: 0.3665919303894043, train_loss: 0.0026527168229222298\n",
            "43480 val_loss: 0.36494481563568115, train_loss: 0.002593471435829997\n",
            "43490 val_loss: 0.36851274967193604, train_loss: 0.0026454865001142025\n",
            "43500 val_loss: 0.34766408801078796, train_loss: 0.002498760586604476\n",
            "43510 val_loss: 0.3607242703437805, train_loss: 0.0025090069975703955\n",
            "43520 val_loss: 0.39017948508262634, train_loss: 0.002446149941533804\n",
            "43530 val_loss: 0.3754739761352539, train_loss: 0.0024799355305731297\n",
            "43540 val_loss: 0.3574604094028473, train_loss: 0.002528215292841196\n",
            "43550 val_loss: 0.33496755361557007, train_loss: 0.0026113311760127544\n",
            "43560 val_loss: 0.3848169445991516, train_loss: 0.002755938796326518\n",
            "43570 val_loss: 0.363455206155777, train_loss: 0.0024734188336879015\n",
            "43580 val_loss: 0.3349570631980896, train_loss: 0.0025461725890636444\n",
            "43590 val_loss: 0.37018704414367676, train_loss: 0.0024770216550678015\n",
            "43600 val_loss: 0.35861915349960327, train_loss: 0.0024703226517885923\n",
            "43610 val_loss: 0.3303305208683014, train_loss: 0.002749974839389324\n",
            "43620 val_loss: 0.3772532641887665, train_loss: 0.002559836022555828\n",
            "43630 val_loss: 0.38771653175354004, train_loss: 0.0026218013372272253\n",
            "43640 val_loss: 0.402079313993454, train_loss: 0.002475174143910408\n",
            "43650 val_loss: 0.388275682926178, train_loss: 0.0025721180718392134\n",
            "43660 val_loss: 0.38239210844039917, train_loss: 0.002455363282933831\n",
            "43670 val_loss: 0.38259556889533997, train_loss: 0.0023685621563345194\n",
            "43680 val_loss: 0.37049156427383423, train_loss: 0.002323394874110818\n",
            "43690 val_loss: 0.36253538727760315, train_loss: 0.002306917915120721\n",
            "43700 val_loss: 0.3789865970611572, train_loss: 0.002356953453272581\n",
            "43710 val_loss: 0.3502582013607025, train_loss: 0.0023970655165612698\n",
            "43720 val_loss: 0.3501659631729126, train_loss: 0.002475766697898507\n",
            "43730 val_loss: 0.3550114631652832, train_loss: 0.0026011220179498196\n",
            "43740 val_loss: 0.3697941303253174, train_loss: 0.0025184007827192545\n",
            "43750 val_loss: 0.3714187443256378, train_loss: 0.0026060633827000856\n",
            "43760 val_loss: 0.3681880235671997, train_loss: 0.0025282802525907755\n",
            "43770 val_loss: 0.38798391819000244, train_loss: 0.002723636571317911\n",
            "43780 val_loss: 0.36150088906288147, train_loss: 0.0023730075918138027\n",
            "43790 val_loss: 0.371739000082016, train_loss: 0.0024625917430967093\n",
            "43800 val_loss: 0.3688037693500519, train_loss: 0.002418522024527192\n",
            "43810 val_loss: 0.36806854605674744, train_loss: 0.0023407430853694677\n",
            "43820 val_loss: 0.36598628759384155, train_loss: 0.0023445230908691883\n",
            "43830 val_loss: 0.3705059587955475, train_loss: 0.002404808532446623\n",
            "43840 val_loss: 0.3690040111541748, train_loss: 0.002304177498444915\n",
            "43850 val_loss: 0.3889707326889038, train_loss: 0.002623985055834055\n",
            "43860 val_loss: 0.3528042435646057, train_loss: 0.002372748451307416\n",
            "43870 val_loss: 0.37237289547920227, train_loss: 0.0024609381798654795\n",
            "43880 val_loss: 0.38441771268844604, train_loss: 0.002513087820261717\n",
            "43890 val_loss: 0.40657028555870056, train_loss: 0.002684745006263256\n",
            "43900 val_loss: 0.3728767931461334, train_loss: 0.0023800821509212255\n",
            "43910 val_loss: 0.37390533089637756, train_loss: 0.002346950815990567\n",
            "43920 val_loss: 0.3754441440105438, train_loss: 0.002498946851119399\n",
            "43930 val_loss: 0.3760084807872772, train_loss: 0.002801340539008379\n",
            "43940 val_loss: 0.38195809721946716, train_loss: 0.002529845340177417\n",
            "43950 val_loss: 0.3801571726799011, train_loss: 0.002398144919425249\n",
            "43960 val_loss: 0.3698619306087494, train_loss: 0.002401321893557906\n",
            "43970 val_loss: 0.36698782444000244, train_loss: 0.0024080986622720957\n",
            "43980 val_loss: 0.36466073989868164, train_loss: 0.0023648878559470177\n",
            "43990 val_loss: 0.3948531746864319, train_loss: 0.0023186183534562588\n",
            "44000 val_loss: 0.36155232787132263, train_loss: 0.0023250950034707785\n",
            "44010 val_loss: 0.3766934871673584, train_loss: 0.0023644815664738417\n",
            "44020 val_loss: 0.3428344130516052, train_loss: 0.0023641034495085478\n",
            "44030 val_loss: 0.367333322763443, train_loss: 0.002345229499042034\n",
            "44040 val_loss: 0.3383483588695526, train_loss: 0.0024440197739750147\n",
            "44050 val_loss: 0.4056844115257263, train_loss: 0.0026799559127539396\n",
            "44060 val_loss: 0.3755491077899933, train_loss: 0.002436538226902485\n",
            "44070 val_loss: 0.36883270740509033, train_loss: 0.0024563483893871307\n",
            "44080 val_loss: 0.44041478633880615, train_loss: 0.004883700981736183\n",
            "44090 val_loss: 0.38604936003685, train_loss: 0.002639510203152895\n",
            "44100 val_loss: 0.4194834232330322, train_loss: 0.003623127704486251\n",
            "44110 val_loss: 0.364215224981308, train_loss: 0.0024755073245614767\n",
            "44120 val_loss: 0.37990304827690125, train_loss: 0.002495463937520981\n",
            "44130 val_loss: 0.35696715116500854, train_loss: 0.002382613020017743\n",
            "44140 val_loss: 0.3643129765987396, train_loss: 0.0024942837189882994\n",
            "44150 val_loss: 0.42209893465042114, train_loss: 0.0023469985462725163\n",
            "44160 val_loss: 0.41207167506217957, train_loss: 0.0025765809696167707\n",
            "44170 val_loss: 0.4056832790374756, train_loss: 0.002719372743740678\n",
            "44180 val_loss: 0.38787510991096497, train_loss: 0.0023431202862411737\n",
            "44190 val_loss: 0.40335941314697266, train_loss: 0.002302824752405286\n",
            "44200 val_loss: 0.39598459005355835, train_loss: 0.0023358671460300684\n",
            "44210 val_loss: 0.3750934600830078, train_loss: 0.0027582638431340456\n",
            "44220 val_loss: 0.3762904703617096, train_loss: 0.0022876725997775793\n",
            "44230 val_loss: 0.3591848909854889, train_loss: 0.002237353939563036\n",
            "44240 val_loss: 0.4171999394893646, train_loss: 0.0028221902903169394\n",
            "44250 val_loss: 0.3715209662914276, train_loss: 0.0022922572679817677\n",
            "44260 val_loss: 0.40901827812194824, train_loss: 0.003221611026674509\n",
            "44270 val_loss: 0.36859285831451416, train_loss: 0.0025086128152906895\n",
            "44280 val_loss: 0.3625323176383972, train_loss: 0.0024497080594301224\n",
            "44290 val_loss: 0.3454287350177765, train_loss: 0.002405022270977497\n",
            "44300 val_loss: 0.3648895025253296, train_loss: 0.002361332532018423\n",
            "44310 val_loss: 0.3777180016040802, train_loss: 0.002381986705586314\n",
            "44320 val_loss: 0.3673597276210785, train_loss: 0.002276181010529399\n",
            "44330 val_loss: 0.35453474521636963, train_loss: 0.0022178995423018932\n",
            "44340 val_loss: 0.36748722195625305, train_loss: 0.00222227000631392\n",
            "44350 val_loss: 0.39274832606315613, train_loss: 0.0023888531140983105\n",
            "44360 val_loss: 0.38596802949905396, train_loss: 0.0023339150939136744\n",
            "44370 val_loss: 0.4041691720485687, train_loss: 0.002931085182353854\n",
            "44380 val_loss: 0.3808417320251465, train_loss: 0.002375275595113635\n",
            "44390 val_loss: 0.37473198771476746, train_loss: 0.002355111064389348\n",
            "44400 val_loss: 0.39953818917274475, train_loss: 0.0026540192775428295\n",
            "44410 val_loss: 0.38090771436691284, train_loss: 0.0023268740624189377\n",
            "44420 val_loss: 0.34829622507095337, train_loss: 0.002284961985424161\n",
            "44430 val_loss: 0.36439669132232666, train_loss: 0.0022487116511911154\n",
            "44440 val_loss: 0.3874186873435974, train_loss: 0.0022898728493601084\n",
            "44450 val_loss: 0.3526712656021118, train_loss: 0.0026439230423420668\n",
            "44460 val_loss: 0.3951130509376526, train_loss: 0.002450284780934453\n",
            "44470 val_loss: 0.406561940908432, train_loss: 0.002608114155009389\n",
            "44480 val_loss: 0.38982895016670227, train_loss: 0.0024011442437767982\n",
            "44490 val_loss: 0.3821841776371002, train_loss: 0.0022106526885181665\n",
            "44500 val_loss: 0.3393045663833618, train_loss: 0.0023975414223968983\n",
            "44510 val_loss: 0.3801182806491852, train_loss: 0.0023511177860200405\n",
            "44520 val_loss: 0.38651615381240845, train_loss: 0.00223679281771183\n",
            "44530 val_loss: 0.384000688791275, train_loss: 0.002294785575941205\n",
            "44540 val_loss: 0.37089622020721436, train_loss: 0.0022356354165822268\n",
            "44550 val_loss: 0.3714468479156494, train_loss: 0.0022496527526527643\n",
            "44560 val_loss: 0.4318249225616455, train_loss: 0.00267230486497283\n",
            "44570 val_loss: 0.39297041296958923, train_loss: 0.002275884384289384\n",
            "44580 val_loss: 0.4060002267360687, train_loss: 0.0026507778093218803\n",
            "44590 val_loss: 0.3898296356201172, train_loss: 0.0023204556200653315\n",
            "44600 val_loss: 0.3899225890636444, train_loss: 0.0024989687371999025\n",
            "44610 val_loss: 0.3949749767780304, train_loss: 0.0026088275481015444\n",
            "44620 val_loss: 0.3679938316345215, train_loss: 0.0024142591282725334\n",
            "44630 val_loss: 0.38501980900764465, train_loss: 0.0024689887650310993\n",
            "44640 val_loss: 0.35720983147621155, train_loss: 0.002293088473379612\n",
            "44650 val_loss: 0.32449477910995483, train_loss: 0.0027104257605969906\n",
            "44660 val_loss: 0.3895232379436493, train_loss: 0.0026500271633267403\n",
            "44670 val_loss: 0.40077659487724304, train_loss: 0.002819762332364917\n",
            "44680 val_loss: 0.37039482593536377, train_loss: 0.0024027181789278984\n",
            "44690 val_loss: 0.3689612150192261, train_loss: 0.0024034082889556885\n",
            "44700 val_loss: 0.40103641152381897, train_loss: 0.0026962957344949245\n",
            "44710 val_loss: 0.38802409172058105, train_loss: 0.00242601428180933\n",
            "44720 val_loss: 0.3606647253036499, train_loss: 0.00237894244492054\n",
            "44730 val_loss: 0.38306427001953125, train_loss: 0.0024542927276343107\n",
            "44740 val_loss: 0.37076300382614136, train_loss: 0.0023477920331060886\n",
            "44750 val_loss: 0.3855666518211365, train_loss: 0.002399920718744397\n",
            "44760 val_loss: 0.3980652093887329, train_loss: 0.0024702611844986677\n",
            "44770 val_loss: 0.3634186089038849, train_loss: 0.002297753933817148\n",
            "44780 val_loss: 0.41579383611679077, train_loss: 0.003008094150573015\n",
            "44790 val_loss: 0.38102230429649353, train_loss: 0.0022944954689592123\n",
            "44800 val_loss: 0.37972187995910645, train_loss: 0.002353291492909193\n",
            "44810 val_loss: 0.3703315556049347, train_loss: 0.002258020918816328\n",
            "44820 val_loss: 0.3981749713420868, train_loss: 0.002502565737813711\n",
            "44830 val_loss: 0.39767029881477356, train_loss: 0.0024908920750021935\n",
            "44840 val_loss: 0.4146634042263031, train_loss: 0.0025920162443071604\n",
            "44850 val_loss: 0.35397282242774963, train_loss: 0.002660186728462577\n",
            "44860 val_loss: 0.40483251214027405, train_loss: 0.0025451367255300283\n",
            "44870 val_loss: 0.3959276080131531, train_loss: 0.0025570925790816545\n",
            "44880 val_loss: 0.3966386020183563, train_loss: 0.002571726217865944\n",
            "44890 val_loss: 0.3858172595500946, train_loss: 0.0025167339481413364\n",
            "44900 val_loss: 0.3735567033290863, train_loss: 0.00241759791970253\n",
            "44910 val_loss: 0.3676542043685913, train_loss: 0.0023311700206249952\n",
            "44920 val_loss: 0.3983629643917084, train_loss: 0.002347637200728059\n",
            "44930 val_loss: 0.38648954033851624, train_loss: 0.0023647842463105917\n",
            "44940 val_loss: 0.38341355323791504, train_loss: 0.0024255819153040648\n",
            "44950 val_loss: 0.3913137912750244, train_loss: 0.002424471778795123\n",
            "44960 val_loss: 0.3590441644191742, train_loss: 0.0022719886619597673\n",
            "44970 val_loss: 0.3685413897037506, train_loss: 0.002347078174352646\n",
            "44980 val_loss: 0.3879750967025757, train_loss: 0.0024582131300121546\n",
            "44990 val_loss: 0.3625326454639435, train_loss: 0.0034637628123164177\n",
            "45000 val_loss: 0.42835476994514465, train_loss: 0.0035035519395023584\n",
            "45010 val_loss: 0.3986237049102783, train_loss: 0.002668969798833132\n",
            "45020 val_loss: 0.36775314807891846, train_loss: 0.0024150197859853506\n",
            "45030 val_loss: 0.38104337453842163, train_loss: 0.0024072364903986454\n",
            "45040 val_loss: 0.36980098485946655, train_loss: 0.0024462766014039516\n",
            "45050 val_loss: 0.36557894945144653, train_loss: 0.002451274311169982\n",
            "45060 val_loss: 0.3797834813594818, train_loss: 0.0024360818788409233\n",
            "45070 val_loss: 0.3813500702381134, train_loss: 0.002394587965682149\n",
            "45080 val_loss: 0.3795585036277771, train_loss: 0.002359546022489667\n",
            "45090 val_loss: 0.396091103553772, train_loss: 0.0024878547992557287\n",
            "45100 val_loss: 0.39112621545791626, train_loss: 0.0024298676289618015\n",
            "45110 val_loss: 0.37308230996131897, train_loss: 0.0023292608093470335\n",
            "45120 val_loss: 0.3953790068626404, train_loss: 0.0023597674444317818\n",
            "45130 val_loss: 0.3853866457939148, train_loss: 0.0023014815524220467\n",
            "45140 val_loss: 0.4025352895259857, train_loss: 0.0024554880801588297\n",
            "45150 val_loss: 0.39551466703414917, train_loss: 0.0023154043592512608\n",
            "45160 val_loss: 0.38314300775527954, train_loss: 0.002268111100420356\n",
            "45170 val_loss: 0.3681319057941437, train_loss: 0.002237164881080389\n",
            "45180 val_loss: 0.36936721205711365, train_loss: 0.0022047562524676323\n",
            "45190 val_loss: 0.40113189816474915, train_loss: 0.002440491458401084\n",
            "45200 val_loss: 0.3829844892024994, train_loss: 0.0023342063650488853\n",
            "45210 val_loss: 0.38925865292549133, train_loss: 0.002247306052595377\n",
            "45220 val_loss: 0.40788978338241577, train_loss: 0.0025932402350008488\n",
            "45230 val_loss: 0.4061906337738037, train_loss: 0.0023558230604976416\n",
            "45240 val_loss: 0.3951924741268158, train_loss: 0.002246108604595065\n",
            "45250 val_loss: 0.3833117187023163, train_loss: 0.002342573832720518\n",
            "45260 val_loss: 0.3826361298561096, train_loss: 0.002364694606512785\n",
            "45270 val_loss: 0.39162763953208923, train_loss: 0.0024122786708176136\n",
            "45280 val_loss: 0.37249869108200073, train_loss: 0.002329490380361676\n",
            "45290 val_loss: 0.4047115445137024, train_loss: 0.0025011368561536074\n",
            "45300 val_loss: 0.37479597330093384, train_loss: 0.0022497756872326136\n",
            "45310 val_loss: 0.3984549045562744, train_loss: 0.002391527406871319\n",
            "45320 val_loss: 0.38889139890670776, train_loss: 0.0022338884882628918\n",
            "45330 val_loss: 0.38341450691223145, train_loss: 0.0021619005128741264\n",
            "45340 val_loss: 0.4010609984397888, train_loss: 0.002395184012129903\n",
            "45350 val_loss: 0.36234134435653687, train_loss: 0.00216552522033453\n",
            "45360 val_loss: 0.34835293889045715, train_loss: 0.002979811280965805\n",
            "45370 val_loss: 0.402754008769989, train_loss: 0.0026710894890129566\n",
            "45380 val_loss: 0.39079752564430237, train_loss: 0.0024870557244867086\n",
            "45390 val_loss: 0.38255277276039124, train_loss: 0.0023577192332595587\n",
            "45400 val_loss: 0.3903440237045288, train_loss: 0.0023961281403899193\n",
            "45410 val_loss: 0.3966549038887024, train_loss: 0.0023422040976583958\n",
            "45420 val_loss: 0.400808185338974, train_loss: 0.002325569512322545\n",
            "45430 val_loss: 0.4087427258491516, train_loss: 0.0023310359101742506\n",
            "45440 val_loss: 0.37636667490005493, train_loss: 0.00223780469968915\n",
            "45450 val_loss: 0.3682212233543396, train_loss: 0.0022523808293044567\n",
            "45460 val_loss: 0.3990129232406616, train_loss: 0.0023745098151266575\n",
            "45470 val_loss: 0.4131210744380951, train_loss: 0.0025830063968896866\n",
            "45480 val_loss: 0.3928544521331787, train_loss: 0.002285714726895094\n",
            "45490 val_loss: 0.44024771451950073, train_loss: 0.0034341902937740088\n",
            "45500 val_loss: 0.3920976519584656, train_loss: 0.002249898621812463\n",
            "45510 val_loss: 0.36764436960220337, train_loss: 0.002190368017181754\n",
            "45520 val_loss: 0.37549498677253723, train_loss: 0.0021738309878855944\n",
            "45530 val_loss: 0.38076525926589966, train_loss: 0.0021301822271198034\n",
            "45540 val_loss: 0.37814241647720337, train_loss: 0.0021133057307451963\n",
            "45550 val_loss: 0.37778428196907043, train_loss: 0.0020802002400159836\n",
            "45560 val_loss: 0.3755826950073242, train_loss: 0.00205364846624434\n",
            "45570 val_loss: 0.39364805817604065, train_loss: 0.0021457821130752563\n",
            "45580 val_loss: 0.37592795491218567, train_loss: 0.002059475751593709\n",
            "45590 val_loss: 0.3921986520290375, train_loss: 0.002091116737574339\n",
            "45600 val_loss: 0.41031011939048767, train_loss: 0.002186916070058942\n",
            "45610 val_loss: 0.3451727032661438, train_loss: 0.0024335237685590982\n",
            "45620 val_loss: 0.37206631898880005, train_loss: 0.002250758931040764\n",
            "45630 val_loss: 0.3838563561439514, train_loss: 0.0022649988532066345\n",
            "45640 val_loss: 0.4086013436317444, train_loss: 0.0025371972005814314\n",
            "45650 val_loss: 0.3744141459465027, train_loss: 0.002317871432751417\n",
            "45660 val_loss: 0.4008667469024658, train_loss: 0.0024164896458387375\n",
            "45670 val_loss: 0.35142356157302856, train_loss: 0.00235475180670619\n",
            "45680 val_loss: 0.3840796649456024, train_loss: 0.0023400275968015194\n",
            "45690 val_loss: 0.3854481279850006, train_loss: 0.002310696989297867\n",
            "45700 val_loss: 0.38531357049942017, train_loss: 0.002231536665931344\n",
            "45710 val_loss: 0.4090200960636139, train_loss: 0.0022588795982301235\n",
            "45720 val_loss: 0.42294979095458984, train_loss: 0.0029372728895395994\n",
            "45730 val_loss: 0.3928018808364868, train_loss: 0.0024155464489012957\n",
            "45740 val_loss: 0.3778712749481201, train_loss: 0.0023055346682667732\n",
            "45750 val_loss: 0.3718811571598053, train_loss: 0.00226779212243855\n",
            "45760 val_loss: 0.393126904964447, train_loss: 0.0022754138335585594\n",
            "45770 val_loss: 0.3991389572620392, train_loss: 0.002319733379408717\n",
            "45780 val_loss: 0.37605026364326477, train_loss: 0.0023028480354696512\n",
            "45790 val_loss: 0.4058121144771576, train_loss: 0.0024503078311681747\n",
            "45800 val_loss: 0.40021616220474243, train_loss: 0.002382736187428236\n",
            "45810 val_loss: 0.3967796862125397, train_loss: 0.0022969285491853952\n",
            "45820 val_loss: 0.3924524188041687, train_loss: 0.0022104361560195684\n",
            "45830 val_loss: 0.39396727085113525, train_loss: 0.0022028086241334677\n",
            "45840 val_loss: 0.43565723299980164, train_loss: 0.00309581751935184\n",
            "45850 val_loss: 0.3694867789745331, train_loss: 0.0022752804215997458\n",
            "45860 val_loss: 0.3896031975746155, train_loss: 0.0022700063418596983\n",
            "45870 val_loss: 0.3964565396308899, train_loss: 0.0023233953397721052\n",
            "45880 val_loss: 0.3877100348472595, train_loss: 0.002201882191002369\n",
            "45890 val_loss: 0.39910829067230225, train_loss: 0.002268873155117035\n",
            "45900 val_loss: 0.37418580055236816, train_loss: 0.0021604320500046015\n",
            "45910 val_loss: 0.3925279974937439, train_loss: 0.0021697725169360638\n",
            "45920 val_loss: 0.4075681269168854, train_loss: 0.0022770920768380165\n",
            "45930 val_loss: 0.4010777175426483, train_loss: 0.002219337737187743\n",
            "45940 val_loss: 0.3828631043434143, train_loss: 0.002134238835424185\n",
            "45950 val_loss: 0.40103504061698914, train_loss: 0.002195331733673811\n",
            "45960 val_loss: 0.38782230019569397, train_loss: 0.0020366173703223467\n",
            "45970 val_loss: 0.3848766088485718, train_loss: 0.0021637259051203728\n",
            "45980 val_loss: 0.38834911584854126, train_loss: 0.0021518035791814327\n",
            "45990 val_loss: 0.3904308080673218, train_loss: 0.0021281735971570015\n",
            "46000 val_loss: 0.3911046087741852, train_loss: 0.002126094186678529\n",
            "46010 val_loss: 0.3922603726387024, train_loss: 0.0020930636674165726\n",
            "46020 val_loss: 0.43561846017837524, train_loss: 0.0028931705746799707\n",
            "46030 val_loss: 0.3622577488422394, train_loss: 0.002235724590718746\n",
            "46040 val_loss: 0.37222084403038025, train_loss: 0.0021994139533489943\n",
            "46050 val_loss: 0.4094527065753937, train_loss: 0.0023942773696035147\n",
            "46060 val_loss: 0.37671715021133423, train_loss: 0.0021541148889809847\n",
            "46070 val_loss: 0.3755528926849365, train_loss: 0.00210685096681118\n",
            "46080 val_loss: 0.3901178240776062, train_loss: 0.0021045070607215166\n",
            "46090 val_loss: 0.4283592104911804, train_loss: 0.002043353859335184\n",
            "46100 val_loss: 0.4001197814941406, train_loss: 0.0022012104745954275\n",
            "46110 val_loss: 0.39326995611190796, train_loss: 0.0021903442684561014\n",
            "46120 val_loss: 0.3844250738620758, train_loss: 0.00216718390583992\n",
            "46130 val_loss: 0.38319993019104004, train_loss: 0.002213180297985673\n",
            "46140 val_loss: 0.40238499641418457, train_loss: 0.0022752073127776384\n",
            "46150 val_loss: 0.41829827427864075, train_loss: 0.0022095893509685993\n",
            "46160 val_loss: 0.3992977738380432, train_loss: 0.002186600584536791\n",
            "46170 val_loss: 0.3855866491794586, train_loss: 0.0022237785160541534\n",
            "46180 val_loss: 0.36094021797180176, train_loss: 0.0021367031149566174\n",
            "46190 val_loss: 0.3962855637073517, train_loss: 0.0021961985621601343\n",
            "46200 val_loss: 0.38502079248428345, train_loss: 0.0021096793934702873\n",
            "46210 val_loss: 0.3959849774837494, train_loss: 0.0021068111527711153\n",
            "46220 val_loss: 0.43806949257850647, train_loss: 0.00281019345857203\n",
            "46230 val_loss: 0.40412887930870056, train_loss: 0.002050970681011677\n",
            "46240 val_loss: 0.40969035029411316, train_loss: 0.002186743775382638\n",
            "46250 val_loss: 0.41247984766960144, train_loss: 0.0021878278348594904\n",
            "46260 val_loss: 0.4086487591266632, train_loss: 0.0022177668288350105\n",
            "46270 val_loss: 0.37698519229888916, train_loss: 0.002088304143399\n",
            "46280 val_loss: 0.39071446657180786, train_loss: 0.0021443176083266735\n",
            "46290 val_loss: 0.3959999680519104, train_loss: 0.0020980914123356342\n",
            "46300 val_loss: 0.4054943323135376, train_loss: 0.002236708765849471\n",
            "46310 val_loss: 0.38441213965415955, train_loss: 0.00205924897454679\n",
            "46320 val_loss: 0.4041084945201874, train_loss: 0.002116719027981162\n",
            "46330 val_loss: 0.40231719613075256, train_loss: 0.002120742341503501\n",
            "46340 val_loss: 0.3888145983219147, train_loss: 0.0019988883286714554\n",
            "46350 val_loss: 0.39758750796318054, train_loss: 0.0019606214482337236\n",
            "46360 val_loss: 0.4416521489620209, train_loss: 0.0027543685864657164\n",
            "46370 val_loss: 0.390373170375824, train_loss: 0.002046925714239478\n",
            "46380 val_loss: 0.4115937054157257, train_loss: 0.0021797537337988615\n",
            "46390 val_loss: 0.3822403848171234, train_loss: 0.0019809033256024122\n",
            "46400 val_loss: 0.42454227805137634, train_loss: 0.002441995544359088\n",
            "46410 val_loss: 0.4585559368133545, train_loss: 0.003941897302865982\n",
            "46420 val_loss: 0.3812071979045868, train_loss: 0.002095830161124468\n",
            "46430 val_loss: 0.41417571902275085, train_loss: 0.002124323509633541\n",
            "46440 val_loss: 0.42633602023124695, train_loss: 0.0021804706193506718\n",
            "46450 val_loss: 0.3991144299507141, train_loss: 0.0021266601979732513\n",
            "46460 val_loss: 0.4039129912853241, train_loss: 0.002296611899510026\n",
            "46470 val_loss: 0.39066827297210693, train_loss: 0.0022455649450421333\n",
            "46480 val_loss: 0.38679540157318115, train_loss: 0.0022135560866445303\n",
            "46490 val_loss: 0.406690776348114, train_loss: 0.002405199222266674\n",
            "46500 val_loss: 0.4101468026638031, train_loss: 0.0022522713989019394\n",
            "46510 val_loss: 0.39226505160331726, train_loss: 0.002202663104981184\n",
            "46520 val_loss: 0.39267975091934204, train_loss: 0.002259215572848916\n",
            "46530 val_loss: 0.4034789800643921, train_loss: 0.002240872010588646\n",
            "46540 val_loss: 0.4052938222885132, train_loss: 0.002314506098628044\n",
            "46550 val_loss: 0.41449519991874695, train_loss: 0.002248940523713827\n",
            "46560 val_loss: 0.4158605933189392, train_loss: 0.0023353530559688807\n",
            "46570 val_loss: 0.39082300662994385, train_loss: 0.002070909831672907\n",
            "46580 val_loss: 0.3871076703071594, train_loss: 0.002069292590022087\n",
            "46590 val_loss: 0.3842709958553314, train_loss: 0.002085216110572219\n",
            "46600 val_loss: 0.3921080231666565, train_loss: 0.002070700516924262\n",
            "46610 val_loss: 0.4060867726802826, train_loss: 0.0021136649884283543\n",
            "46620 val_loss: 0.38796886801719666, train_loss: 0.0020598445553332567\n",
            "46630 val_loss: 0.39556440711021423, train_loss: 0.002043439308181405\n",
            "46640 val_loss: 0.4001259207725525, train_loss: 0.0020217907149344683\n",
            "46650 val_loss: 0.398460328578949, train_loss: 0.0020849674474447966\n",
            "46660 val_loss: 0.4378837049007416, train_loss: 0.0027227369137108326\n",
            "46670 val_loss: 0.38394540548324585, train_loss: 0.002003695350140333\n",
            "46680 val_loss: 0.44077813625335693, train_loss: 0.0029547414742410183\n",
            "46690 val_loss: 0.41155725717544556, train_loss: 0.002223193645477295\n",
            "46700 val_loss: 0.43187272548675537, train_loss: 0.0020384294912219048\n",
            "46710 val_loss: 0.4307156503200531, train_loss: 0.0021043422166258097\n",
            "46720 val_loss: 0.422848641872406, train_loss: 0.0022586577106267214\n",
            "46730 val_loss: 0.42643314599990845, train_loss: 0.0021681981161236763\n",
            "46740 val_loss: 0.4463996887207031, train_loss: 0.0030288281850516796\n",
            "46750 val_loss: 0.40587761998176575, train_loss: 0.002142227254807949\n",
            "46760 val_loss: 0.3747188150882721, train_loss: 0.00220475229434669\n",
            "46770 val_loss: 0.4004737436771393, train_loss: 0.0023179801646620035\n",
            "46780 val_loss: 0.4041259288787842, train_loss: 0.00229590549133718\n",
            "46790 val_loss: 0.4079104959964752, train_loss: 0.0023238849826157093\n",
            "46800 val_loss: 0.35993942618370056, train_loss: 0.0021954101976007223\n",
            "46810 val_loss: 0.3806016445159912, train_loss: 0.0022228537127375603\n",
            "46820 val_loss: 0.3991820514202118, train_loss: 0.00217129266820848\n",
            "46830 val_loss: 0.40756431221961975, train_loss: 0.0022632938344031572\n",
            "46840 val_loss: 0.394517719745636, train_loss: 0.0021878548432141542\n",
            "46850 val_loss: 0.410903662443161, train_loss: 0.002164574572816491\n",
            "46860 val_loss: 0.4103054404258728, train_loss: 0.0022351613733917475\n",
            "46870 val_loss: 0.4165898859500885, train_loss: 0.002208670135587454\n",
            "46880 val_loss: 0.38210758566856384, train_loss: 0.0021116326097398996\n",
            "46890 val_loss: 0.4576667547225952, train_loss: 0.0037040135357528925\n",
            "46900 val_loss: 0.41530147194862366, train_loss: 0.0022629383020102978\n",
            "46910 val_loss: 0.3796170651912689, train_loss: 0.0020922936964780092\n",
            "46920 val_loss: 0.3973122537136078, train_loss: 0.002108439803123474\n",
            "46930 val_loss: 0.40106019377708435, train_loss: 0.0021319901570677757\n",
            "46940 val_loss: 0.3932797908782959, train_loss: 0.002004822948947549\n",
            "46950 val_loss: 0.4330616891384125, train_loss: 0.001999635249376297\n",
            "46960 val_loss: 0.40637221932411194, train_loss: 0.0019663998391479254\n",
            "46970 val_loss: 0.44897550344467163, train_loss: 0.0020938925445079803\n",
            "46980 val_loss: 0.3925851285457611, train_loss: 0.002061310922726989\n",
            "46990 val_loss: 0.4175449013710022, train_loss: 0.0022126652766019106\n",
            "47000 val_loss: 0.397879421710968, train_loss: 0.0020547802560031414\n",
            "47010 val_loss: 0.3891860544681549, train_loss: 0.0020195276010781527\n",
            "47020 val_loss: 0.40960127115249634, train_loss: 0.0020752090495079756\n",
            "47030 val_loss: 0.39550432562828064, train_loss: 0.001961903180927038\n",
            "47040 val_loss: 0.39197608828544617, train_loss: 0.0019212202169001102\n",
            "47050 val_loss: 0.3866523206233978, train_loss: 0.0020556470844894648\n",
            "47060 val_loss: 0.39115843176841736, train_loss: 0.002011852106079459\n",
            "47070 val_loss: 0.38671624660491943, train_loss: 0.0020642501767724752\n",
            "47080 val_loss: 0.38899531960487366, train_loss: 0.0020264750346541405\n",
            "47090 val_loss: 0.38823673129081726, train_loss: 0.0020405675750225782\n",
            "47100 val_loss: 0.4640495181083679, train_loss: 0.0038532039616256952\n",
            "47110 val_loss: 0.4414142370223999, train_loss: 0.0019496346358209848\n",
            "47120 val_loss: 0.4036153256893158, train_loss: 0.0019903180655092\n",
            "47130 val_loss: 0.4054255187511444, train_loss: 0.00203119614161551\n",
            "47140 val_loss: 0.4011000990867615, train_loss: 0.001988816075026989\n",
            "47150 val_loss: 0.3973256051540375, train_loss: 0.0020402579102665186\n",
            "47160 val_loss: 0.39237117767333984, train_loss: 0.0019189674640074372\n",
            "47170 val_loss: 0.3921559154987335, train_loss: 0.0019697060342878103\n",
            "47180 val_loss: 0.4172169268131256, train_loss: 0.0021047648042440414\n",
            "47190 val_loss: 0.4211790859699249, train_loss: 0.002020676154643297\n",
            "47200 val_loss: 0.4243971109390259, train_loss: 0.0024036180693656206\n",
            "47210 val_loss: 0.43892255425453186, train_loss: 0.0021541768219321966\n",
            "47220 val_loss: 0.4305407702922821, train_loss: 0.0021815933287143707\n",
            "47230 val_loss: 0.4101402163505554, train_loss: 0.0020629491191357374\n",
            "47240 val_loss: 0.37484419345855713, train_loss: 0.0020701263565570116\n",
            "47250 val_loss: 0.41724592447280884, train_loss: 0.0022910817060619593\n",
            "47260 val_loss: 0.4205792248249054, train_loss: 0.002147669205442071\n",
            "47270 val_loss: 0.4156807065010071, train_loss: 0.0020139296539127827\n",
            "47280 val_loss: 0.40941429138183594, train_loss: 0.0020288608502596617\n",
            "47290 val_loss: 0.4201485216617584, train_loss: 0.002363199135288596\n",
            "47300 val_loss: 0.40832123160362244, train_loss: 0.0022007992956787348\n",
            "47310 val_loss: 0.43334901332855225, train_loss: 0.002427341416478157\n",
            "47320 val_loss: 0.4107695519924164, train_loss: 0.002113274997100234\n",
            "47330 val_loss: 0.40891194343566895, train_loss: 0.002092877170071006\n",
            "47340 val_loss: 0.40386754274368286, train_loss: 0.002038574079051614\n",
            "47350 val_loss: 0.4485020935535431, train_loss: 0.002828286960721016\n",
            "47360 val_loss: 0.3951382040977478, train_loss: 0.002032126300036907\n",
            "47370 val_loss: 0.4056221544742584, train_loss: 0.00203452305868268\n",
            "47380 val_loss: 0.38386666774749756, train_loss: 0.0020801133941859007\n",
            "47390 val_loss: 0.37876513600349426, train_loss: 0.00200859853066504\n",
            "47400 val_loss: 0.38130778074264526, train_loss: 0.002009328920394182\n",
            "47410 val_loss: 0.4060235619544983, train_loss: 0.0020884808618575335\n",
            "47420 val_loss: 0.41745877265930176, train_loss: 0.002214074367657304\n",
            "47430 val_loss: 0.4137369692325592, train_loss: 0.002112479181960225\n",
            "47440 val_loss: 0.38317617774009705, train_loss: 0.0019675828516483307\n",
            "47450 val_loss: 0.4009025990962982, train_loss: 0.002032293938100338\n",
            "47460 val_loss: 0.43033450841903687, train_loss: 0.0022769803181290627\n",
            "47470 val_loss: 0.40359774231910706, train_loss: 0.001969709526747465\n",
            "47480 val_loss: 0.4306941628456116, train_loss: 0.0019561706576496363\n",
            "47490 val_loss: 0.40147721767425537, train_loss: 0.016689574345946312\n",
            "47500 val_loss: 0.38340455293655396, train_loss: 0.0021548825316131115\n",
            "47510 val_loss: 0.3887220025062561, train_loss: 0.0021283377427607775\n",
            "47520 val_loss: 0.4151626229286194, train_loss: 0.002254786901175976\n",
            "47530 val_loss: 0.4007580280303955, train_loss: 0.0020346776582300663\n",
            "47540 val_loss: 0.3923813998699188, train_loss: 0.0019930691923946142\n",
            "47550 val_loss: 0.48665332794189453, train_loss: 0.0020143811125308275\n",
            "47560 val_loss: 0.3894679546356201, train_loss: 0.002080996986478567\n",
            "47570 val_loss: 0.38651812076568604, train_loss: 0.002033458324149251\n",
            "47580 val_loss: 0.4975307583808899, train_loss: 0.002126162638887763\n",
            "47590 val_loss: 0.5030129551887512, train_loss: 0.001982631627470255\n",
            "47600 val_loss: 0.4684385061264038, train_loss: 0.0021080218721181154\n",
            "47610 val_loss: 0.442305326461792, train_loss: 0.0020010131411254406\n",
            "47620 val_loss: 0.43051719665527344, train_loss: 0.0020339544862508774\n",
            "47630 val_loss: 0.4002518057823181, train_loss: 0.0019535135943442583\n",
            "47640 val_loss: 0.42177286744117737, train_loss: 0.0021806599106639624\n",
            "47650 val_loss: 0.37515830993652344, train_loss: 0.0019833955448120832\n",
            "47660 val_loss: 0.4008060097694397, train_loss: 0.002018487546592951\n",
            "47670 val_loss: 0.42360395193099976, train_loss: 0.002255033003166318\n",
            "47680 val_loss: 0.3982287347316742, train_loss: 0.0019807068165391684\n",
            "47690 val_loss: 0.391960084438324, train_loss: 0.0019057232420891523\n",
            "47700 val_loss: 0.41391807794570923, train_loss: 0.0019826553761959076\n",
            "47710 val_loss: 0.38622942566871643, train_loss: 0.0018898118287324905\n",
            "47720 val_loss: 0.4050716757774353, train_loss: 0.0019623897969722748\n",
            "47730 val_loss: 0.4125698506832123, train_loss: 0.001989156473428011\n",
            "47740 val_loss: 0.3976452648639679, train_loss: 0.0019003951456397772\n",
            "47750 val_loss: 0.43369215726852417, train_loss: 0.0021851910278201103\n",
            "47760 val_loss: 0.37296441197395325, train_loss: 0.00198022136464715\n",
            "47770 val_loss: 0.4299898147583008, train_loss: 0.002453649416565895\n",
            "47780 val_loss: 0.4368472695350647, train_loss: 0.0021163199562579393\n",
            "47790 val_loss: 0.4241486191749573, train_loss: 0.002007054630666971\n",
            "47800 val_loss: 0.3963695764541626, train_loss: 0.0021626364905387163\n",
            "47810 val_loss: 0.4001358151435852, train_loss: 0.001986040035262704\n",
            "47820 val_loss: 0.3977678716182709, train_loss: 0.0019407818326726556\n",
            "47830 val_loss: 0.4263083040714264, train_loss: 0.0019888184033334255\n",
            "47840 val_loss: 0.4048926830291748, train_loss: 0.0019485129741951823\n",
            "47850 val_loss: 0.5011487603187561, train_loss: 0.006531659048050642\n",
            "47860 val_loss: 0.41642260551452637, train_loss: 0.0019990112632513046\n",
            "47870 val_loss: 0.3956170678138733, train_loss: 0.0018561759497970343\n",
            "47880 val_loss: 0.42861971259117126, train_loss: 0.002180171897634864\n",
            "47890 val_loss: 0.43909990787506104, train_loss: 0.0020118204411119223\n",
            "47900 val_loss: 0.4906998574733734, train_loss: 0.0019250035984441638\n",
            "47910 val_loss: 0.425545334815979, train_loss: 0.0021102740429341793\n",
            "47920 val_loss: 0.4206509292125702, train_loss: 0.0020396264735609293\n",
            "47930 val_loss: 0.41403692960739136, train_loss: 0.0020008713472634554\n",
            "47940 val_loss: 0.4152703583240509, train_loss: 0.001967834308743477\n",
            "47950 val_loss: 0.38566604256629944, train_loss: 0.0018906979821622372\n",
            "47960 val_loss: 0.407979279756546, train_loss: 0.002065054839476943\n",
            "47970 val_loss: 0.3904532790184021, train_loss: 0.0019277628744021058\n",
            "47980 val_loss: 0.4054732620716095, train_loss: 0.0019871743861585855\n",
            "47990 val_loss: 0.43786463141441345, train_loss: 0.002249796874821186\n",
            "48000 val_loss: 0.39533916115760803, train_loss: 0.0018925247713923454\n",
            "48010 val_loss: 0.4252379238605499, train_loss: 0.0019463583594188094\n",
            "48020 val_loss: 0.41641488671302795, train_loss: 0.001902989693917334\n",
            "48030 val_loss: 0.38789618015289307, train_loss: 0.0018336503999307752\n",
            "48040 val_loss: 0.412034273147583, train_loss: 0.0018438086844980717\n",
            "48050 val_loss: 0.41258376836776733, train_loss: 0.0018931645900011063\n",
            "48060 val_loss: 0.4017835259437561, train_loss: 0.001829837099649012\n",
            "48070 val_loss: 0.40354642271995544, train_loss: 0.0018177585443481803\n",
            "48080 val_loss: 0.4193350076675415, train_loss: 0.0019396682037040591\n",
            "48090 val_loss: 0.38777032494544983, train_loss: 0.001813261304050684\n",
            "48100 val_loss: 0.40770766139030457, train_loss: 0.0018361967522650957\n",
            "48110 val_loss: 0.38935044407844543, train_loss: 0.00172431452665478\n",
            "48120 val_loss: 0.38990074396133423, train_loss: 0.0017362250946462154\n",
            "48130 val_loss: 0.4113164246082306, train_loss: 0.0017960859695449471\n",
            "48140 val_loss: 0.41717788577079773, train_loss: 0.0017606705660000443\n",
            "48150 val_loss: 0.4427277147769928, train_loss: 0.001989039359614253\n",
            "48160 val_loss: 0.4114585220813751, train_loss: 0.0016784768085926771\n",
            "48170 val_loss: 0.39291200041770935, train_loss: 0.001655653933994472\n",
            "48180 val_loss: 0.4010547995567322, train_loss: 0.0017498163506388664\n",
            "48190 val_loss: 0.39021536707878113, train_loss: 0.0017215319676324725\n",
            "48200 val_loss: 0.42697805166244507, train_loss: 0.0019083674997091293\n",
            "48210 val_loss: 0.4092342257499695, train_loss: 0.0017808075062930584\n",
            "48220 val_loss: 0.3898003399372101, train_loss: 0.0017192168161273003\n",
            "48230 val_loss: 0.40840253233909607, train_loss: 0.0017547884490340948\n",
            "48240 val_loss: 0.4358910918235779, train_loss: 0.002095684874802828\n",
            "48250 val_loss: 0.38661134243011475, train_loss: 0.0018116898136213422\n",
            "48260 val_loss: 0.40845081210136414, train_loss: 0.001867395592853427\n",
            "48270 val_loss: 0.3938925564289093, train_loss: 0.0018071407685056329\n",
            "48280 val_loss: 0.40884149074554443, train_loss: 0.0018793870694935322\n",
            "48290 val_loss: 0.4105110764503479, train_loss: 0.0018215245800092816\n",
            "48300 val_loss: 0.39122509956359863, train_loss: 0.0017373714363202453\n",
            "48310 val_loss: 0.40753835439682007, train_loss: 0.0017627397319301963\n",
            "48320 val_loss: 0.41257980465888977, train_loss: 0.001753138960339129\n",
            "48330 val_loss: 0.3988267481327057, train_loss: 0.0017133225919678807\n",
            "48340 val_loss: 0.4197379946708679, train_loss: 0.0018833453068509698\n",
            "48350 val_loss: 0.4248299300670624, train_loss: 0.001827675150707364\n",
            "48360 val_loss: 0.4015483856201172, train_loss: 0.001789641915820539\n",
            "48370 val_loss: 0.42100203037261963, train_loss: 0.0018466983456164598\n",
            "48380 val_loss: 0.4069986343383789, train_loss: 0.0016929850680753589\n",
            "48390 val_loss: 0.4055394232273102, train_loss: 0.0017561379354447126\n",
            "48400 val_loss: 0.4053921103477478, train_loss: 0.003919706214219332\n",
            "48410 val_loss: 0.416297048330307, train_loss: 0.0018371095648035407\n",
            "48420 val_loss: 0.4191528558731079, train_loss: 0.0017802630318328738\n",
            "48430 val_loss: 0.4206702709197998, train_loss: 0.001828939188271761\n",
            "48440 val_loss: 0.42028018832206726, train_loss: 0.0018488942878320813\n",
            "48450 val_loss: 0.40788695216178894, train_loss: 0.0017469675512984395\n",
            "48460 val_loss: 0.39221277832984924, train_loss: 0.0018253163434565067\n",
            "48470 val_loss: 0.4254891574382782, train_loss: 0.001837828429415822\n",
            "48480 val_loss: 0.4053114950656891, train_loss: 0.0017837813356891274\n",
            "48490 val_loss: 0.4171290993690491, train_loss: 0.0018485517939552665\n",
            "48500 val_loss: 0.452072411775589, train_loss: 0.0023570340126752853\n",
            "48510 val_loss: 0.4227549135684967, train_loss: 0.0019242059206590056\n",
            "48520 val_loss: 0.4056895673274994, train_loss: 0.001813761075027287\n",
            "48530 val_loss: 0.4359382092952728, train_loss: 0.0017025585984811187\n",
            "48540 val_loss: 0.4008086025714874, train_loss: 0.001810232293792069\n",
            "48550 val_loss: 0.4000459313392639, train_loss: 0.001854212605394423\n",
            "48560 val_loss: 0.40388810634613037, train_loss: 0.0018382881535217166\n",
            "48570 val_loss: 0.4719342589378357, train_loss: 0.003191910218447447\n",
            "48580 val_loss: 0.4054335653781891, train_loss: 0.0019472625572234392\n",
            "48590 val_loss: 0.4263440668582916, train_loss: 0.0020835744217038155\n",
            "48600 val_loss: 0.43879884481430054, train_loss: 0.0019315367098897696\n",
            "48610 val_loss: 0.4230031371116638, train_loss: 0.0018716542981564999\n",
            "48620 val_loss: 0.4246704578399658, train_loss: 0.001881660195067525\n",
            "48630 val_loss: 0.4072832763195038, train_loss: 0.0018823618302121758\n",
            "48640 val_loss: 0.4456862807273865, train_loss: 0.0019235472427681088\n",
            "48650 val_loss: 0.4192078411579132, train_loss: 0.0018954698462039232\n",
            "48660 val_loss: 0.42549148201942444, train_loss: 0.00198772712610662\n",
            "48670 val_loss: 0.42078936100006104, train_loss: 0.0019555448088794947\n",
            "48680 val_loss: 0.43432164192199707, train_loss: 0.0021214818116277456\n",
            "48690 val_loss: 0.435416042804718, train_loss: 0.002171874511986971\n",
            "48700 val_loss: 0.43675562739372253, train_loss: 0.001999286003410816\n",
            "48710 val_loss: 0.41831308603286743, train_loss: 0.0019025615183636546\n",
            "48720 val_loss: 0.4183375835418701, train_loss: 0.0019134718459099531\n",
            "48730 val_loss: 0.38506418466567993, train_loss: 0.0018399824621155858\n",
            "48740 val_loss: 0.42098718881607056, train_loss: 0.0019882642664015293\n",
            "48750 val_loss: 0.40190982818603516, train_loss: 0.001812006114050746\n",
            "48760 val_loss: 0.390250027179718, train_loss: 0.0018299476942047477\n",
            "48770 val_loss: 0.40457892417907715, train_loss: 0.0018534298287704587\n",
            "48780 val_loss: 0.4209636449813843, train_loss: 0.0019210471073165536\n",
            "48790 val_loss: 0.41965556144714355, train_loss: 0.0018837606767192483\n",
            "48800 val_loss: 0.42463260889053345, train_loss: 0.0018842403078451753\n",
            "48810 val_loss: 0.40912654995918274, train_loss: 0.0018032049993053079\n",
            "48820 val_loss: 0.4303398132324219, train_loss: 0.001921003102324903\n",
            "48830 val_loss: 0.41848036646842957, train_loss: 0.0017845294205471873\n",
            "48840 val_loss: 0.3910684585571289, train_loss: 0.0017405212856829166\n",
            "48850 val_loss: 0.40556979179382324, train_loss: 0.0017761467024683952\n",
            "48860 val_loss: 0.4093778133392334, train_loss: 0.001817197073251009\n",
            "48870 val_loss: 0.3831893503665924, train_loss: 0.0017114230431616306\n",
            "48880 val_loss: 0.39163053035736084, train_loss: 0.001791935763321817\n",
            "48890 val_loss: 0.3976401686668396, train_loss: 0.0017366440733894706\n",
            "48900 val_loss: 0.4322786331176758, train_loss: 0.0017064562998712063\n",
            "48910 val_loss: 0.4067305624485016, train_loss: 0.001757967984303832\n",
            "48920 val_loss: 0.4657154679298401, train_loss: 0.0026577748358249664\n",
            "48930 val_loss: 0.40463986992836, train_loss: 0.0017005675472319126\n",
            "48940 val_loss: 0.41375622153282166, train_loss: 0.0017135735834017396\n",
            "48950 val_loss: 0.3952803313732147, train_loss: 0.0016390129458159208\n",
            "48960 val_loss: 0.4048808813095093, train_loss: 0.0017010342562571168\n",
            "48970 val_loss: 0.4101417660713196, train_loss: 0.001707723829895258\n",
            "48980 val_loss: 0.42360663414001465, train_loss: 0.0018023318843916059\n",
            "48990 val_loss: 0.425131618976593, train_loss: 0.001891051302663982\n",
            "49000 val_loss: 0.40563711524009705, train_loss: 0.0017863090615719557\n",
            "49010 val_loss: 0.3286174237728119, train_loss: 0.002224620897322893\n",
            "49020 val_loss: 0.42143163084983826, train_loss: 0.0019743330776691437\n",
            "49030 val_loss: 0.4049863815307617, train_loss: 0.001861562835983932\n",
            "49040 val_loss: 0.40629300475120544, train_loss: 0.0018397498643025756\n",
            "49050 val_loss: 0.3801686763763428, train_loss: 0.0017668168293312192\n",
            "49060 val_loss: 0.4182893633842468, train_loss: 0.0018714566249400377\n",
            "49070 val_loss: 0.44113144278526306, train_loss: 0.0017766800010576844\n",
            "49080 val_loss: 0.41464686393737793, train_loss: 0.0018874530214816332\n",
            "49090 val_loss: 0.40255460143089294, train_loss: 0.001798666431568563\n",
            "49100 val_loss: 0.41669854521751404, train_loss: 0.0018637225730344653\n",
            "49110 val_loss: 0.4210349917411804, train_loss: 0.0018006939208135009\n",
            "49120 val_loss: 0.4267062246799469, train_loss: 0.0018699989886954427\n",
            "49130 val_loss: 0.48482903838157654, train_loss: 0.0036507088225334883\n",
            "49140 val_loss: 0.4882177710533142, train_loss: 0.0038920773658901453\n",
            "49150 val_loss: 0.43056029081344604, train_loss: 0.0019605441484600306\n",
            "49160 val_loss: 0.44493207335472107, train_loss: 0.0021343003027141094\n",
            "49170 val_loss: 0.43264326453208923, train_loss: 0.002054252428933978\n",
            "49180 val_loss: 0.4144144058227539, train_loss: 0.0019247264135628939\n",
            "49190 val_loss: 0.41168880462646484, train_loss: 0.0018916542176157236\n",
            "49200 val_loss: 0.42033812403678894, train_loss: 0.0019081000937148929\n",
            "49210 val_loss: 0.4311319589614868, train_loss: 0.002027579816058278\n",
            "49220 val_loss: 0.41281190514564514, train_loss: 0.0018814369104802608\n",
            "49230 val_loss: 0.5097874402999878, train_loss: 0.03008953481912613\n",
            "49240 val_loss: 0.2981545627117157, train_loss: 0.010150696150958538\n",
            "49250 val_loss: 0.36630600690841675, train_loss: 0.009829245507717133\n",
            "49260 val_loss: 0.3676777482032776, train_loss: 0.008574744686484337\n",
            "49270 val_loss: 0.36688145995140076, train_loss: 0.007479058112949133\n",
            "49280 val_loss: 0.4009552001953125, train_loss: 0.0073357257060706615\n",
            "49290 val_loss: 0.39241644740104675, train_loss: 0.0064870319329202175\n",
            "49300 val_loss: 0.39405903220176697, train_loss: 0.005996663123369217\n",
            "49310 val_loss: 0.4038262963294983, train_loss: 0.005753358826041222\n",
            "49320 val_loss: 0.4211377203464508, train_loss: 0.005765903741121292\n",
            "49330 val_loss: 0.3989301025867462, train_loss: 0.004931278061121702\n",
            "49340 val_loss: 0.40510907769203186, train_loss: 0.004810065496712923\n",
            "49350 val_loss: 0.39533647894859314, train_loss: 0.004385251551866531\n",
            "49360 val_loss: 0.39707818627357483, train_loss: 0.00412069633603096\n",
            "49370 val_loss: 0.40177085995674133, train_loss: 0.003877811599522829\n",
            "49380 val_loss: 0.39977842569351196, train_loss: 0.00364698632620275\n",
            "49390 val_loss: 0.40457969903945923, train_loss: 0.0034570451825857162\n",
            "49400 val_loss: 0.3988601267337799, train_loss: 0.0032494605984538794\n",
            "49410 val_loss: 0.41186031699180603, train_loss: 0.003210189286619425\n",
            "49420 val_loss: 0.403708815574646, train_loss: 0.003016238333657384\n",
            "49430 val_loss: 0.41463905572891235, train_loss: 0.003016877453774214\n",
            "49440 val_loss: 0.4026079773902893, train_loss: 0.002717075403779745\n",
            "49450 val_loss: 0.39768186211586, train_loss: 0.0025888634845614433\n",
            "49460 val_loss: 0.4220739006996155, train_loss: 0.002661467297002673\n",
            "49470 val_loss: 0.4136921465396881, train_loss: 0.0025153516326099634\n",
            "49480 val_loss: 0.43244484066963196, train_loss: 0.0026029557920992374\n",
            "49490 val_loss: 0.5187476873397827, train_loss: 0.008648291230201721\n",
            "49500 val_loss: 0.40609830617904663, train_loss: 0.002258725231513381\n",
            "49510 val_loss: 0.4142533838748932, train_loss: 0.00228170957416296\n",
            "49520 val_loss: 0.43165823817253113, train_loss: 0.00236380472779274\n",
            "49530 val_loss: 0.4019871652126312, train_loss: 0.0021755003836005926\n",
            "49540 val_loss: 0.41968321800231934, train_loss: 0.0022114787716418505\n",
            "49550 val_loss: 0.38822633028030396, train_loss: 0.002029256895184517\n",
            "49560 val_loss: 0.3997577428817749, train_loss: 0.002026880392804742\n",
            "49570 val_loss: 0.40257981419563293, train_loss: 0.0020789236295968294\n",
            "49580 val_loss: 0.39323896169662476, train_loss: 0.0019605690613389015\n",
            "49590 val_loss: 0.4223332703113556, train_loss: 0.0020630660001188517\n",
            "49600 val_loss: 0.39879441261291504, train_loss: 0.0018929661018773913\n",
            "49610 val_loss: 0.41009286046028137, train_loss: 0.0019502266077324748\n",
            "49620 val_loss: 0.41009384393692017, train_loss: 0.0019070242997258902\n",
            "49630 val_loss: 0.431149423122406, train_loss: 0.002017546910792589\n",
            "49640 val_loss: 0.4065057337284088, train_loss: 0.0018901592120528221\n",
            "49650 val_loss: 0.4486822783946991, train_loss: 0.0018569434760138392\n",
            "49660 val_loss: 0.4468148946762085, train_loss: 0.0019712767098098993\n",
            "49670 val_loss: 0.41772356629371643, train_loss: 0.0019307751208543777\n",
            "49680 val_loss: 0.424903005361557, train_loss: 0.0020111200865358114\n",
            "49690 val_loss: 0.4211876392364502, train_loss: 0.0019729852210730314\n",
            "49700 val_loss: 0.41661980748176575, train_loss: 0.001963224960491061\n",
            "49710 val_loss: 0.4036378860473633, train_loss: 0.0018917482811957598\n",
            "49720 val_loss: 0.3994564414024353, train_loss: 0.0018604432698339224\n",
            "49730 val_loss: 0.40775635838508606, train_loss: 0.0018471945077180862\n",
            "49740 val_loss: 0.39216312766075134, train_loss: 0.001763481879606843\n",
            "49750 val_loss: 0.478527307510376, train_loss: 0.0017332748975604773\n",
            "49760 val_loss: 0.42528265714645386, train_loss: 0.0017389639979228377\n",
            "49770 val_loss: 0.45810654759407043, train_loss: 0.0022743158042430878\n",
            "49780 val_loss: 0.39770159125328064, train_loss: 0.0018007743638008833\n",
            "49790 val_loss: 0.38900861144065857, train_loss: 0.001782971085049212\n",
            "49800 val_loss: 0.4221253991127014, train_loss: 0.0019417267758399248\n",
            "49810 val_loss: 0.41263118386268616, train_loss: 0.0019400552846491337\n",
            "49820 val_loss: 0.4226185381412506, train_loss: 0.0019041907507926226\n",
            "49830 val_loss: 0.4139004349708557, train_loss: 0.0018087959615513682\n",
            "49840 val_loss: 0.4111262857913971, train_loss: 0.0018420531414449215\n",
            "49850 val_loss: 0.39143577218055725, train_loss: 0.0017954452196136117\n",
            "49860 val_loss: 0.4164868891239166, train_loss: 0.0018453587545081973\n",
            "49870 val_loss: 0.4037879705429077, train_loss: 0.0017544234870001674\n",
            "49880 val_loss: 0.41442108154296875, train_loss: 0.001798978541046381\n",
            "49890 val_loss: 0.4127528667449951, train_loss: 0.0018349761376157403\n",
            "49900 val_loss: 0.4256216287612915, train_loss: 0.0018466324545443058\n",
            "49910 val_loss: 0.3737327754497528, train_loss: 0.0022132168523967266\n",
            "49920 val_loss: 0.438667356967926, train_loss: 0.002085223328322172\n",
            "49930 val_loss: 0.4468407928943634, train_loss: 0.002116836840286851\n",
            "49940 val_loss: 0.41966232657432556, train_loss: 0.001865874626673758\n",
            "49950 val_loss: 0.41428083181381226, train_loss: 0.001823066733777523\n",
            "49960 val_loss: 0.41688865423202515, train_loss: 0.0017983097350224853\n",
            "49970 val_loss: 0.3960847854614258, train_loss: 0.0017067115986719728\n",
            "49980 val_loss: 0.41527703404426575, train_loss: 0.0017297897720709443\n",
            "49990 val_loss: 0.41386762261390686, train_loss: 0.0016717504477128386\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(t_losses, label='Training Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.title('Training and Validation Losses')\n",
        "plt.xlabel('Steps (x10)')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "0JSZ6eJwrn_O",
        "outputId": "424b0e78-7088-4d55-9d94-3877432e1091"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "g = torch.Generator().manual_seed(42)\n",
        "g.manual_seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "class DNet(nn.Module):\n",
        "    def __init__(self, input_size: int, hidden_size: int, output_size: int):\n",
        "        super().__init__()\n",
        "        self.seq_model = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_size,bias=True),\n",
        "\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size,bias=True),\n",
        "\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size,bias=True),\n",
        "\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(hidden_size, hidden_size,bias=True),\n",
        "\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(hidden_size, output_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.seq_model(x)\n",
        "\n",
        "\n",
        "dnet = DNet(input_size=2,hidden_size=16,output_size=1)\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.SGD(dnet.parameters(), lr=0.01)\n",
        "\n",
        "t_losses = []\n",
        "val_losses = []\n",
        "\n",
        "min_val_loss : float = float('inf')\n",
        "patience : int = 100  # Number of epochs to wait for improvement before stopping\n",
        "steps_no_improve : int= 0\n",
        "min_step : int = 0\n",
        "\n",
        "for steps in range(50000):\n",
        "    dnet.train()\n",
        "\n",
        "    output = dnet(train_data)\n",
        "    train_loss = loss_fn(output, train_labels)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    train_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if steps % 10 == 0:\n",
        "        dnet.eval()\n",
        "        output = dnet(val_data)\n",
        "        val_loss = loss_fn(output, val_labels)\n",
        "        output = dnet(train_data)\n",
        "        t_loss = loss_fn(output,train_labels)\n",
        "\n",
        "        if val_loss < min_val_loss:\n",
        "            min_val_loss = val_loss\n",
        "            steps_no_improve = 0\n",
        "            min_step = steps\n",
        "            #save model here\n",
        "        else:\n",
        "            steps_no_improve += 1\n",
        "            if steps_no_improve == patience:\n",
        "                print(f'Early stopping! min step : {min_step}')\n",
        "                break  # Early stop\n",
        "\n",
        "\n",
        "        t_losses.append(t_loss.item())\n",
        "        val_losses.append(val_loss.item())\n",
        "        print(f\"{steps} val_loss: {val_loss.item()}, train_loss: {t_loss.item()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzk4CezwsIq4",
        "outputId": "2c910b61-8db6-4d8f-d080-d3e1f4fb4d9e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 val_loss: 0.6986224055290222, train_loss: 0.6956242322921753\n",
            "10 val_loss: 0.6983293294906616, train_loss: 0.6954178214073181\n",
            "20 val_loss: 0.6980583667755127, train_loss: 0.6952297687530518\n",
            "30 val_loss: 0.6977936625480652, train_loss: 0.695051908493042\n",
            "40 val_loss: 0.6975544691085815, train_loss: 0.6948878169059753\n",
            "50 val_loss: 0.6973187327384949, train_loss: 0.6947313547134399\n",
            "60 val_loss: 0.6971179842948914, train_loss: 0.6945965886116028\n",
            "70 val_loss: 0.6969249248504639, train_loss: 0.694471538066864\n",
            "80 val_loss: 0.6967272758483887, train_loss: 0.694343626499176\n",
            "90 val_loss: 0.6965352296829224, train_loss: 0.694225013256073\n",
            "100 val_loss: 0.6963778734207153, train_loss: 0.6941238641738892\n",
            "110 val_loss: 0.6961909532546997, train_loss: 0.6940051317214966\n",
            "120 val_loss: 0.6960338354110718, train_loss: 0.6939107179641724\n",
            "130 val_loss: 0.6958953142166138, train_loss: 0.6938242316246033\n",
            "140 val_loss: 0.6957404017448425, train_loss: 0.6937288045883179\n",
            "150 val_loss: 0.6956164240837097, train_loss: 0.6936526894569397\n",
            "160 val_loss: 0.6954829096794128, train_loss: 0.6935704946517944\n",
            "170 val_loss: 0.6953544020652771, train_loss: 0.6934943795204163\n",
            "180 val_loss: 0.6952447295188904, train_loss: 0.6934296488761902\n",
            "190 val_loss: 0.6951436996459961, train_loss: 0.6933684945106506\n",
            "200 val_loss: 0.6950479745864868, train_loss: 0.6933143138885498\n",
            "210 val_loss: 0.6949601769447327, train_loss: 0.693263828754425\n",
            "220 val_loss: 0.6948691606521606, train_loss: 0.6932122111320496\n",
            "230 val_loss: 0.6947861909866333, train_loss: 0.6931622624397278\n",
            "240 val_loss: 0.6947013139724731, train_loss: 0.6931119561195374\n",
            "250 val_loss: 0.6946109533309937, train_loss: 0.6930546760559082\n",
            "260 val_loss: 0.694517970085144, train_loss: 0.6930022239685059\n",
            "270 val_loss: 0.6944314241409302, train_loss: 0.6929523944854736\n",
            "280 val_loss: 0.6943609714508057, train_loss: 0.6929115653038025\n",
            "290 val_loss: 0.6943004131317139, train_loss: 0.6928800344467163\n",
            "300 val_loss: 0.6942197680473328, train_loss: 0.692834734916687\n",
            "310 val_loss: 0.6941444873809814, train_loss: 0.6927933096885681\n",
            "320 val_loss: 0.6940871477127075, train_loss: 0.6927616596221924\n",
            "330 val_loss: 0.6940280795097351, train_loss: 0.6927245855331421\n",
            "340 val_loss: 0.6939780712127686, train_loss: 0.6926968693733215\n",
            "350 val_loss: 0.6939160823822021, train_loss: 0.6926615834236145\n",
            "360 val_loss: 0.6938666701316833, train_loss: 0.6926331520080566\n",
            "370 val_loss: 0.6938115358352661, train_loss: 0.6925972700119019\n",
            "380 val_loss: 0.6937521696090698, train_loss: 0.6925570964813232\n",
            "390 val_loss: 0.6937128901481628, train_loss: 0.6925361156463623\n",
            "400 val_loss: 0.6936817169189453, train_loss: 0.6925147771835327\n",
            "410 val_loss: 0.6936354637145996, train_loss: 0.6924836039543152\n",
            "420 val_loss: 0.693587064743042, train_loss: 0.6924529075622559\n",
            "430 val_loss: 0.6935314536094666, train_loss: 0.6924214959144592\n",
            "440 val_loss: 0.6934927701950073, train_loss: 0.6923958659172058\n",
            "450 val_loss: 0.6934506297111511, train_loss: 0.6923667192459106\n",
            "460 val_loss: 0.6934126615524292, train_loss: 0.6923388838768005\n",
            "470 val_loss: 0.6933730244636536, train_loss: 0.6923096179962158\n",
            "480 val_loss: 0.6933351159095764, train_loss: 0.6922845840454102\n",
            "490 val_loss: 0.6933022141456604, train_loss: 0.692267119884491\n",
            "500 val_loss: 0.6932724714279175, train_loss: 0.6922435164451599\n",
            "510 val_loss: 0.6932367086410522, train_loss: 0.6922217607498169\n",
            "520 val_loss: 0.6932129859924316, train_loss: 0.6922017335891724\n",
            "530 val_loss: 0.6931706666946411, train_loss: 0.692170262336731\n",
            "540 val_loss: 0.693138837814331, train_loss: 0.6921470761299133\n",
            "550 val_loss: 0.6931076049804688, train_loss: 0.6921270489692688\n",
            "560 val_loss: 0.693081796169281, train_loss: 0.6921055316925049\n",
            "570 val_loss: 0.6930547952651978, train_loss: 0.6920812726020813\n",
            "580 val_loss: 0.6930263042449951, train_loss: 0.6920593976974487\n",
            "590 val_loss: 0.693000078201294, train_loss: 0.6920377612113953\n",
            "600 val_loss: 0.6929676532745361, train_loss: 0.6920087337493896\n",
            "610 val_loss: 0.6929473876953125, train_loss: 0.6919886469841003\n",
            "620 val_loss: 0.6929133534431458, train_loss: 0.6919660568237305\n",
            "630 val_loss: 0.6928979158401489, train_loss: 0.6919482350349426\n",
            "640 val_loss: 0.6928694844245911, train_loss: 0.6919345259666443\n",
            "650 val_loss: 0.6928446888923645, train_loss: 0.69191974401474\n",
            "660 val_loss: 0.6928279399871826, train_loss: 0.6919099688529968\n",
            "670 val_loss: 0.6928048133850098, train_loss: 0.6918991208076477\n",
            "680 val_loss: 0.6927871108055115, train_loss: 0.6918908953666687\n",
            "690 val_loss: 0.6927605867385864, train_loss: 0.6918773055076599\n",
            "700 val_loss: 0.6927416920661926, train_loss: 0.6918665766716003\n",
            "710 val_loss: 0.692712128162384, train_loss: 0.6918469071388245\n",
            "720 val_loss: 0.6926930546760559, train_loss: 0.691831111907959\n",
            "730 val_loss: 0.6926857233047485, train_loss: 0.6918221116065979\n",
            "740 val_loss: 0.6926693320274353, train_loss: 0.6918085813522339\n",
            "750 val_loss: 0.6926559209823608, train_loss: 0.6917937397956848\n",
            "760 val_loss: 0.6926453113555908, train_loss: 0.6917803883552551\n",
            "770 val_loss: 0.6926349401473999, train_loss: 0.6917728781700134\n",
            "780 val_loss: 0.692615270614624, train_loss: 0.6917589902877808\n",
            "790 val_loss: 0.6926082968711853, train_loss: 0.6917476058006287\n",
            "800 val_loss: 0.6925990581512451, train_loss: 0.6917323470115662\n",
            "810 val_loss: 0.6925764679908752, train_loss: 0.6917212605476379\n",
            "820 val_loss: 0.6925652027130127, train_loss: 0.6917086243629456\n",
            "830 val_loss: 0.69255131483078, train_loss: 0.6916897296905518\n",
            "840 val_loss: 0.6925489902496338, train_loss: 0.6916791796684265\n",
            "850 val_loss: 0.692542314529419, train_loss: 0.6916695237159729\n",
            "860 val_loss: 0.6925341486930847, train_loss: 0.6916505694389343\n",
            "870 val_loss: 0.6925248503684998, train_loss: 0.6916411519050598\n",
            "880 val_loss: 0.6925165057182312, train_loss: 0.691631019115448\n",
            "890 val_loss: 0.6925035119056702, train_loss: 0.6916178464889526\n",
            "900 val_loss: 0.692492663860321, train_loss: 0.6916003823280334\n",
            "910 val_loss: 0.6924899220466614, train_loss: 0.6915880441665649\n",
            "920 val_loss: 0.6924691200256348, train_loss: 0.6915712356567383\n",
            "930 val_loss: 0.6924639940261841, train_loss: 0.6915575861930847\n",
            "940 val_loss: 0.692450225353241, train_loss: 0.6915435194969177\n",
            "950 val_loss: 0.6924433708190918, train_loss: 0.6915321946144104\n",
            "960 val_loss: 0.6924261450767517, train_loss: 0.6915150880813599\n",
            "970 val_loss: 0.6924098134040833, train_loss: 0.6914975047111511\n",
            "980 val_loss: 0.692395806312561, train_loss: 0.6914870738983154\n",
            "990 val_loss: 0.6923810243606567, train_loss: 0.6914725303649902\n",
            "1000 val_loss: 0.6923654079437256, train_loss: 0.6914576888084412\n",
            "1010 val_loss: 0.6923540234565735, train_loss: 0.6914436221122742\n",
            "1020 val_loss: 0.692337155342102, train_loss: 0.6914286017417908\n",
            "1030 val_loss: 0.6923214197158813, train_loss: 0.6914095878601074\n",
            "1040 val_loss: 0.6923008561134338, train_loss: 0.6913905143737793\n",
            "1050 val_loss: 0.6922875046730042, train_loss: 0.6913709044456482\n",
            "1060 val_loss: 0.69227534532547, train_loss: 0.6913602352142334\n",
            "1070 val_loss: 0.6922657489776611, train_loss: 0.6913480758666992\n",
            "1080 val_loss: 0.6922522783279419, train_loss: 0.6913310289382935\n",
            "1090 val_loss: 0.6922390460968018, train_loss: 0.6913096904754639\n",
            "1100 val_loss: 0.6922286152839661, train_loss: 0.6912931203842163\n",
            "1110 val_loss: 0.6922094821929932, train_loss: 0.6912749409675598\n",
            "1120 val_loss: 0.6921989917755127, train_loss: 0.6912592649459839\n",
            "1130 val_loss: 0.6921839714050293, train_loss: 0.6912394762039185\n",
            "1140 val_loss: 0.692175567150116, train_loss: 0.6912232041358948\n",
            "1150 val_loss: 0.6921657919883728, train_loss: 0.6912100911140442\n",
            "1160 val_loss: 0.6921536326408386, train_loss: 0.691197395324707\n",
            "1170 val_loss: 0.6921393275260925, train_loss: 0.6911804676055908\n",
            "1180 val_loss: 0.692124605178833, train_loss: 0.6911643743515015\n",
            "1190 val_loss: 0.692119300365448, train_loss: 0.6911507248878479\n",
            "1200 val_loss: 0.692103385925293, train_loss: 0.6911296248435974\n",
            "1210 val_loss: 0.6920764446258545, train_loss: 0.6911066174507141\n",
            "1220 val_loss: 0.6920592784881592, train_loss: 0.6910876035690308\n",
            "1230 val_loss: 0.6920510530471802, train_loss: 0.6910756826400757\n",
            "1240 val_loss: 0.6920400857925415, train_loss: 0.691062867641449\n",
            "1250 val_loss: 0.692030131816864, train_loss: 0.691041886806488\n",
            "1260 val_loss: 0.6920132637023926, train_loss: 0.691021203994751\n",
            "1270 val_loss: 0.6919946670532227, train_loss: 0.6909990310668945\n",
            "1280 val_loss: 0.6919823884963989, train_loss: 0.6909778118133545\n",
            "1290 val_loss: 0.6919683814048767, train_loss: 0.6909545063972473\n",
            "1300 val_loss: 0.6919470429420471, train_loss: 0.6909279227256775\n",
            "1310 val_loss: 0.6919277310371399, train_loss: 0.6909014582633972\n",
            "1320 val_loss: 0.6919124722480774, train_loss: 0.6908788084983826\n",
            "1330 val_loss: 0.6919035911560059, train_loss: 0.6908684372901917\n",
            "1340 val_loss: 0.6918809413909912, train_loss: 0.6908456683158875\n",
            "1350 val_loss: 0.6918681859970093, train_loss: 0.6908279657363892\n",
            "1360 val_loss: 0.6918532848358154, train_loss: 0.690807580947876\n",
            "1370 val_loss: 0.691830575466156, train_loss: 0.6907796859741211\n",
            "1380 val_loss: 0.6918042302131653, train_loss: 0.6907513737678528\n",
            "1390 val_loss: 0.6917850971221924, train_loss: 0.6907297968864441\n",
            "1400 val_loss: 0.6917664408683777, train_loss: 0.6907028555870056\n",
            "1410 val_loss: 0.6917487382888794, train_loss: 0.6906896233558655\n",
            "1420 val_loss: 0.6917386054992676, train_loss: 0.6906679272651672\n",
            "1430 val_loss: 0.6917333602905273, train_loss: 0.6906461119651794\n",
            "1440 val_loss: 0.691713273525238, train_loss: 0.6906196475028992\n",
            "1450 val_loss: 0.6916943788528442, train_loss: 0.6905945539474487\n",
            "1460 val_loss: 0.6916840672492981, train_loss: 0.6905768513679504\n",
            "1470 val_loss: 0.6916686296463013, train_loss: 0.690553605556488\n",
            "1480 val_loss: 0.6916513442993164, train_loss: 0.6905263662338257\n",
            "1490 val_loss: 0.6916332244873047, train_loss: 0.6905019283294678\n",
            "1500 val_loss: 0.6916108727455139, train_loss: 0.6904690265655518\n",
            "1510 val_loss: 0.6915917992591858, train_loss: 0.6904383897781372\n",
            "1520 val_loss: 0.6915658116340637, train_loss: 0.6904080510139465\n",
            "1530 val_loss: 0.6915551424026489, train_loss: 0.6903879642486572\n",
            "1540 val_loss: 0.6915367841720581, train_loss: 0.6903632879257202\n",
            "1550 val_loss: 0.6915205121040344, train_loss: 0.6903323531150818\n",
            "1560 val_loss: 0.6914879083633423, train_loss: 0.6902968883514404\n",
            "1570 val_loss: 0.691451370716095, train_loss: 0.6902621388435364\n",
            "1580 val_loss: 0.6914345026016235, train_loss: 0.6902385354042053\n",
            "1590 val_loss: 0.6914202570915222, train_loss: 0.6902155876159668\n",
            "1600 val_loss: 0.6914083361625671, train_loss: 0.6901975870132446\n",
            "1610 val_loss: 0.6913882493972778, train_loss: 0.6901669502258301\n",
            "1620 val_loss: 0.6913662552833557, train_loss: 0.6901354193687439\n",
            "1630 val_loss: 0.6913472414016724, train_loss: 0.6901125311851501\n",
            "1640 val_loss: 0.6913257837295532, train_loss: 0.6900789141654968\n",
            "1650 val_loss: 0.6913008689880371, train_loss: 0.6900506019592285\n",
            "1660 val_loss: 0.6912813186645508, train_loss: 0.6900215744972229\n",
            "1670 val_loss: 0.6912556290626526, train_loss: 0.689994752407074\n",
            "1680 val_loss: 0.6912278532981873, train_loss: 0.6899574398994446\n",
            "1690 val_loss: 0.6912108063697815, train_loss: 0.6899300217628479\n",
            "1700 val_loss: 0.6911808848381042, train_loss: 0.6898971796035767\n",
            "1710 val_loss: 0.6911530494689941, train_loss: 0.6898673176765442\n",
            "1720 val_loss: 0.6911274790763855, train_loss: 0.6898372173309326\n",
            "1730 val_loss: 0.6911112666130066, train_loss: 0.689805269241333\n",
            "1740 val_loss: 0.6910884976387024, train_loss: 0.6897783279418945\n",
            "1750 val_loss: 0.6910658478736877, train_loss: 0.6897458434104919\n",
            "1760 val_loss: 0.6910428404808044, train_loss: 0.6897184252738953\n",
            "1770 val_loss: 0.691013753414154, train_loss: 0.6896800398826599\n",
            "1780 val_loss: 0.69099360704422, train_loss: 0.6896454095840454\n",
            "1790 val_loss: 0.6909670829772949, train_loss: 0.6896083354949951\n",
            "1800 val_loss: 0.6909477710723877, train_loss: 0.6895802021026611\n",
            "1810 val_loss: 0.6909187436103821, train_loss: 0.6895432472229004\n",
            "1820 val_loss: 0.6908948421478271, train_loss: 0.6895135641098022\n",
            "1830 val_loss: 0.6908633708953857, train_loss: 0.6894843578338623\n",
            "1840 val_loss: 0.690834641456604, train_loss: 0.6894465684890747\n",
            "1850 val_loss: 0.6908021569252014, train_loss: 0.6894063949584961\n",
            "1860 val_loss: 0.690770149230957, train_loss: 0.6893640756607056\n",
            "1870 val_loss: 0.6907359957695007, train_loss: 0.6893229484558105\n",
            "1880 val_loss: 0.6907066106796265, train_loss: 0.6892911195755005\n",
            "1890 val_loss: 0.6906760334968567, train_loss: 0.6892403364181519\n",
            "1900 val_loss: 0.6906419396400452, train_loss: 0.689196765422821\n",
            "1910 val_loss: 0.6906115412712097, train_loss: 0.6891580820083618\n",
            "1920 val_loss: 0.690579891204834, train_loss: 0.6891138553619385\n",
            "1930 val_loss: 0.6905508041381836, train_loss: 0.6890732049942017\n",
            "1940 val_loss: 0.6905280351638794, train_loss: 0.689039409160614\n",
            "1950 val_loss: 0.690485417842865, train_loss: 0.6889907121658325\n",
            "1960 val_loss: 0.6904513835906982, train_loss: 0.6889459490776062\n",
            "1970 val_loss: 0.6904171705245972, train_loss: 0.6889020204544067\n",
            "1980 val_loss: 0.6903876066207886, train_loss: 0.6888641119003296\n",
            "1990 val_loss: 0.6903533339500427, train_loss: 0.6888247132301331\n",
            "2000 val_loss: 0.6903262138366699, train_loss: 0.6887895464897156\n",
            "2010 val_loss: 0.6902928352355957, train_loss: 0.6887494325637817\n",
            "2020 val_loss: 0.690255343914032, train_loss: 0.6887064576148987\n",
            "2030 val_loss: 0.6902247667312622, train_loss: 0.6886594295501709\n",
            "2040 val_loss: 0.6901849508285522, train_loss: 0.688621997833252\n",
            "2050 val_loss: 0.6901481747627258, train_loss: 0.6885727643966675\n",
            "2060 val_loss: 0.690118134021759, train_loss: 0.6885309815406799\n",
            "2070 val_loss: 0.6900849342346191, train_loss: 0.6884889602661133\n",
            "2080 val_loss: 0.6900469660758972, train_loss: 0.6884300112724304\n",
            "2090 val_loss: 0.6900128722190857, train_loss: 0.6883761286735535\n",
            "2100 val_loss: 0.6899760961532593, train_loss: 0.6883323192596436\n",
            "2110 val_loss: 0.6899369359016418, train_loss: 0.6882836222648621\n",
            "2120 val_loss: 0.6898897886276245, train_loss: 0.6882250308990479\n",
            "2130 val_loss: 0.6898581981658936, train_loss: 0.6881718635559082\n",
            "2140 val_loss: 0.689824640750885, train_loss: 0.6881197690963745\n",
            "2150 val_loss: 0.6897708177566528, train_loss: 0.6880603432655334\n",
            "2160 val_loss: 0.6897329688072205, train_loss: 0.6880126595497131\n",
            "2170 val_loss: 0.6896950006484985, train_loss: 0.6879717707633972\n",
            "2180 val_loss: 0.6896466612815857, train_loss: 0.6879116892814636\n",
            "2190 val_loss: 0.689594566822052, train_loss: 0.6878496408462524\n",
            "2200 val_loss: 0.6895480751991272, train_loss: 0.6877922415733337\n",
            "2210 val_loss: 0.689507007598877, train_loss: 0.6877380609512329\n",
            "2220 val_loss: 0.6894561052322388, train_loss: 0.6876806020736694\n",
            "2230 val_loss: 0.6894232630729675, train_loss: 0.687635600566864\n",
            "2240 val_loss: 0.6893734931945801, train_loss: 0.687575101852417\n",
            "2250 val_loss: 0.689323365688324, train_loss: 0.6875090599060059\n",
            "2260 val_loss: 0.6892748475074768, train_loss: 0.6874425411224365\n",
            "2270 val_loss: 0.6892293691635132, train_loss: 0.6873778700828552\n",
            "2280 val_loss: 0.689180850982666, train_loss: 0.6873236894607544\n",
            "2290 val_loss: 0.6891309022903442, train_loss: 0.6872603893280029\n",
            "2300 val_loss: 0.6890899538993835, train_loss: 0.6872100830078125\n",
            "2310 val_loss: 0.689030110836029, train_loss: 0.6871386766433716\n",
            "2320 val_loss: 0.68897545337677, train_loss: 0.6870733499526978\n",
            "2330 val_loss: 0.6889289617538452, train_loss: 0.6870109438896179\n",
            "2340 val_loss: 0.6888787150382996, train_loss: 0.6869519352912903\n",
            "2350 val_loss: 0.6888189315795898, train_loss: 0.6868777871131897\n",
            "2360 val_loss: 0.6887589693069458, train_loss: 0.6868054270744324\n",
            "2370 val_loss: 0.6886999607086182, train_loss: 0.6867334842681885\n",
            "2380 val_loss: 0.6886395812034607, train_loss: 0.6866650581359863\n",
            "2390 val_loss: 0.6885777711868286, train_loss: 0.6865783929824829\n",
            "2400 val_loss: 0.688523530960083, train_loss: 0.6865065693855286\n",
            "2410 val_loss: 0.6884713172912598, train_loss: 0.6864407658576965\n",
            "2420 val_loss: 0.6884165406227112, train_loss: 0.6863728165626526\n",
            "2430 val_loss: 0.6883531212806702, train_loss: 0.6862990856170654\n",
            "2440 val_loss: 0.6882927417755127, train_loss: 0.686225414276123\n",
            "2450 val_loss: 0.6882333755493164, train_loss: 0.6861626505851746\n",
            "2460 val_loss: 0.6881773471832275, train_loss: 0.6860947012901306\n",
            "2470 val_loss: 0.6880994439125061, train_loss: 0.6860064268112183\n",
            "2480 val_loss: 0.6880313754081726, train_loss: 0.685917317867279\n",
            "2490 val_loss: 0.6879633665084839, train_loss: 0.6858202219009399\n",
            "2500 val_loss: 0.6878875494003296, train_loss: 0.68573397397995\n",
            "2510 val_loss: 0.6878228187561035, train_loss: 0.6856511235237122\n",
            "2520 val_loss: 0.6877405047416687, train_loss: 0.6855527758598328\n",
            "2530 val_loss: 0.6876649260520935, train_loss: 0.6854680180549622\n",
            "2540 val_loss: 0.6876072883605957, train_loss: 0.6853936910629272\n",
            "2550 val_loss: 0.6875323057174683, train_loss: 0.6853064894676208\n",
            "2560 val_loss: 0.6874615550041199, train_loss: 0.6852167248725891\n",
            "2570 val_loss: 0.687403678894043, train_loss: 0.6851423382759094\n",
            "2580 val_loss: 0.6873351335525513, train_loss: 0.6850600242614746\n",
            "2590 val_loss: 0.6872660517692566, train_loss: 0.684979259967804\n",
            "2600 val_loss: 0.6871905326843262, train_loss: 0.6849002242088318\n",
            "2610 val_loss: 0.6871055960655212, train_loss: 0.6847993731498718\n",
            "2620 val_loss: 0.6870302557945251, train_loss: 0.6847106218338013\n",
            "2630 val_loss: 0.6869336366653442, train_loss: 0.6846033334732056\n",
            "2640 val_loss: 0.6868551969528198, train_loss: 0.6845070719718933\n",
            "2650 val_loss: 0.6867612600326538, train_loss: 0.6844019889831543\n",
            "2660 val_loss: 0.686683714389801, train_loss: 0.6843072772026062\n",
            "2670 val_loss: 0.6866061091423035, train_loss: 0.6842181086540222\n",
            "2680 val_loss: 0.6865079998970032, train_loss: 0.6841115951538086\n",
            "2690 val_loss: 0.6864218711853027, train_loss: 0.6840079426765442\n",
            "2700 val_loss: 0.6863453984260559, train_loss: 0.6839150190353394\n",
            "2710 val_loss: 0.6862525939941406, train_loss: 0.6838110089302063\n",
            "2720 val_loss: 0.6861550211906433, train_loss: 0.6836996078491211\n",
            "2730 val_loss: 0.6860664486885071, train_loss: 0.683595597743988\n",
            "2740 val_loss: 0.685973048210144, train_loss: 0.6834867000579834\n",
            "2750 val_loss: 0.6858584880828857, train_loss: 0.6833574771881104\n",
            "2760 val_loss: 0.6857647895812988, train_loss: 0.683245062828064\n",
            "2770 val_loss: 0.6856768727302551, train_loss: 0.6831390857696533\n",
            "2780 val_loss: 0.6855992078781128, train_loss: 0.6830434203147888\n",
            "2790 val_loss: 0.6854842901229858, train_loss: 0.6829075813293457\n",
            "2800 val_loss: 0.6853795051574707, train_loss: 0.6827861070632935\n",
            "2810 val_loss: 0.6852767467498779, train_loss: 0.682673990726471\n",
            "2820 val_loss: 0.6851783990859985, train_loss: 0.6825531721115112\n",
            "2830 val_loss: 0.6850837469100952, train_loss: 0.6824401617050171\n",
            "2840 val_loss: 0.6849681735038757, train_loss: 0.6823154091835022\n",
            "2850 val_loss: 0.6848772764205933, train_loss: 0.6822054982185364\n",
            "2860 val_loss: 0.6847710013389587, train_loss: 0.6820845603942871\n",
            "2870 val_loss: 0.6846726536750793, train_loss: 0.6819612383842468\n",
            "2880 val_loss: 0.6845801472663879, train_loss: 0.6818427443504333\n",
            "2890 val_loss: 0.6844708323478699, train_loss: 0.6817184686660767\n",
            "2900 val_loss: 0.6843516230583191, train_loss: 0.6815798282623291\n",
            "2910 val_loss: 0.6842455863952637, train_loss: 0.6814537048339844\n",
            "2920 val_loss: 0.6841077208518982, train_loss: 0.6813085675239563\n",
            "2930 val_loss: 0.6839762926101685, train_loss: 0.6811673045158386\n",
            "2940 val_loss: 0.6838461756706238, train_loss: 0.681024968624115\n",
            "2950 val_loss: 0.683715283870697, train_loss: 0.6808768510818481\n",
            "2960 val_loss: 0.6835989952087402, train_loss: 0.6807429194450378\n",
            "2970 val_loss: 0.6834843754768372, train_loss: 0.6806021928787231\n",
            "2980 val_loss: 0.6833773255348206, train_loss: 0.6804822087287903\n",
            "2990 val_loss: 0.6832519769668579, train_loss: 0.6803327202796936\n",
            "3000 val_loss: 0.6831033825874329, train_loss: 0.680171012878418\n",
            "3010 val_loss: 0.6829630136489868, train_loss: 0.6800069212913513\n",
            "3020 val_loss: 0.6828272342681885, train_loss: 0.6798508167266846\n",
            "3030 val_loss: 0.6827050447463989, train_loss: 0.6797124743461609\n",
            "3040 val_loss: 0.682545006275177, train_loss: 0.6795381903648376\n",
            "3050 val_loss: 0.6823950409889221, train_loss: 0.6793765425682068\n",
            "3060 val_loss: 0.6822415590286255, train_loss: 0.6792035102844238\n",
            "3070 val_loss: 0.6820880770683289, train_loss: 0.6790255904197693\n",
            "3080 val_loss: 0.681921660900116, train_loss: 0.6788455247879028\n",
            "3090 val_loss: 0.6817739009857178, train_loss: 0.6786760687828064\n",
            "3100 val_loss: 0.6816222071647644, train_loss: 0.6785080432891846\n",
            "3110 val_loss: 0.6814612746238708, train_loss: 0.6783158183097839\n",
            "3120 val_loss: 0.6813122034072876, train_loss: 0.6781452894210815\n",
            "3130 val_loss: 0.6811831593513489, train_loss: 0.6779947876930237\n",
            "3140 val_loss: 0.6810246109962463, train_loss: 0.6778190732002258\n",
            "3150 val_loss: 0.680870771408081, train_loss: 0.6776410937309265\n",
            "3160 val_loss: 0.6806967854499817, train_loss: 0.6774454712867737\n",
            "3170 val_loss: 0.6805256605148315, train_loss: 0.6772506833076477\n",
            "3180 val_loss: 0.6803719401359558, train_loss: 0.6770814657211304\n",
            "3190 val_loss: 0.6801813244819641, train_loss: 0.6768823862075806\n",
            "3200 val_loss: 0.6800089478492737, train_loss: 0.6766889691352844\n",
            "3210 val_loss: 0.6798503994941711, train_loss: 0.6764976382255554\n",
            "3220 val_loss: 0.6796746253967285, train_loss: 0.6763023138046265\n",
            "3230 val_loss: 0.6794949173927307, train_loss: 0.6761065125465393\n",
            "3240 val_loss: 0.6793080568313599, train_loss: 0.6758890748023987\n",
            "3250 val_loss: 0.6791293621063232, train_loss: 0.6756976246833801\n",
            "3260 val_loss: 0.6789529919624329, train_loss: 0.6755017042160034\n",
            "3270 val_loss: 0.6787773370742798, train_loss: 0.675298273563385\n",
            "3280 val_loss: 0.6785944104194641, train_loss: 0.6750997304916382\n",
            "3290 val_loss: 0.6784395575523376, train_loss: 0.6749128103256226\n",
            "3300 val_loss: 0.6782531142234802, train_loss: 0.6746999025344849\n",
            "3310 val_loss: 0.6780585050582886, train_loss: 0.6744892597198486\n",
            "3320 val_loss: 0.6778663396835327, train_loss: 0.6742731332778931\n",
            "3330 val_loss: 0.677655816078186, train_loss: 0.6740437746047974\n",
            "3340 val_loss: 0.6774687767028809, train_loss: 0.6738354563713074\n",
            "3350 val_loss: 0.6772563457489014, train_loss: 0.673606812953949\n",
            "3360 val_loss: 0.6770445108413696, train_loss: 0.673368513584137\n",
            "3370 val_loss: 0.676861047744751, train_loss: 0.673149049282074\n",
            "3380 val_loss: 0.6766699552536011, train_loss: 0.6729339361190796\n",
            "3390 val_loss: 0.6764528751373291, train_loss: 0.6726897954940796\n",
            "3400 val_loss: 0.6762309074401855, train_loss: 0.6724373698234558\n",
            "3410 val_loss: 0.676018238067627, train_loss: 0.6722002625465393\n",
            "3420 val_loss: 0.6757911443710327, train_loss: 0.6719452738761902\n",
            "3430 val_loss: 0.6755855679512024, train_loss: 0.6716957092285156\n",
            "3440 val_loss: 0.6753716468811035, train_loss: 0.6714497804641724\n",
            "3450 val_loss: 0.6751528382301331, train_loss: 0.6712146401405334\n",
            "3460 val_loss: 0.6748560667037964, train_loss: 0.6709063053131104\n",
            "3470 val_loss: 0.6746337413787842, train_loss: 0.6706537008285522\n",
            "3480 val_loss: 0.6744109392166138, train_loss: 0.6703832149505615\n",
            "3490 val_loss: 0.6741726994514465, train_loss: 0.6701153516769409\n",
            "3500 val_loss: 0.6739147305488586, train_loss: 0.6698322892189026\n",
            "3510 val_loss: 0.6736195683479309, train_loss: 0.6695502996444702\n",
            "3520 val_loss: 0.6733887791633606, train_loss: 0.6692769527435303\n",
            "3530 val_loss: 0.6731671690940857, train_loss: 0.6690343618392944\n",
            "3540 val_loss: 0.6729121804237366, train_loss: 0.6687579154968262\n",
            "3550 val_loss: 0.6726462244987488, train_loss: 0.6684806942939758\n",
            "3560 val_loss: 0.672343373298645, train_loss: 0.6681731939315796\n",
            "3570 val_loss: 0.6720587015151978, train_loss: 0.6678757667541504\n",
            "3580 val_loss: 0.671818733215332, train_loss: 0.6676129102706909\n",
            "3590 val_loss: 0.6715661287307739, train_loss: 0.6673434376716614\n",
            "3600 val_loss: 0.6712918281555176, train_loss: 0.6670634746551514\n",
            "3610 val_loss: 0.671053409576416, train_loss: 0.6667892336845398\n",
            "3620 val_loss: 0.670801043510437, train_loss: 0.6664988994598389\n",
            "3630 val_loss: 0.670545756816864, train_loss: 0.6662237048149109\n",
            "3640 val_loss: 0.6703036427497864, train_loss: 0.665956974029541\n",
            "3650 val_loss: 0.6700651049613953, train_loss: 0.665662407875061\n",
            "3660 val_loss: 0.6698288321495056, train_loss: 0.6654136776924133\n",
            "3670 val_loss: 0.6695912480354309, train_loss: 0.665126383304596\n",
            "3680 val_loss: 0.6693409085273743, train_loss: 0.6648737788200378\n",
            "3690 val_loss: 0.6690571308135986, train_loss: 0.6645723581314087\n",
            "3700 val_loss: 0.6687919497489929, train_loss: 0.6642923951148987\n",
            "3710 val_loss: 0.6685003638267517, train_loss: 0.6639950275421143\n",
            "3720 val_loss: 0.6682397127151489, train_loss: 0.6637044548988342\n",
            "3730 val_loss: 0.6679790019989014, train_loss: 0.6634131669998169\n",
            "3740 val_loss: 0.6676880121231079, train_loss: 0.6631130576133728\n",
            "3750 val_loss: 0.6673739552497864, train_loss: 0.6627768874168396\n",
            "3760 val_loss: 0.6670908331871033, train_loss: 0.6624619364738464\n",
            "3770 val_loss: 0.6667701005935669, train_loss: 0.6621515154838562\n",
            "3780 val_loss: 0.6664502024650574, train_loss: 0.6618155837059021\n",
            "3790 val_loss: 0.6661173701286316, train_loss: 0.6614658832550049\n",
            "3800 val_loss: 0.6658028364181519, train_loss: 0.661128580570221\n",
            "3810 val_loss: 0.6654982566833496, train_loss: 0.6607755422592163\n",
            "3820 val_loss: 0.6652161478996277, train_loss: 0.6604657769203186\n",
            "3830 val_loss: 0.6649101972579956, train_loss: 0.6601290106773376\n",
            "3840 val_loss: 0.6645761132240295, train_loss: 0.6597516536712646\n",
            "3850 val_loss: 0.6642141342163086, train_loss: 0.6594286561012268\n",
            "3860 val_loss: 0.6638185381889343, train_loss: 0.6590583920478821\n",
            "3870 val_loss: 0.6634661555290222, train_loss: 0.658698320388794\n",
            "3880 val_loss: 0.6632086634635925, train_loss: 0.658420741558075\n",
            "3890 val_loss: 0.6628998517990112, train_loss: 0.6580801010131836\n",
            "3900 val_loss: 0.6625658869743347, train_loss: 0.6577336192131042\n",
            "3910 val_loss: 0.6622132658958435, train_loss: 0.6573731303215027\n",
            "3920 val_loss: 0.6619254350662231, train_loss: 0.6570903062820435\n",
            "3930 val_loss: 0.6615701913833618, train_loss: 0.656733512878418\n",
            "3940 val_loss: 0.661217212677002, train_loss: 0.6563575267791748\n",
            "3950 val_loss: 0.6608832478523254, train_loss: 0.6560207009315491\n",
            "3960 val_loss: 0.6605696678161621, train_loss: 0.6557055711746216\n",
            "3970 val_loss: 0.6602540016174316, train_loss: 0.655366063117981\n",
            "3980 val_loss: 0.6599708199501038, train_loss: 0.6550473570823669\n",
            "3990 val_loss: 0.6595888137817383, train_loss: 0.6546609997749329\n",
            "4000 val_loss: 0.6592937707901001, train_loss: 0.6543227434158325\n",
            "4010 val_loss: 0.6589916944503784, train_loss: 0.6539637446403503\n",
            "4020 val_loss: 0.6585797667503357, train_loss: 0.6535127758979797\n",
            "4030 val_loss: 0.6582028269767761, train_loss: 0.6531832814216614\n",
            "4040 val_loss: 0.6578882932662964, train_loss: 0.6528133749961853\n",
            "4050 val_loss: 0.657508909702301, train_loss: 0.6524235010147095\n",
            "4060 val_loss: 0.6572394371032715, train_loss: 0.6520854234695435\n",
            "4070 val_loss: 0.6568812131881714, train_loss: 0.6517126560211182\n",
            "4080 val_loss: 0.6565251350402832, train_loss: 0.6513345837593079\n",
            "4090 val_loss: 0.6561588644981384, train_loss: 0.6509553790092468\n",
            "4100 val_loss: 0.6557793617248535, train_loss: 0.6505630016326904\n",
            "4110 val_loss: 0.6554513573646545, train_loss: 0.6502075791358948\n",
            "4120 val_loss: 0.6550891399383545, train_loss: 0.6498275399208069\n",
            "4130 val_loss: 0.6546616554260254, train_loss: 0.6493868827819824\n",
            "4140 val_loss: 0.654266893863678, train_loss: 0.6490181684494019\n",
            "4150 val_loss: 0.6538734436035156, train_loss: 0.6486322283744812\n",
            "4160 val_loss: 0.6535239219665527, train_loss: 0.648293137550354\n",
            "4170 val_loss: 0.6531838178634644, train_loss: 0.6479161977767944\n",
            "4180 val_loss: 0.6528518199920654, train_loss: 0.6475247144699097\n",
            "4190 val_loss: 0.6524460315704346, train_loss: 0.647127091884613\n",
            "4200 val_loss: 0.6520774364471436, train_loss: 0.6467592716217041\n",
            "4210 val_loss: 0.651740550994873, train_loss: 0.6464241147041321\n",
            "4220 val_loss: 0.6513009071350098, train_loss: 0.6460091471672058\n",
            "4230 val_loss: 0.6509673595428467, train_loss: 0.6456148028373718\n",
            "4240 val_loss: 0.6506623029708862, train_loss: 0.6452759504318237\n",
            "4250 val_loss: 0.6502701044082642, train_loss: 0.6448831558227539\n",
            "4260 val_loss: 0.6498813629150391, train_loss: 0.6445529460906982\n",
            "4270 val_loss: 0.649503231048584, train_loss: 0.644208550453186\n",
            "4280 val_loss: 0.6492449641227722, train_loss: 0.6439152956008911\n",
            "4290 val_loss: 0.6488741636276245, train_loss: 0.6436085104942322\n",
            "4300 val_loss: 0.6484020948410034, train_loss: 0.6431658267974854\n",
            "4310 val_loss: 0.6479979157447815, train_loss: 0.6427830457687378\n",
            "4320 val_loss: 0.647619903087616, train_loss: 0.6424037218093872\n",
            "4330 val_loss: 0.6472945809364319, train_loss: 0.6420552730560303\n",
            "4340 val_loss: 0.6469590067863464, train_loss: 0.6417323350906372\n",
            "4350 val_loss: 0.6466874480247498, train_loss: 0.6414711475372314\n",
            "4360 val_loss: 0.6462952494621277, train_loss: 0.6410967111587524\n",
            "4370 val_loss: 0.6458495259284973, train_loss: 0.6407297253608704\n",
            "4380 val_loss: 0.6455853581428528, train_loss: 0.640363335609436\n",
            "4390 val_loss: 0.6451488137245178, train_loss: 0.6399995684623718\n",
            "4400 val_loss: 0.6448283791542053, train_loss: 0.639696478843689\n",
            "4410 val_loss: 0.6444969773292542, train_loss: 0.6393244862556458\n",
            "4420 val_loss: 0.6441681385040283, train_loss: 0.6390001177787781\n",
            "4430 val_loss: 0.6437085270881653, train_loss: 0.6385906934738159\n",
            "4440 val_loss: 0.6434256434440613, train_loss: 0.6383084654808044\n",
            "4450 val_loss: 0.6430193781852722, train_loss: 0.6379140615463257\n",
            "4460 val_loss: 0.6426339745521545, train_loss: 0.637499213218689\n",
            "4470 val_loss: 0.6422685384750366, train_loss: 0.6371474862098694\n",
            "4480 val_loss: 0.6419422626495361, train_loss: 0.6367800235748291\n",
            "4490 val_loss: 0.6416324973106384, train_loss: 0.6364262104034424\n",
            "4500 val_loss: 0.6412345767021179, train_loss: 0.6360344290733337\n",
            "4510 val_loss: 0.640893280506134, train_loss: 0.6357014179229736\n",
            "4520 val_loss: 0.6406243443489075, train_loss: 0.635438859462738\n",
            "4530 val_loss: 0.6402496099472046, train_loss: 0.6351349353790283\n",
            "4540 val_loss: 0.6397803425788879, train_loss: 0.6347197890281677\n",
            "4550 val_loss: 0.6394184231758118, train_loss: 0.6343712210655212\n",
            "4560 val_loss: 0.6389883756637573, train_loss: 0.6339492201805115\n",
            "4570 val_loss: 0.6385979652404785, train_loss: 0.6335861682891846\n",
            "4580 val_loss: 0.6383095979690552, train_loss: 0.6333737373352051\n",
            "4590 val_loss: 0.6378412842750549, train_loss: 0.6329628229141235\n",
            "4600 val_loss: 0.6375012397766113, train_loss: 0.6325554251670837\n",
            "4610 val_loss: 0.6371413469314575, train_loss: 0.632283091545105\n",
            "4620 val_loss: 0.6368339657783508, train_loss: 0.6319414377212524\n",
            "4630 val_loss: 0.6364619135856628, train_loss: 0.6315850615501404\n",
            "4640 val_loss: 0.6360760927200317, train_loss: 0.6312009692192078\n",
            "4650 val_loss: 0.635509192943573, train_loss: 0.6308238506317139\n",
            "4660 val_loss: 0.6352477669715881, train_loss: 0.6305866837501526\n",
            "4670 val_loss: 0.6348550915718079, train_loss: 0.6302743554115295\n",
            "4680 val_loss: 0.634522557258606, train_loss: 0.6299237012863159\n",
            "4690 val_loss: 0.6341801285743713, train_loss: 0.6295885443687439\n",
            "4700 val_loss: 0.634026050567627, train_loss: 0.629353404045105\n",
            "4710 val_loss: 0.6337133049964905, train_loss: 0.6289740800857544\n",
            "4720 val_loss: 0.6332865357398987, train_loss: 0.6286162734031677\n",
            "4730 val_loss: 0.6329030394554138, train_loss: 0.6282593607902527\n",
            "4740 val_loss: 0.6325415372848511, train_loss: 0.627913236618042\n",
            "4750 val_loss: 0.6322413682937622, train_loss: 0.6276620030403137\n",
            "4760 val_loss: 0.6319000124931335, train_loss: 0.6273483633995056\n",
            "4770 val_loss: 0.6316350698471069, train_loss: 0.6270868182182312\n",
            "4780 val_loss: 0.6312702298164368, train_loss: 0.6266937851905823\n",
            "4790 val_loss: 0.6310082077980042, train_loss: 0.6264171004295349\n",
            "4800 val_loss: 0.6306294202804565, train_loss: 0.626100480556488\n",
            "4810 val_loss: 0.6304202079772949, train_loss: 0.6257889866828918\n",
            "4820 val_loss: 0.630085825920105, train_loss: 0.6254781484603882\n",
            "4830 val_loss: 0.6297529935836792, train_loss: 0.6251668334007263\n",
            "4840 val_loss: 0.6294655799865723, train_loss: 0.6248778700828552\n",
            "4850 val_loss: 0.6290258765220642, train_loss: 0.6245282888412476\n",
            "4860 val_loss: 0.6285697817802429, train_loss: 0.6241620182991028\n",
            "4870 val_loss: 0.6281866431236267, train_loss: 0.623863697052002\n",
            "4880 val_loss: 0.6278535723686218, train_loss: 0.6235570907592773\n",
            "4890 val_loss: 0.6275841593742371, train_loss: 0.6232805848121643\n",
            "4900 val_loss: 0.6273034811019897, train_loss: 0.6230308413505554\n",
            "4910 val_loss: 0.6268534660339355, train_loss: 0.622675895690918\n",
            "4920 val_loss: 0.6265021562576294, train_loss: 0.6223731637001038\n",
            "4930 val_loss: 0.6263625025749207, train_loss: 0.6220873594284058\n",
            "4940 val_loss: 0.6261624097824097, train_loss: 0.621830940246582\n",
            "4950 val_loss: 0.6258133053779602, train_loss: 0.6215416193008423\n",
            "4960 val_loss: 0.6254352331161499, train_loss: 0.6212507486343384\n",
            "4970 val_loss: 0.6252071261405945, train_loss: 0.6210405826568604\n",
            "4980 val_loss: 0.6248894333839417, train_loss: 0.6207610368728638\n",
            "4990 val_loss: 0.6246770620346069, train_loss: 0.6204658150672913\n",
            "5000 val_loss: 0.6242577433586121, train_loss: 0.6201533079147339\n",
            "5010 val_loss: 0.6237369775772095, train_loss: 0.6198142170906067\n",
            "5020 val_loss: 0.6235141158103943, train_loss: 0.6194726824760437\n",
            "5030 val_loss: 0.6231625080108643, train_loss: 0.6192026138305664\n",
            "5040 val_loss: 0.6227681636810303, train_loss: 0.6188870072364807\n",
            "5050 val_loss: 0.6225457191467285, train_loss: 0.6187494993209839\n",
            "5060 val_loss: 0.6222453117370605, train_loss: 0.6185738444328308\n",
            "5070 val_loss: 0.6219573020935059, train_loss: 0.6183935403823853\n",
            "5080 val_loss: 0.6216834187507629, train_loss: 0.6181025505065918\n",
            "5090 val_loss: 0.6215617060661316, train_loss: 0.6178857684135437\n",
            "5100 val_loss: 0.6212765574455261, train_loss: 0.6176608204841614\n",
            "5110 val_loss: 0.6208785176277161, train_loss: 0.6174302101135254\n",
            "5120 val_loss: 0.6205536723136902, train_loss: 0.6172225475311279\n",
            "5130 val_loss: 0.6205281019210815, train_loss: 0.6169254779815674\n",
            "5140 val_loss: 0.6202301979064941, train_loss: 0.616701602935791\n",
            "5150 val_loss: 0.6200282573699951, train_loss: 0.6165173053741455\n",
            "5160 val_loss: 0.6198156476020813, train_loss: 0.6162906289100647\n",
            "5170 val_loss: 0.6195511817932129, train_loss: 0.6160334944725037\n",
            "5180 val_loss: 0.6192814111709595, train_loss: 0.6157220602035522\n",
            "5190 val_loss: 0.618927538394928, train_loss: 0.6153983473777771\n",
            "5200 val_loss: 0.6188479661941528, train_loss: 0.615293562412262\n",
            "5210 val_loss: 0.6184794902801514, train_loss: 0.6151230931282043\n",
            "5220 val_loss: 0.6181150078773499, train_loss: 0.6148635149002075\n",
            "5230 val_loss: 0.6179253458976746, train_loss: 0.6147159934043884\n",
            "5240 val_loss: 0.6175398230552673, train_loss: 0.6143696308135986\n",
            "5250 val_loss: 0.6173771023750305, train_loss: 0.6142217516899109\n",
            "5260 val_loss: 0.6171913146972656, train_loss: 0.6139795184135437\n",
            "5270 val_loss: 0.6168872117996216, train_loss: 0.6137323975563049\n",
            "5280 val_loss: 0.6167824864387512, train_loss: 0.6136595010757446\n",
            "5290 val_loss: 0.6166066527366638, train_loss: 0.613514244556427\n",
            "5300 val_loss: 0.6165452003479004, train_loss: 0.613371729850769\n",
            "5310 val_loss: 0.6161021590232849, train_loss: 0.6131219863891602\n",
            "5320 val_loss: 0.6156366467475891, train_loss: 0.6127740740776062\n",
            "5330 val_loss: 0.6152507066726685, train_loss: 0.6124951243400574\n",
            "5340 val_loss: 0.6150350570678711, train_loss: 0.6123208999633789\n",
            "5350 val_loss: 0.6150395274162292, train_loss: 0.6121586561203003\n",
            "5360 val_loss: 0.6147735714912415, train_loss: 0.6118996143341064\n",
            "5370 val_loss: 0.614423930644989, train_loss: 0.6116752028465271\n",
            "5380 val_loss: 0.6141225695610046, train_loss: 0.6114745140075684\n",
            "5390 val_loss: 0.6140490174293518, train_loss: 0.6113418936729431\n",
            "5400 val_loss: 0.6140183210372925, train_loss: 0.6112262010574341\n",
            "5410 val_loss: 0.6138455271720886, train_loss: 0.611025869846344\n",
            "5420 val_loss: 0.613776683807373, train_loss: 0.6109455823898315\n",
            "5430 val_loss: 0.6138899326324463, train_loss: 0.6107617020606995\n",
            "5440 val_loss: 0.6135324835777283, train_loss: 0.6106576323509216\n",
            "5450 val_loss: 0.6132705807685852, train_loss: 0.6104406118392944\n",
            "5460 val_loss: 0.6131632328033447, train_loss: 0.6102659702301025\n",
            "5470 val_loss: 0.6128105521202087, train_loss: 0.609959065914154\n",
            "5480 val_loss: 0.6126933097839355, train_loss: 0.609810471534729\n",
            "5490 val_loss: 0.6122987270355225, train_loss: 0.6095718741416931\n",
            "5500 val_loss: 0.6122496724128723, train_loss: 0.609468400478363\n",
            "5510 val_loss: 0.6120702028274536, train_loss: 0.609294593334198\n",
            "5520 val_loss: 0.6118142604827881, train_loss: 0.6090309023857117\n",
            "5530 val_loss: 0.6114562153816223, train_loss: 0.6087396144866943\n",
            "5540 val_loss: 0.6109752655029297, train_loss: 0.6083982586860657\n",
            "5550 val_loss: 0.6106830835342407, train_loss: 0.6081734299659729\n",
            "5560 val_loss: 0.6103375554084778, train_loss: 0.6079453229904175\n",
            "5570 val_loss: 0.6102282404899597, train_loss: 0.6077890396118164\n",
            "5580 val_loss: 0.6098854541778564, train_loss: 0.6075986623764038\n",
            "5590 val_loss: 0.6097257137298584, train_loss: 0.6074668169021606\n",
            "5600 val_loss: 0.609662652015686, train_loss: 0.6073017716407776\n",
            "5610 val_loss: 0.6094748377799988, train_loss: 0.6072283983230591\n",
            "5620 val_loss: 0.6092461347579956, train_loss: 0.6070858240127563\n",
            "5630 val_loss: 0.6093142032623291, train_loss: 0.6070083975791931\n",
            "5640 val_loss: 0.6090800762176514, train_loss: 0.6067920327186584\n",
            "5650 val_loss: 0.6087974309921265, train_loss: 0.6065917611122131\n",
            "5660 val_loss: 0.6087458729743958, train_loss: 0.6065469980239868\n",
            "5670 val_loss: 0.6088342666625977, train_loss: 0.6063793897628784\n",
            "5680 val_loss: 0.6083491444587708, train_loss: 0.6061356067657471\n",
            "5690 val_loss: 0.6082218885421753, train_loss: 0.6059436202049255\n",
            "5700 val_loss: 0.6079646348953247, train_loss: 0.6058782339096069\n",
            "5710 val_loss: 0.6077215075492859, train_loss: 0.6057218909263611\n",
            "5720 val_loss: 0.6073668599128723, train_loss: 0.6054142713546753\n",
            "5730 val_loss: 0.606975793838501, train_loss: 0.6051961183547974\n",
            "5740 val_loss: 0.606854259967804, train_loss: 0.6051638722419739\n",
            "5750 val_loss: 0.6068010926246643, train_loss: 0.6049778461456299\n",
            "5760 val_loss: 0.6065722703933716, train_loss: 0.6047394871711731\n",
            "5770 val_loss: 0.6063623428344727, train_loss: 0.6046431660652161\n",
            "5780 val_loss: 0.6063714623451233, train_loss: 0.6044951677322388\n",
            "5790 val_loss: 0.6063219904899597, train_loss: 0.6043784022331238\n",
            "5800 val_loss: 0.6061162352561951, train_loss: 0.6040830612182617\n",
            "5810 val_loss: 0.6058634519577026, train_loss: 0.6038650870323181\n",
            "5820 val_loss: 0.6054764986038208, train_loss: 0.6036843061447144\n",
            "5830 val_loss: 0.6054474115371704, train_loss: 0.6036405563354492\n",
            "5840 val_loss: 0.6052955389022827, train_loss: 0.6034671664237976\n",
            "5850 val_loss: 0.6052611470222473, train_loss: 0.6033222675323486\n",
            "5860 val_loss: 0.6051745414733887, train_loss: 0.6031450629234314\n",
            "5870 val_loss: 0.6049972772598267, train_loss: 0.6029499769210815\n",
            "5880 val_loss: 0.6048609018325806, train_loss: 0.6028245687484741\n",
            "5890 val_loss: 0.6046655774116516, train_loss: 0.6026393175125122\n",
            "5900 val_loss: 0.604297935962677, train_loss: 0.6024566888809204\n",
            "5910 val_loss: 0.6041351556777954, train_loss: 0.6023196578025818\n",
            "5920 val_loss: 0.6039586067199707, train_loss: 0.602107584476471\n",
            "5930 val_loss: 0.6036795377731323, train_loss: 0.6018645167350769\n",
            "5940 val_loss: 0.6035979390144348, train_loss: 0.6017791032791138\n",
            "5950 val_loss: 0.6033595204353333, train_loss: 0.60170978307724\n",
            "5960 val_loss: 0.6031261086463928, train_loss: 0.6015623211860657\n",
            "5970 val_loss: 0.6031267642974854, train_loss: 0.6014155149459839\n",
            "5980 val_loss: 0.6030212640762329, train_loss: 0.601379930973053\n",
            "5990 val_loss: 0.6028286814689636, train_loss: 0.6012709140777588\n",
            "6000 val_loss: 0.6028269529342651, train_loss: 0.6011606454849243\n",
            "6010 val_loss: 0.602933406829834, train_loss: 0.6010597944259644\n",
            "6020 val_loss: 0.6029488444328308, train_loss: 0.6009480953216553\n",
            "6030 val_loss: 0.6026135087013245, train_loss: 0.6008623838424683\n",
            "6040 val_loss: 0.6024283170700073, train_loss: 0.6006767749786377\n",
            "6050 val_loss: 0.6023871898651123, train_loss: 0.6004834175109863\n",
            "6060 val_loss: 0.6020447611808777, train_loss: 0.6003344058990479\n",
            "6070 val_loss: 0.6017912030220032, train_loss: 0.600047767162323\n",
            "6080 val_loss: 0.6015965342521667, train_loss: 0.5998185873031616\n",
            "6090 val_loss: 0.6014218330383301, train_loss: 0.5996910333633423\n",
            "6100 val_loss: 0.6013422608375549, train_loss: 0.599516749382019\n",
            "6110 val_loss: 0.601425290107727, train_loss: 0.5993648171424866\n",
            "6120 val_loss: 0.6011454463005066, train_loss: 0.5991799235343933\n",
            "6130 val_loss: 0.6009027361869812, train_loss: 0.599088728427887\n",
            "6140 val_loss: 0.6008232831954956, train_loss: 0.5990334153175354\n",
            "6150 val_loss: 0.600391149520874, train_loss: 0.5986687541007996\n",
            "6160 val_loss: 0.6004163026809692, train_loss: 0.598579466342926\n",
            "6170 val_loss: 0.599976122379303, train_loss: 0.5983620882034302\n",
            "6180 val_loss: 0.5997391939163208, train_loss: 0.5981928110122681\n",
            "6190 val_loss: 0.5997671484947205, train_loss: 0.598198652267456\n",
            "6200 val_loss: 0.5996521711349487, train_loss: 0.5980016589164734\n",
            "6210 val_loss: 0.5996999144554138, train_loss: 0.5979533791542053\n",
            "6220 val_loss: 0.5994362235069275, train_loss: 0.597766101360321\n",
            "6230 val_loss: 0.599259078502655, train_loss: 0.5977658033370972\n",
            "6240 val_loss: 0.5993134379386902, train_loss: 0.5976335406303406\n",
            "6250 val_loss: 0.5991250276565552, train_loss: 0.5974687337875366\n",
            "6260 val_loss: 0.5991584062576294, train_loss: 0.597380518913269\n",
            "6270 val_loss: 0.5989680290222168, train_loss: 0.5972079038619995\n",
            "6280 val_loss: 0.5989850163459778, train_loss: 0.5971452593803406\n",
            "6290 val_loss: 0.5987796783447266, train_loss: 0.5969545245170593\n",
            "6300 val_loss: 0.5985170006752014, train_loss: 0.5967483520507812\n",
            "6310 val_loss: 0.5983124375343323, train_loss: 0.5965422987937927\n",
            "6320 val_loss: 0.5980904698371887, train_loss: 0.5964728593826294\n",
            "6330 val_loss: 0.5980396866798401, train_loss: 0.5963512063026428\n",
            "6340 val_loss: 0.5977253913879395, train_loss: 0.5961760878562927\n",
            "6350 val_loss: 0.5976181030273438, train_loss: 0.5959292054176331\n",
            "6360 val_loss: 0.5973889827728271, train_loss: 0.5958731770515442\n",
            "6370 val_loss: 0.5972456932067871, train_loss: 0.5957441329956055\n",
            "6380 val_loss: 0.5969998240470886, train_loss: 0.5954268574714661\n",
            "6390 val_loss: 0.596823513507843, train_loss: 0.59536212682724\n",
            "6400 val_loss: 0.5968239903450012, train_loss: 0.5953572392463684\n",
            "6410 val_loss: 0.5966448187828064, train_loss: 0.5951943397521973\n",
            "6420 val_loss: 0.5965254902839661, train_loss: 0.5949563980102539\n",
            "6430 val_loss: 0.5962739586830139, train_loss: 0.5947653651237488\n",
            "6440 val_loss: 0.5964592099189758, train_loss: 0.5947219133377075\n",
            "6450 val_loss: 0.596245288848877, train_loss: 0.5945801138877869\n",
            "6460 val_loss: 0.5961722731590271, train_loss: 0.5943669080734253\n",
            "6470 val_loss: 0.5959957838058472, train_loss: 0.5942468643188477\n",
            "6480 val_loss: 0.5956457853317261, train_loss: 0.5940291881561279\n",
            "6490 val_loss: 0.5956014394760132, train_loss: 0.5938295125961304\n",
            "6500 val_loss: 0.59550541639328, train_loss: 0.5936834216117859\n",
            "6510 val_loss: 0.595312774181366, train_loss: 0.5935348272323608\n",
            "6520 val_loss: 0.5950793027877808, train_loss: 0.5933533310890198\n",
            "6530 val_loss: 0.5947885513305664, train_loss: 0.5931153297424316\n",
            "6540 val_loss: 0.5945723652839661, train_loss: 0.5930508971214294\n",
            "6550 val_loss: 0.5944182276725769, train_loss: 0.5928375720977783\n",
            "6560 val_loss: 0.5940924882888794, train_loss: 0.5926766395568848\n",
            "6570 val_loss: 0.5939753651618958, train_loss: 0.5925447344779968\n",
            "6580 val_loss: 0.5939902663230896, train_loss: 0.5924328565597534\n",
            "6590 val_loss: 0.5937662720680237, train_loss: 0.592258095741272\n",
            "6600 val_loss: 0.5936889052391052, train_loss: 0.5920707583427429\n",
            "6610 val_loss: 0.5936886072158813, train_loss: 0.5920354723930359\n",
            "6620 val_loss: 0.5935370326042175, train_loss: 0.5918643474578857\n",
            "6630 val_loss: 0.5934966206550598, train_loss: 0.5916658639907837\n",
            "6640 val_loss: 0.5936398506164551, train_loss: 0.5915749669075012\n",
            "6650 val_loss: 0.5934016108512878, train_loss: 0.5915260910987854\n",
            "6660 val_loss: 0.5931822061538696, train_loss: 0.5913406610488892\n",
            "6670 val_loss: 0.5929356217384338, train_loss: 0.5911549925804138\n",
            "6680 val_loss: 0.592841625213623, train_loss: 0.5909183025360107\n",
            "6690 val_loss: 0.5923855900764465, train_loss: 0.5906869769096375\n",
            "6700 val_loss: 0.5922929644584656, train_loss: 0.5905781984329224\n",
            "6710 val_loss: 0.5923270583152771, train_loss: 0.5905010104179382\n",
            "6720 val_loss: 0.5921155214309692, train_loss: 0.590318500995636\n",
            "6730 val_loss: 0.592037558555603, train_loss: 0.5902622938156128\n",
            "6740 val_loss: 0.5916988253593445, train_loss: 0.589998722076416\n",
            "6750 val_loss: 0.5914452075958252, train_loss: 0.5897382497787476\n",
            "6760 val_loss: 0.5913680195808411, train_loss: 0.5895848274230957\n",
            "6770 val_loss: 0.5911073088645935, train_loss: 0.5894457101821899\n",
            "6780 val_loss: 0.59095698595047, train_loss: 0.5893798470497131\n",
            "6790 val_loss: 0.5907496213912964, train_loss: 0.589179515838623\n",
            "6800 val_loss: 0.5906404256820679, train_loss: 0.5889194011688232\n",
            "6810 val_loss: 0.5905564427375793, train_loss: 0.5887634754180908\n",
            "6820 val_loss: 0.5903976559638977, train_loss: 0.5886327624320984\n",
            "6830 val_loss: 0.5904320478439331, train_loss: 0.588391125202179\n",
            "6840 val_loss: 0.5902264714241028, train_loss: 0.5881359577178955\n",
            "6850 val_loss: 0.5901223421096802, train_loss: 0.5880827903747559\n",
            "6860 val_loss: 0.5897772908210754, train_loss: 0.5877793431282043\n",
            "6870 val_loss: 0.5894010066986084, train_loss: 0.5874480605125427\n",
            "6880 val_loss: 0.5893216133117676, train_loss: 0.5873962044715881\n",
            "6890 val_loss: 0.5890825986862183, train_loss: 0.5871919393539429\n",
            "6900 val_loss: 0.5889776349067688, train_loss: 0.5870673656463623\n",
            "6910 val_loss: 0.5890358686447144, train_loss: 0.5869370698928833\n",
            "6920 val_loss: 0.5888607501983643, train_loss: 0.5867076516151428\n",
            "6930 val_loss: 0.5887089967727661, train_loss: 0.5865599513053894\n",
            "6940 val_loss: 0.5886368751525879, train_loss: 0.586461067199707\n",
            "6950 val_loss: 0.5884369611740112, train_loss: 0.5863019824028015\n",
            "6960 val_loss: 0.5883446335792542, train_loss: 0.5861758589744568\n",
            "6970 val_loss: 0.5881390571594238, train_loss: 0.5860718488693237\n",
            "6980 val_loss: 0.5880264043807983, train_loss: 0.58576899766922\n",
            "6990 val_loss: 0.5878139734268188, train_loss: 0.5854128003120422\n",
            "7000 val_loss: 0.5876692533493042, train_loss: 0.5851942300796509\n",
            "7010 val_loss: 0.5873951315879822, train_loss: 0.5851131081581116\n",
            "7020 val_loss: 0.5871762633323669, train_loss: 0.5849299430847168\n",
            "7030 val_loss: 0.5871214270591736, train_loss: 0.5847318172454834\n",
            "7040 val_loss: 0.5869765281677246, train_loss: 0.5844873189926147\n",
            "7050 val_loss: 0.5867056250572205, train_loss: 0.5842885971069336\n",
            "7060 val_loss: 0.586494505405426, train_loss: 0.5840914845466614\n",
            "7070 val_loss: 0.5863066911697388, train_loss: 0.5838852524757385\n",
            "7080 val_loss: 0.5861282348632812, train_loss: 0.5836690664291382\n",
            "7090 val_loss: 0.5859858393669128, train_loss: 0.5836304426193237\n",
            "7100 val_loss: 0.585942804813385, train_loss: 0.5835062265396118\n",
            "7110 val_loss: 0.5857594013214111, train_loss: 0.5833058953285217\n",
            "7120 val_loss: 0.5857400298118591, train_loss: 0.5832056999206543\n",
            "7130 val_loss: 0.5855915546417236, train_loss: 0.5830667018890381\n",
            "7140 val_loss: 0.5853369832038879, train_loss: 0.5827426314353943\n",
            "7150 val_loss: 0.5852323174476624, train_loss: 0.5826020240783691\n",
            "7160 val_loss: 0.5850775837898254, train_loss: 0.5823199152946472\n",
            "7170 val_loss: 0.5848053693771362, train_loss: 0.5820813179016113\n",
            "7180 val_loss: 0.584679126739502, train_loss: 0.5819636583328247\n",
            "7190 val_loss: 0.5846694111824036, train_loss: 0.5818500518798828\n",
            "7200 val_loss: 0.5843797326087952, train_loss: 0.5816831588745117\n",
            "7210 val_loss: 0.5843337774276733, train_loss: 0.5815507769584656\n",
            "7220 val_loss: 0.5839409828186035, train_loss: 0.5812165141105652\n",
            "7230 val_loss: 0.58368980884552, train_loss: 0.5808870196342468\n",
            "7240 val_loss: 0.5834605097770691, train_loss: 0.5807468295097351\n",
            "7250 val_loss: 0.5833989977836609, train_loss: 0.5804264545440674\n",
            "7260 val_loss: 0.583189845085144, train_loss: 0.5802440047264099\n",
            "7270 val_loss: 0.5829567909240723, train_loss: 0.5800737142562866\n",
            "7280 val_loss: 0.582752525806427, train_loss: 0.5799224376678467\n",
            "7290 val_loss: 0.5826051831245422, train_loss: 0.579716682434082\n",
            "7300 val_loss: 0.582348644733429, train_loss: 0.5795209407806396\n",
            "7310 val_loss: 0.5819997191429138, train_loss: 0.5792425870895386\n",
            "7320 val_loss: 0.5818659663200378, train_loss: 0.578983724117279\n",
            "7330 val_loss: 0.5815184712409973, train_loss: 0.5788060426712036\n",
            "7340 val_loss: 0.5813871026039124, train_loss: 0.5787347555160522\n",
            "7350 val_loss: 0.5812374949455261, train_loss: 0.578392744064331\n",
            "7360 val_loss: 0.5810481309890747, train_loss: 0.5781755447387695\n",
            "7370 val_loss: 0.5808407068252563, train_loss: 0.5780627727508545\n",
            "7380 val_loss: 0.5807328820228577, train_loss: 0.5779892206192017\n",
            "7390 val_loss: 0.5807229280471802, train_loss: 0.5779000520706177\n",
            "7400 val_loss: 0.5804902911186218, train_loss: 0.5776745676994324\n",
            "7410 val_loss: 0.5803540349006653, train_loss: 0.5774356126785278\n",
            "7420 val_loss: 0.5800621509552002, train_loss: 0.5772081017494202\n",
            "7430 val_loss: 0.5798386335372925, train_loss: 0.5770449638366699\n",
            "7440 val_loss: 0.5796425938606262, train_loss: 0.5768185257911682\n",
            "7450 val_loss: 0.5795450210571289, train_loss: 0.5766566395759583\n",
            "7460 val_loss: 0.5794147849082947, train_loss: 0.5764984488487244\n",
            "7470 val_loss: 0.5791492462158203, train_loss: 0.5764251351356506\n",
            "7480 val_loss: 0.5789881944656372, train_loss: 0.5761953592300415\n",
            "7490 val_loss: 0.5787073373794556, train_loss: 0.5759016275405884\n",
            "7500 val_loss: 0.5784358382225037, train_loss: 0.5757016539573669\n",
            "7510 val_loss: 0.5783419609069824, train_loss: 0.5755257606506348\n",
            "7520 val_loss: 0.5781392455101013, train_loss: 0.5753283500671387\n",
            "7530 val_loss: 0.578004002571106, train_loss: 0.5749848484992981\n",
            "7540 val_loss: 0.5778072476387024, train_loss: 0.574883222579956\n",
            "7550 val_loss: 0.5775454044342041, train_loss: 0.5745598077774048\n",
            "7560 val_loss: 0.5773219466209412, train_loss: 0.5743934512138367\n",
            "7570 val_loss: 0.5772467255592346, train_loss: 0.5742524862289429\n",
            "7580 val_loss: 0.5769791603088379, train_loss: 0.5739018321037292\n",
            "7590 val_loss: 0.5768701434135437, train_loss: 0.573630690574646\n",
            "7600 val_loss: 0.5768172144889832, train_loss: 0.5735790729522705\n",
            "7610 val_loss: 0.5764992833137512, train_loss: 0.5733199119567871\n",
            "7620 val_loss: 0.576281726360321, train_loss: 0.5730469226837158\n",
            "7630 val_loss: 0.5760035514831543, train_loss: 0.5729357004165649\n",
            "7640 val_loss: 0.575954794883728, train_loss: 0.5726698040962219\n",
            "7650 val_loss: 0.5757937431335449, train_loss: 0.5725910067558289\n",
            "7660 val_loss: 0.5755942463874817, train_loss: 0.5723918080329895\n",
            "7670 val_loss: 0.5752539038658142, train_loss: 0.5721660256385803\n",
            "7680 val_loss: 0.5749095678329468, train_loss: 0.5717523097991943\n",
            "7690 val_loss: 0.5746632814407349, train_loss: 0.5715596079826355\n",
            "7700 val_loss: 0.574626624584198, train_loss: 0.5714114308357239\n",
            "7710 val_loss: 0.5743156671524048, train_loss: 0.5711530447006226\n",
            "7720 val_loss: 0.5740495920181274, train_loss: 0.5708757638931274\n",
            "7730 val_loss: 0.5739378929138184, train_loss: 0.5706937909126282\n",
            "7740 val_loss: 0.5737948417663574, train_loss: 0.5704032778739929\n",
            "7750 val_loss: 0.5736214518547058, train_loss: 0.5702441930770874\n",
            "7760 val_loss: 0.5735393166542053, train_loss: 0.5699965357780457\n",
            "7770 val_loss: 0.5734117031097412, train_loss: 0.5697734951972961\n",
            "7780 val_loss: 0.5732735395431519, train_loss: 0.5695498585700989\n",
            "7790 val_loss: 0.5729559063911438, train_loss: 0.5692886710166931\n",
            "7800 val_loss: 0.5726994276046753, train_loss: 0.5689964890480042\n",
            "7810 val_loss: 0.5725582242012024, train_loss: 0.5688337683677673\n",
            "7820 val_loss: 0.5720393061637878, train_loss: 0.5683777928352356\n",
            "7830 val_loss: 0.5718472599983215, train_loss: 0.5680191516876221\n",
            "7840 val_loss: 0.5716788172721863, train_loss: 0.567650556564331\n",
            "7850 val_loss: 0.5714073181152344, train_loss: 0.5672886967658997\n",
            "7860 val_loss: 0.5710583925247192, train_loss: 0.5669234395027161\n",
            "7870 val_loss: 0.5708451271057129, train_loss: 0.5666354894638062\n",
            "7880 val_loss: 0.5705150365829468, train_loss: 0.5662828683853149\n",
            "7890 val_loss: 0.5703755617141724, train_loss: 0.5661397576332092\n",
            "7900 val_loss: 0.569850504398346, train_loss: 0.5656001567840576\n",
            "7910 val_loss: 0.5699000358581543, train_loss: 0.5655829310417175\n",
            "7920 val_loss: 0.5697224736213684, train_loss: 0.5653547048568726\n",
            "7930 val_loss: 0.569412112236023, train_loss: 0.5650832653045654\n",
            "7940 val_loss: 0.5692481994628906, train_loss: 0.5648300647735596\n",
            "7950 val_loss: 0.5689868330955505, train_loss: 0.5644676685333252\n",
            "7960 val_loss: 0.5686052441596985, train_loss: 0.5640597939491272\n",
            "7970 val_loss: 0.5682169795036316, train_loss: 0.5636633038520813\n",
            "7980 val_loss: 0.5679783225059509, train_loss: 0.563409149646759\n",
            "7990 val_loss: 0.5677762627601624, train_loss: 0.563144862651825\n",
            "8000 val_loss: 0.5675116777420044, train_loss: 0.5628044009208679\n",
            "8010 val_loss: 0.5671948790550232, train_loss: 0.5624730587005615\n",
            "8020 val_loss: 0.5668993592262268, train_loss: 0.5621185898780823\n",
            "8030 val_loss: 0.5664306879043579, train_loss: 0.5616574287414551\n",
            "8040 val_loss: 0.5663602352142334, train_loss: 0.5614825487136841\n",
            "8050 val_loss: 0.5660677552223206, train_loss: 0.5612598061561584\n",
            "8060 val_loss: 0.5658656358718872, train_loss: 0.5610691905021667\n",
            "8070 val_loss: 0.5654690861701965, train_loss: 0.5605299472808838\n",
            "8080 val_loss: 0.5651754140853882, train_loss: 0.5600978136062622\n",
            "8090 val_loss: 0.5647174715995789, train_loss: 0.5597266554832458\n",
            "8100 val_loss: 0.5645036101341248, train_loss: 0.5594866871833801\n",
            "8110 val_loss: 0.5643334984779358, train_loss: 0.5592591166496277\n",
            "8120 val_loss: 0.5640179514884949, train_loss: 0.5589051246643066\n",
            "8130 val_loss: 0.5636868476867676, train_loss: 0.5585359334945679\n",
            "8140 val_loss: 0.5632476806640625, train_loss: 0.5580840110778809\n",
            "8150 val_loss: 0.5628820657730103, train_loss: 0.5576756000518799\n",
            "8160 val_loss: 0.5626928210258484, train_loss: 0.5574308633804321\n",
            "8170 val_loss: 0.562468945980072, train_loss: 0.5571774244308472\n",
            "8180 val_loss: 0.5621726512908936, train_loss: 0.5567793846130371\n",
            "8190 val_loss: 0.5618398785591125, train_loss: 0.5563706159591675\n",
            "8200 val_loss: 0.561403214931488, train_loss: 0.5560051798820496\n",
            "8210 val_loss: 0.5609057545661926, train_loss: 0.5554579496383667\n",
            "8220 val_loss: 0.5605728030204773, train_loss: 0.5552029609680176\n",
            "8230 val_loss: 0.5600427985191345, train_loss: 0.5546259880065918\n",
            "8240 val_loss: 0.5594925284385681, train_loss: 0.5542386770248413\n",
            "8250 val_loss: 0.5590718984603882, train_loss: 0.5538283586502075\n",
            "8260 val_loss: 0.5587480664253235, train_loss: 0.5534314513206482\n",
            "8270 val_loss: 0.5584851503372192, train_loss: 0.5531595349311829\n",
            "8280 val_loss: 0.5579005479812622, train_loss: 0.5526989102363586\n",
            "8290 val_loss: 0.5574902296066284, train_loss: 0.5522814989089966\n",
            "8300 val_loss: 0.5570148825645447, train_loss: 0.5517567992210388\n",
            "8310 val_loss: 0.5565062165260315, train_loss: 0.5512194633483887\n",
            "8320 val_loss: 0.5561042428016663, train_loss: 0.5507702231407166\n",
            "8330 val_loss: 0.5557681322097778, train_loss: 0.5504470467567444\n",
            "8340 val_loss: 0.555530309677124, train_loss: 0.5500965714454651\n",
            "8350 val_loss: 0.5551391839981079, train_loss: 0.5497145652770996\n",
            "8360 val_loss: 0.5548736453056335, train_loss: 0.5494062304496765\n",
            "8370 val_loss: 0.5545501112937927, train_loss: 0.549091637134552\n",
            "8380 val_loss: 0.5540693998336792, train_loss: 0.5487446188926697\n",
            "8390 val_loss: 0.553516149520874, train_loss: 0.548030436038971\n",
            "8400 val_loss: 0.5531742572784424, train_loss: 0.5476118922233582\n",
            "8410 val_loss: 0.5527861714363098, train_loss: 0.5470492839813232\n",
            "8420 val_loss: 0.5523634552955627, train_loss: 0.5465940833091736\n",
            "8430 val_loss: 0.5520000457763672, train_loss: 0.5461955070495605\n",
            "8440 val_loss: 0.5514198541641235, train_loss: 0.5455213785171509\n",
            "8450 val_loss: 0.5511384010314941, train_loss: 0.5453270673751831\n",
            "8460 val_loss: 0.5508332252502441, train_loss: 0.5449395775794983\n",
            "8470 val_loss: 0.5504509210586548, train_loss: 0.5445430278778076\n",
            "8480 val_loss: 0.5500794053077698, train_loss: 0.544176459312439\n",
            "8490 val_loss: 0.5495539903640747, train_loss: 0.543648898601532\n",
            "8500 val_loss: 0.5490842461585999, train_loss: 0.5432297587394714\n",
            "8510 val_loss: 0.5486226081848145, train_loss: 0.5426803231239319\n",
            "8520 val_loss: 0.5483914613723755, train_loss: 0.5424314737319946\n",
            "8530 val_loss: 0.5477584004402161, train_loss: 0.5417989492416382\n",
            "8540 val_loss: 0.5473716259002686, train_loss: 0.5413919687271118\n",
            "8550 val_loss: 0.5469319224357605, train_loss: 0.5407350659370422\n",
            "8560 val_loss: 0.5464328527450562, train_loss: 0.5403305888175964\n",
            "8570 val_loss: 0.5458946228027344, train_loss: 0.5397782325744629\n",
            "8580 val_loss: 0.5453571081161499, train_loss: 0.5391932725906372\n",
            "8590 val_loss: 0.5449572801589966, train_loss: 0.5388005375862122\n",
            "8600 val_loss: 0.5444783568382263, train_loss: 0.5383170247077942\n",
            "8610 val_loss: 0.5439543128013611, train_loss: 0.5378372669219971\n",
            "8620 val_loss: 0.5436951518058777, train_loss: 0.5374528765678406\n",
            "8630 val_loss: 0.5432088971138, train_loss: 0.5369429588317871\n",
            "8640 val_loss: 0.5429022908210754, train_loss: 0.5364884734153748\n",
            "8650 val_loss: 0.5425719022750854, train_loss: 0.5361049175262451\n",
            "8660 val_loss: 0.5422198176383972, train_loss: 0.5355995893478394\n",
            "8670 val_loss: 0.5416285395622253, train_loss: 0.5350499153137207\n",
            "8680 val_loss: 0.5412226319313049, train_loss: 0.5345404744148254\n",
            "8690 val_loss: 0.54078209400177, train_loss: 0.5340667366981506\n",
            "8700 val_loss: 0.5403481125831604, train_loss: 0.5337096452713013\n",
            "8710 val_loss: 0.5400503277778625, train_loss: 0.5332555770874023\n",
            "8720 val_loss: 0.5394537448883057, train_loss: 0.5326768755912781\n",
            "8730 val_loss: 0.5388897657394409, train_loss: 0.5321184396743774\n",
            "8740 val_loss: 0.538279116153717, train_loss: 0.5314152240753174\n",
            "8750 val_loss: 0.53779536485672, train_loss: 0.5308663845062256\n",
            "8760 val_loss: 0.5372307300567627, train_loss: 0.5303460955619812\n",
            "8770 val_loss: 0.5368340015411377, train_loss: 0.5298196077346802\n",
            "8780 val_loss: 0.5363761782646179, train_loss: 0.5293542742729187\n",
            "8790 val_loss: 0.5359399318695068, train_loss: 0.5288733839988708\n",
            "8800 val_loss: 0.5357557535171509, train_loss: 0.5285369157791138\n",
            "8810 val_loss: 0.5350989103317261, train_loss: 0.5278904438018799\n",
            "8820 val_loss: 0.5347450375556946, train_loss: 0.5274531841278076\n",
            "8830 val_loss: 0.534176230430603, train_loss: 0.5268597602844238\n",
            "8840 val_loss: 0.5336724519729614, train_loss: 0.5262000560760498\n",
            "8850 val_loss: 0.5332218408584595, train_loss: 0.5257108211517334\n",
            "8860 val_loss: 0.5328881144523621, train_loss: 0.525201141834259\n",
            "8870 val_loss: 0.5322719216346741, train_loss: 0.5246323943138123\n",
            "8880 val_loss: 0.5317436456680298, train_loss: 0.5240607261657715\n",
            "8890 val_loss: 0.5314305424690247, train_loss: 0.5235449075698853\n",
            "8900 val_loss: 0.5309833288192749, train_loss: 0.5229715704917908\n",
            "8910 val_loss: 0.5303705334663391, train_loss: 0.5223137736320496\n",
            "8920 val_loss: 0.5301137566566467, train_loss: 0.5219363570213318\n",
            "8930 val_loss: 0.5296441912651062, train_loss: 0.521391749382019\n",
            "8940 val_loss: 0.5294619798660278, train_loss: 0.5209252238273621\n",
            "8950 val_loss: 0.5286321043968201, train_loss: 0.5201992392539978\n",
            "8960 val_loss: 0.5279610753059387, train_loss: 0.5196038484573364\n",
            "8970 val_loss: 0.5278629660606384, train_loss: 0.5193656086921692\n",
            "8980 val_loss: 0.527458667755127, train_loss: 0.5188406109809875\n",
            "8990 val_loss: 0.5271332859992981, train_loss: 0.5183871388435364\n",
            "9000 val_loss: 0.5266045331954956, train_loss: 0.5178608894348145\n",
            "9010 val_loss: 0.5261720418930054, train_loss: 0.5174108147621155\n",
            "9020 val_loss: 0.5257084369659424, train_loss: 0.5169172286987305\n",
            "9030 val_loss: 0.5252253413200378, train_loss: 0.516252875328064\n",
            "9040 val_loss: 0.5247632265090942, train_loss: 0.5157504677772522\n",
            "9050 val_loss: 0.5240346193313599, train_loss: 0.5150596499443054\n",
            "9060 val_loss: 0.5235846638679504, train_loss: 0.5144948959350586\n",
            "9070 val_loss: 0.5228094458580017, train_loss: 0.5138666033744812\n",
            "9080 val_loss: 0.5223702192306519, train_loss: 0.5134721994400024\n",
            "9090 val_loss: 0.522020161151886, train_loss: 0.5131463408470154\n",
            "9100 val_loss: 0.5218862891197205, train_loss: 0.5127697587013245\n",
            "9110 val_loss: 0.521327555179596, train_loss: 0.5122542381286621\n",
            "9120 val_loss: 0.5207787156105042, train_loss: 0.511570930480957\n",
            "9130 val_loss: 0.5203630328178406, train_loss: 0.5111226439476013\n",
            "9140 val_loss: 0.5196865200996399, train_loss: 0.5104386806488037\n",
            "9150 val_loss: 0.5192546248435974, train_loss: 0.509890615940094\n",
            "9160 val_loss: 0.5188248157501221, train_loss: 0.5092336535453796\n",
            "9170 val_loss: 0.5181118845939636, train_loss: 0.508586049079895\n",
            "9180 val_loss: 0.51755291223526, train_loss: 0.5079812407493591\n",
            "9190 val_loss: 0.5169757008552551, train_loss: 0.5073671340942383\n",
            "9200 val_loss: 0.5167237520217896, train_loss: 0.5069918632507324\n",
            "9210 val_loss: 0.51613849401474, train_loss: 0.5063885450363159\n",
            "9220 val_loss: 0.5157236456871033, train_loss: 0.5058038830757141\n",
            "9230 val_loss: 0.5148497223854065, train_loss: 0.5051364302635193\n",
            "9240 val_loss: 0.5147501230239868, train_loss: 0.5046420693397522\n",
            "9250 val_loss: 0.5143738985061646, train_loss: 0.5042119026184082\n",
            "9260 val_loss: 0.5139754414558411, train_loss: 0.5038232803344727\n",
            "9270 val_loss: 0.513689398765564, train_loss: 0.5033593773841858\n",
            "9280 val_loss: 0.5135148763656616, train_loss: 0.5028697848320007\n",
            "9290 val_loss: 0.513177752494812, train_loss: 0.5024707317352295\n",
            "9300 val_loss: 0.5127846002578735, train_loss: 0.5018839836120605\n",
            "9310 val_loss: 0.512362003326416, train_loss: 0.5013233423233032\n",
            "9320 val_loss: 0.5121050477027893, train_loss: 0.5008887648582458\n",
            "9330 val_loss: 0.5115339159965515, train_loss: 0.500325083732605\n",
            "9340 val_loss: 0.51103675365448, train_loss: 0.49967053532600403\n",
            "9350 val_loss: 0.5102115273475647, train_loss: 0.4990186393260956\n",
            "9360 val_loss: 0.5096399784088135, train_loss: 0.49842149019241333\n",
            "9370 val_loss: 0.5094035267829895, train_loss: 0.49798986315727234\n",
            "9380 val_loss: 0.5090287327766418, train_loss: 0.49748992919921875\n",
            "9390 val_loss: 0.5087658166885376, train_loss: 0.4969809055328369\n",
            "9400 val_loss: 0.5083687901496887, train_loss: 0.4964238405227661\n",
            "9410 val_loss: 0.5080053806304932, train_loss: 0.49601659178733826\n",
            "9420 val_loss: 0.5076178312301636, train_loss: 0.49557650089263916\n",
            "9430 val_loss: 0.5072008967399597, train_loss: 0.49496281147003174\n",
            "9440 val_loss: 0.5066792964935303, train_loss: 0.4942948818206787\n",
            "9450 val_loss: 0.5065491795539856, train_loss: 0.49385762214660645\n",
            "9460 val_loss: 0.5060026049613953, train_loss: 0.4932573139667511\n",
            "9470 val_loss: 0.5054606199264526, train_loss: 0.49261462688446045\n",
            "9480 val_loss: 0.5051836967468262, train_loss: 0.49207282066345215\n",
            "9490 val_loss: 0.5048438310623169, train_loss: 0.4915430545806885\n",
            "9500 val_loss: 0.5042909979820251, train_loss: 0.4909801185131073\n",
            "9510 val_loss: 0.5036119222640991, train_loss: 0.49040472507476807\n",
            "9520 val_loss: 0.5033804774284363, train_loss: 0.48987317085266113\n",
            "9530 val_loss: 0.5031295418739319, train_loss: 0.48945653438568115\n",
            "9540 val_loss: 0.5025179982185364, train_loss: 0.48871278762817383\n",
            "9550 val_loss: 0.5022618770599365, train_loss: 0.4882461726665497\n",
            "9560 val_loss: 0.5018223524093628, train_loss: 0.48774901032447815\n",
            "9570 val_loss: 0.5011994242668152, train_loss: 0.48708659410476685\n",
            "9580 val_loss: 0.5009240508079529, train_loss: 0.4866311252117157\n",
            "9590 val_loss: 0.5006693005561829, train_loss: 0.48633140325546265\n",
            "9600 val_loss: 0.5000542402267456, train_loss: 0.48564863204956055\n",
            "9610 val_loss: 0.49938100576400757, train_loss: 0.4849773049354553\n",
            "9620 val_loss: 0.4989367127418518, train_loss: 0.48439574241638184\n",
            "9630 val_loss: 0.49857330322265625, train_loss: 0.4838566780090332\n",
            "9640 val_loss: 0.4983426034450531, train_loss: 0.4833451211452484\n",
            "9650 val_loss: 0.49789583683013916, train_loss: 0.4827417731285095\n",
            "9660 val_loss: 0.4974573254585266, train_loss: 0.48225337266921997\n",
            "9670 val_loss: 0.4970107674598694, train_loss: 0.4818108081817627\n",
            "9680 val_loss: 0.49624648690223694, train_loss: 0.48097148537635803\n",
            "9690 val_loss: 0.49634188413619995, train_loss: 0.4805923104286194\n",
            "9700 val_loss: 0.49654343724250793, train_loss: 0.4804489314556122\n",
            "9710 val_loss: 0.4958367645740509, train_loss: 0.4795692265033722\n",
            "9720 val_loss: 0.4951282739639282, train_loss: 0.47868144512176514\n",
            "9730 val_loss: 0.4949359893798828, train_loss: 0.4782099425792694\n",
            "9740 val_loss: 0.4945088326931, train_loss: 0.47752416133880615\n",
            "9750 val_loss: 0.4939873516559601, train_loss: 0.47687798738479614\n",
            "9760 val_loss: 0.4933735132217407, train_loss: 0.47626471519470215\n",
            "9770 val_loss: 0.4930640757083893, train_loss: 0.4757346212863922\n",
            "9780 val_loss: 0.4924204349517822, train_loss: 0.4750312566757202\n",
            "9790 val_loss: 0.49199581146240234, train_loss: 0.47446972131729126\n",
            "9800 val_loss: 0.4915698766708374, train_loss: 0.4738183617591858\n",
            "9810 val_loss: 0.4915490746498108, train_loss: 0.4734534025192261\n",
            "9820 val_loss: 0.4917248785495758, train_loss: 0.47325384616851807\n",
            "9830 val_loss: 0.4911597967147827, train_loss: 0.4723941385746002\n",
            "9840 val_loss: 0.4905489385128021, train_loss: 0.47156304121017456\n",
            "9850 val_loss: 0.4900362491607666, train_loss: 0.4709847569465637\n",
            "9860 val_loss: 0.48983368277549744, train_loss: 0.4705943167209625\n",
            "9870 val_loss: 0.4895354211330414, train_loss: 0.4700114130973816\n",
            "9880 val_loss: 0.48919376730918884, train_loss: 0.46939799189567566\n",
            "9890 val_loss: 0.4888583719730377, train_loss: 0.4688224494457245\n",
            "9900 val_loss: 0.4881766140460968, train_loss: 0.468182772397995\n",
            "9910 val_loss: 0.4877742528915405, train_loss: 0.46759113669395447\n",
            "9920 val_loss: 0.4874584674835205, train_loss: 0.46697938442230225\n",
            "9930 val_loss: 0.4872036576271057, train_loss: 0.46642419695854187\n",
            "9940 val_loss: 0.4868955612182617, train_loss: 0.4659014344215393\n",
            "9950 val_loss: 0.4862421154975891, train_loss: 0.4652358889579773\n",
            "9960 val_loss: 0.4857126772403717, train_loss: 0.4643731117248535\n",
            "9970 val_loss: 0.48551228642463684, train_loss: 0.4638531804084778\n",
            "9980 val_loss: 0.48524466156959534, train_loss: 0.4632056951522827\n",
            "9990 val_loss: 0.4850849509239197, train_loss: 0.46281445026397705\n",
            "10000 val_loss: 0.4843011498451233, train_loss: 0.4617747366428375\n",
            "10010 val_loss: 0.48394352197647095, train_loss: 0.46114322543144226\n",
            "10020 val_loss: 0.48365187644958496, train_loss: 0.4605167806148529\n",
            "10030 val_loss: 0.4832276701927185, train_loss: 0.45984867215156555\n",
            "10040 val_loss: 0.48230889439582825, train_loss: 0.45863088965415955\n",
            "10050 val_loss: 0.4819685220718384, train_loss: 0.4579971730709076\n",
            "10060 val_loss: 0.4817347228527069, train_loss: 0.45762643218040466\n",
            "10070 val_loss: 0.4813292324542999, train_loss: 0.45670509338378906\n",
            "10080 val_loss: 0.48089754581451416, train_loss: 0.45600563287734985\n",
            "10090 val_loss: 0.480511337518692, train_loss: 0.4552895724773407\n",
            "10100 val_loss: 0.48012223839759827, train_loss: 0.4546242654323578\n",
            "10110 val_loss: 0.4796099364757538, train_loss: 0.45387929677963257\n",
            "10120 val_loss: 0.4791942238807678, train_loss: 0.4531567692756653\n",
            "10130 val_loss: 0.4789251685142517, train_loss: 0.45261573791503906\n",
            "10140 val_loss: 0.47842735052108765, train_loss: 0.4519338011741638\n",
            "10150 val_loss: 0.4779159128665924, train_loss: 0.45118317008018494\n",
            "10160 val_loss: 0.47736087441444397, train_loss: 0.4501732289791107\n",
            "10170 val_loss: 0.47718843817710876, train_loss: 0.4496433138847351\n",
            "10180 val_loss: 0.47680041193962097, train_loss: 0.4489918053150177\n",
            "10190 val_loss: 0.4763846695423126, train_loss: 0.4482506811618805\n",
            "10200 val_loss: 0.4760631024837494, train_loss: 0.4477098286151886\n",
            "10210 val_loss: 0.47550272941589355, train_loss: 0.4467460513114929\n",
            "10220 val_loss: 0.47504591941833496, train_loss: 0.44602352380752563\n",
            "10230 val_loss: 0.4750840365886688, train_loss: 0.44571802020072937\n",
            "10240 val_loss: 0.4747048020362854, train_loss: 0.44490480422973633\n",
            "10250 val_loss: 0.47402453422546387, train_loss: 0.44389304518699646\n",
            "10260 val_loss: 0.47414690256118774, train_loss: 0.4436376094818115\n",
            "10270 val_loss: 0.47359925508499146, train_loss: 0.44286665320396423\n",
            "10280 val_loss: 0.4728715121746063, train_loss: 0.44176238775253296\n",
            "10290 val_loss: 0.4726651906967163, train_loss: 0.4411138892173767\n",
            "10300 val_loss: 0.4722471833229065, train_loss: 0.4404035210609436\n",
            "10310 val_loss: 0.4716246426105499, train_loss: 0.4394069314002991\n",
            "10320 val_loss: 0.4712761342525482, train_loss: 0.43851926922798157\n",
            "10330 val_loss: 0.47097623348236084, train_loss: 0.43777450919151306\n",
            "10340 val_loss: 0.470289945602417, train_loss: 0.4366757273674011\n",
            "10350 val_loss: 0.4704107642173767, train_loss: 0.4365234673023224\n",
            "10360 val_loss: 0.46967247128486633, train_loss: 0.435437947511673\n",
            "10370 val_loss: 0.4692547619342804, train_loss: 0.4345611333847046\n",
            "10380 val_loss: 0.4686175286769867, train_loss: 0.4337043762207031\n",
            "10390 val_loss: 0.46810904145240784, train_loss: 0.43260639905929565\n",
            "10400 val_loss: 0.4679476320743561, train_loss: 0.4320191442966461\n",
            "10410 val_loss: 0.4677177965641022, train_loss: 0.4315296411514282\n",
            "10420 val_loss: 0.4672359824180603, train_loss: 0.43082940578460693\n",
            "10430 val_loss: 0.4667205810546875, train_loss: 0.4297502934932709\n",
            "10440 val_loss: 0.46646618843078613, train_loss: 0.42916277050971985\n",
            "10450 val_loss: 0.46569007635116577, train_loss: 0.4280163049697876\n",
            "10460 val_loss: 0.4653319716453552, train_loss: 0.42720454931259155\n",
            "10470 val_loss: 0.4648136794567108, train_loss: 0.4263644218444824\n",
            "10480 val_loss: 0.46459832787513733, train_loss: 0.4259386658668518\n",
            "10490 val_loss: 0.46411141753196716, train_loss: 0.4252275824546814\n",
            "10500 val_loss: 0.46361687779426575, train_loss: 0.4242261052131653\n",
            "10510 val_loss: 0.4630855917930603, train_loss: 0.423328697681427\n",
            "10520 val_loss: 0.4628613293170929, train_loss: 0.42265838384628296\n",
            "10530 val_loss: 0.4623587429523468, train_loss: 0.4218807518482208\n",
            "10540 val_loss: 0.46192827820777893, train_loss: 0.42099088430404663\n",
            "10550 val_loss: 0.4616212248802185, train_loss: 0.42059722542762756\n",
            "10560 val_loss: 0.46145662665367126, train_loss: 0.42004895210266113\n",
            "10570 val_loss: 0.4611022472381592, train_loss: 0.4194028675556183\n",
            "10580 val_loss: 0.46070170402526855, train_loss: 0.41849127411842346\n",
            "10590 val_loss: 0.46011874079704285, train_loss: 0.4176363945007324\n",
            "10600 val_loss: 0.4600977301597595, train_loss: 0.41709038615226746\n",
            "10610 val_loss: 0.45942360162734985, train_loss: 0.4160943031311035\n",
            "10620 val_loss: 0.459505170583725, train_loss: 0.41614091396331787\n",
            "10630 val_loss: 0.4587344527244568, train_loss: 0.4149748384952545\n",
            "10640 val_loss: 0.4581727087497711, train_loss: 0.41401785612106323\n",
            "10650 val_loss: 0.4577265679836273, train_loss: 0.41328904032707214\n",
            "10660 val_loss: 0.457476943731308, train_loss: 0.41291576623916626\n",
            "10670 val_loss: 0.4570368826389313, train_loss: 0.4121369421482086\n",
            "10680 val_loss: 0.45637696981430054, train_loss: 0.4109984338283539\n",
            "10690 val_loss: 0.45593270659446716, train_loss: 0.41009241342544556\n",
            "10700 val_loss: 0.455719530582428, train_loss: 0.4097367823123932\n",
            "10710 val_loss: 0.45452558994293213, train_loss: 0.40823644399642944\n",
            "10720 val_loss: 0.454739511013031, train_loss: 0.40820372104644775\n",
            "10730 val_loss: 0.45416712760925293, train_loss: 0.40707385540008545\n",
            "10740 val_loss: 0.4537922441959381, train_loss: 0.40653523802757263\n",
            "10750 val_loss: 0.45317625999450684, train_loss: 0.4057283103466034\n",
            "10760 val_loss: 0.4530125558376312, train_loss: 0.40535518527030945\n",
            "10770 val_loss: 0.4527040719985962, train_loss: 0.40477094054222107\n",
            "10780 val_loss: 0.452042818069458, train_loss: 0.40384840965270996\n",
            "10790 val_loss: 0.4516442120075226, train_loss: 0.40314966440200806\n",
            "10800 val_loss: 0.45143502950668335, train_loss: 0.402677446603775\n",
            "10810 val_loss: 0.45117151737213135, train_loss: 0.40197324752807617\n",
            "10820 val_loss: 0.45052751898765564, train_loss: 0.4009602665901184\n",
            "10830 val_loss: 0.4502005875110626, train_loss: 0.3999808430671692\n",
            "10840 val_loss: 0.4499850869178772, train_loss: 0.3995715081691742\n",
            "10850 val_loss: 0.44979172945022583, train_loss: 0.39914408326148987\n",
            "10860 val_loss: 0.4496382772922516, train_loss: 0.39825278520584106\n",
            "10870 val_loss: 0.4493260979652405, train_loss: 0.3975544273853302\n",
            "10880 val_loss: 0.4490535855293274, train_loss: 0.39670372009277344\n",
            "10890 val_loss: 0.44890084862709045, train_loss: 0.39608243107795715\n",
            "10900 val_loss: 0.4485020339488983, train_loss: 0.3950207233428955\n",
            "10910 val_loss: 0.4480758309364319, train_loss: 0.39417529106140137\n",
            "10920 val_loss: 0.44767528772354126, train_loss: 0.393154501914978\n",
            "10930 val_loss: 0.44748109579086304, train_loss: 0.39292871952056885\n",
            "10940 val_loss: 0.4473075866699219, train_loss: 0.3922607898712158\n",
            "10950 val_loss: 0.44696298241615295, train_loss: 0.3913973569869995\n",
            "10960 val_loss: 0.44678905606269836, train_loss: 0.3908115327358246\n",
            "10970 val_loss: 0.4461265206336975, train_loss: 0.39018550515174866\n",
            "10980 val_loss: 0.44626036286354065, train_loss: 0.3899692893028259\n",
            "10990 val_loss: 0.4466189742088318, train_loss: 0.38994330167770386\n",
            "11000 val_loss: 0.44579315185546875, train_loss: 0.38874953985214233\n",
            "11010 val_loss: 0.44556546211242676, train_loss: 0.3882436156272888\n",
            "11020 val_loss: 0.4447901248931885, train_loss: 0.3871200680732727\n",
            "11030 val_loss: 0.4443294405937195, train_loss: 0.3863998055458069\n",
            "11040 val_loss: 0.44461795687675476, train_loss: 0.38634413480758667\n",
            "11050 val_loss: 0.4445513188838959, train_loss: 0.3862200379371643\n",
            "11060 val_loss: 0.4441993832588196, train_loss: 0.38548725843429565\n",
            "11070 val_loss: 0.4439678192138672, train_loss: 0.3849077820777893\n",
            "11080 val_loss: 0.4432966709136963, train_loss: 0.3838423490524292\n",
            "11090 val_loss: 0.44304966926574707, train_loss: 0.3832699656486511\n",
            "11100 val_loss: 0.4427521526813507, train_loss: 0.3827000558376312\n",
            "11110 val_loss: 0.4425228536128998, train_loss: 0.38240620493888855\n",
            "11120 val_loss: 0.4422663748264313, train_loss: 0.38191574811935425\n",
            "11130 val_loss: 0.4417788088321686, train_loss: 0.38120612502098083\n",
            "11140 val_loss: 0.44132182002067566, train_loss: 0.3807489573955536\n",
            "11150 val_loss: 0.4412100315093994, train_loss: 0.3803633451461792\n",
            "11160 val_loss: 0.4410513937473297, train_loss: 0.38002827763557434\n",
            "11170 val_loss: 0.4406290054321289, train_loss: 0.37929201126098633\n",
            "11180 val_loss: 0.4405086040496826, train_loss: 0.37901750206947327\n",
            "11190 val_loss: 0.4401988983154297, train_loss: 0.37848079204559326\n",
            "11200 val_loss: 0.4398448169231415, train_loss: 0.37795907258987427\n",
            "11210 val_loss: 0.4397132396697998, train_loss: 0.3775414824485779\n",
            "11220 val_loss: 0.4393027722835541, train_loss: 0.3773325979709625\n",
            "11230 val_loss: 0.43899399042129517, train_loss: 0.37677454948425293\n",
            "11240 val_loss: 0.4383503198623657, train_loss: 0.37617039680480957\n",
            "11250 val_loss: 0.43809500336647034, train_loss: 0.3753916323184967\n",
            "11260 val_loss: 0.43801039457321167, train_loss: 0.3749644160270691\n",
            "11270 val_loss: 0.4375320076942444, train_loss: 0.37435826659202576\n",
            "11280 val_loss: 0.43691036105155945, train_loss: 0.37397128343582153\n",
            "11290 val_loss: 0.43667393922805786, train_loss: 0.37323492765426636\n",
            "11300 val_loss: 0.4362120032310486, train_loss: 0.37288016080856323\n",
            "11310 val_loss: 0.4358636736869812, train_loss: 0.37208276987075806\n",
            "11320 val_loss: 0.4355938732624054, train_loss: 0.3719346225261688\n",
            "11330 val_loss: 0.4352932870388031, train_loss: 0.37108683586120605\n",
            "11340 val_loss: 0.43500417470932007, train_loss: 0.37115517258644104\n",
            "11350 val_loss: 0.4349197745323181, train_loss: 0.3709680736064911\n",
            "11360 val_loss: 0.4347485899925232, train_loss: 0.3699455261230469\n",
            "11370 val_loss: 0.43453988432884216, train_loss: 0.3700537383556366\n",
            "11380 val_loss: 0.4342091977596283, train_loss: 0.36963051557540894\n",
            "11390 val_loss: 0.43379491567611694, train_loss: 0.3691970407962799\n",
            "11400 val_loss: 0.43325772881507874, train_loss: 0.3680427074432373\n",
            "11410 val_loss: 0.43307632207870483, train_loss: 0.36750975251197815\n",
            "11420 val_loss: 0.4326620101928711, train_loss: 0.3672793507575989\n",
            "11430 val_loss: 0.4323942959308624, train_loss: 0.3669602572917938\n",
            "11440 val_loss: 0.4322030246257782, train_loss: 0.3664287328720093\n",
            "11450 val_loss: 0.4317479729652405, train_loss: 0.3659147024154663\n",
            "11460 val_loss: 0.4310501217842102, train_loss: 0.3649533987045288\n",
            "11470 val_loss: 0.4308793246746063, train_loss: 0.3645729124546051\n",
            "11480 val_loss: 0.430367112159729, train_loss: 0.3641434907913208\n",
            "11490 val_loss: 0.43009141087532043, train_loss: 0.36352086067199707\n",
            "11500 val_loss: 0.4297178387641907, train_loss: 0.36279624700546265\n",
            "11510 val_loss: 0.42941638827323914, train_loss: 0.36216872930526733\n",
            "11520 val_loss: 0.4292714297771454, train_loss: 0.36223167181015015\n",
            "11530 val_loss: 0.4284791052341461, train_loss: 0.3613385856151581\n",
            "11540 val_loss: 0.42805004119873047, train_loss: 0.3605017066001892\n",
            "11550 val_loss: 0.4276493489742279, train_loss: 0.35994189977645874\n",
            "11560 val_loss: 0.42727628350257874, train_loss: 0.35959362983703613\n",
            "11570 val_loss: 0.42675963044166565, train_loss: 0.358739972114563\n",
            "11580 val_loss: 0.4264352023601532, train_loss: 0.3582158386707306\n",
            "11590 val_loss: 0.4259181618690491, train_loss: 0.3580106496810913\n",
            "11600 val_loss: 0.4258217215538025, train_loss: 0.35737165808677673\n",
            "11610 val_loss: 0.42540422081947327, train_loss: 0.35707494616508484\n",
            "11620 val_loss: 0.42506399750709534, train_loss: 0.3564686179161072\n",
            "11630 val_loss: 0.4245562255382538, train_loss: 0.3557388186454773\n",
            "11640 val_loss: 0.42455440759658813, train_loss: 0.35562530159950256\n",
            "11650 val_loss: 0.42383480072021484, train_loss: 0.35502180457115173\n",
            "11660 val_loss: 0.4236588180065155, train_loss: 0.35465773940086365\n",
            "11670 val_loss: 0.4233936071395874, train_loss: 0.35424143075942993\n",
            "11680 val_loss: 0.42326807975769043, train_loss: 0.3534359335899353\n",
            "11690 val_loss: 0.4225548803806305, train_loss: 0.3530735373497009\n",
            "11700 val_loss: 0.42242369055747986, train_loss: 0.35294660925865173\n",
            "11710 val_loss: 0.42184603214263916, train_loss: 0.35230207443237305\n",
            "11720 val_loss: 0.4217209219932556, train_loss: 0.35237401723861694\n",
            "11730 val_loss: 0.42111414670944214, train_loss: 0.35097888112068176\n",
            "11740 val_loss: 0.42055442929267883, train_loss: 0.35041505098342896\n",
            "11750 val_loss: 0.4201202392578125, train_loss: 0.3497658669948578\n",
            "11760 val_loss: 0.4198403060436249, train_loss: 0.3493221700191498\n",
            "11770 val_loss: 0.4192955791950226, train_loss: 0.34889817237854004\n",
            "11780 val_loss: 0.41891539096832275, train_loss: 0.3483564555644989\n",
            "11790 val_loss: 0.4189586043357849, train_loss: 0.3478379547595978\n",
            "11800 val_loss: 0.41803860664367676, train_loss: 0.34733960032463074\n",
            "11810 val_loss: 0.4178980886936188, train_loss: 0.3471105992794037\n",
            "11820 val_loss: 0.417470782995224, train_loss: 0.34626504778862\n",
            "11830 val_loss: 0.4171486496925354, train_loss: 0.34583842754364014\n",
            "11840 val_loss: 0.41680899262428284, train_loss: 0.3457774817943573\n",
            "11850 val_loss: 0.4168115258216858, train_loss: 0.3458418548107147\n",
            "11860 val_loss: 0.41624265909194946, train_loss: 0.34499701857566833\n",
            "11870 val_loss: 0.4155040979385376, train_loss: 0.34390968084335327\n",
            "11880 val_loss: 0.4151436388492584, train_loss: 0.3433706760406494\n",
            "11890 val_loss: 0.41473615169525146, train_loss: 0.34293967485427856\n",
            "11900 val_loss: 0.4143849015235901, train_loss: 0.3423115909099579\n",
            "11910 val_loss: 0.41450318694114685, train_loss: 0.34239575266838074\n",
            "11920 val_loss: 0.4139362573623657, train_loss: 0.34165456891059875\n",
            "11930 val_loss: 0.41367262601852417, train_loss: 0.3413371741771698\n",
            "11940 val_loss: 0.4134574234485626, train_loss: 0.34100255370140076\n",
            "11950 val_loss: 0.41319525241851807, train_loss: 0.3409128785133362\n",
            "11960 val_loss: 0.4127921462059021, train_loss: 0.34047022461891174\n",
            "11970 val_loss: 0.41245073080062866, train_loss: 0.3400251567363739\n",
            "11980 val_loss: 0.4119257628917694, train_loss: 0.33927005529403687\n",
            "11990 val_loss: 0.41157132387161255, train_loss: 0.33868107199668884\n",
            "12000 val_loss: 0.41124966740608215, train_loss: 0.33817681670188904\n",
            "12010 val_loss: 0.4109536111354828, train_loss: 0.3379099369049072\n",
            "12020 val_loss: 0.4106444716453552, train_loss: 0.3376476764678955\n",
            "12030 val_loss: 0.4102870523929596, train_loss: 0.3372560143470764\n",
            "12040 val_loss: 0.40967410802841187, train_loss: 0.33633583784103394\n",
            "12050 val_loss: 0.4095200002193451, train_loss: 0.3359723687171936\n",
            "12060 val_loss: 0.4093133509159088, train_loss: 0.3355282247066498\n",
            "12070 val_loss: 0.40897637605667114, train_loss: 0.3352520167827606\n",
            "12080 val_loss: 0.40886297821998596, train_loss: 0.334997296333313\n",
            "12090 val_loss: 0.40835312008857727, train_loss: 0.3347227871417999\n",
            "12100 val_loss: 0.4079836308956146, train_loss: 0.33411088585853577\n",
            "12110 val_loss: 0.4075528085231781, train_loss: 0.3337808847427368\n",
            "12120 val_loss: 0.406901091337204, train_loss: 0.33345943689346313\n",
            "12130 val_loss: 0.40670812129974365, train_loss: 0.33292582631111145\n",
            "12140 val_loss: 0.406302273273468, train_loss: 0.3325760066509247\n",
            "12150 val_loss: 0.4060593545436859, train_loss: 0.33255505561828613\n",
            "12160 val_loss: 0.40582501888275146, train_loss: 0.3322145640850067\n",
            "12170 val_loss: 0.40561217069625854, train_loss: 0.33173519372940063\n",
            "12180 val_loss: 0.4049939513206482, train_loss: 0.33102214336395264\n",
            "12190 val_loss: 0.40481752157211304, train_loss: 0.33112555742263794\n",
            "12200 val_loss: 0.4047907888889313, train_loss: 0.330695241689682\n",
            "12210 val_loss: 0.40460094809532166, train_loss: 0.33040088415145874\n",
            "12220 val_loss: 0.40417569875717163, train_loss: 0.33016276359558105\n",
            "12230 val_loss: 0.40394333004951477, train_loss: 0.3296753764152527\n",
            "12240 val_loss: 0.4034983813762665, train_loss: 0.3293226957321167\n",
            "12250 val_loss: 0.40319398045539856, train_loss: 0.3289813995361328\n",
            "12260 val_loss: 0.4028507471084595, train_loss: 0.328653484582901\n",
            "12270 val_loss: 0.40264692902565, train_loss: 0.32802635431289673\n",
            "12280 val_loss: 0.4021128714084625, train_loss: 0.3275775611400604\n",
            "12290 val_loss: 0.40165475010871887, train_loss: 0.3272307217121124\n",
            "12300 val_loss: 0.40138572454452515, train_loss: 0.32669901847839355\n",
            "12310 val_loss: 0.4012039601802826, train_loss: 0.32661351561546326\n",
            "12320 val_loss: 0.40110111236572266, train_loss: 0.3261636793613434\n",
            "12330 val_loss: 0.40080025792121887, train_loss: 0.3258657157421112\n",
            "12340 val_loss: 0.4005625247955322, train_loss: 0.3255960941314697\n",
            "12350 val_loss: 0.40020987391471863, train_loss: 0.32529816031455994\n",
            "12360 val_loss: 0.4001498520374298, train_loss: 0.3250309228897095\n",
            "12370 val_loss: 0.40003108978271484, train_loss: 0.3249264359474182\n",
            "12380 val_loss: 0.39996543526649475, train_loss: 0.3245503902435303\n",
            "12390 val_loss: 0.3996509909629822, train_loss: 0.32441815733909607\n",
            "12400 val_loss: 0.39901646971702576, train_loss: 0.32361701130867004\n",
            "12410 val_loss: 0.3988952040672302, train_loss: 0.3232962489128113\n",
            "12420 val_loss: 0.39848726987838745, train_loss: 0.3229616582393646\n",
            "12430 val_loss: 0.3982885479927063, train_loss: 0.3226741552352905\n",
            "12440 val_loss: 0.39788001775741577, train_loss: 0.3222077190876007\n",
            "12450 val_loss: 0.39761292934417725, train_loss: 0.3220183253288269\n",
            "12460 val_loss: 0.39747923612594604, train_loss: 0.3218659460544586\n",
            "12470 val_loss: 0.39729219675064087, train_loss: 0.321694016456604\n",
            "12480 val_loss: 0.3969481885433197, train_loss: 0.32127511501312256\n",
            "12490 val_loss: 0.39685577154159546, train_loss: 0.32113659381866455\n",
            "12500 val_loss: 0.3962409496307373, train_loss: 0.3205321729183197\n",
            "12510 val_loss: 0.39615732431411743, train_loss: 0.3201436996459961\n",
            "12520 val_loss: 0.3959968090057373, train_loss: 0.31979772448539734\n",
            "12530 val_loss: 0.3959766924381256, train_loss: 0.31963104009628296\n",
            "12540 val_loss: 0.395500123500824, train_loss: 0.3192030191421509\n",
            "12550 val_loss: 0.3953310549259186, train_loss: 0.3191019892692566\n",
            "12560 val_loss: 0.3950520157814026, train_loss: 0.3188191056251526\n",
            "12570 val_loss: 0.3950125277042389, train_loss: 0.31880736351013184\n",
            "12580 val_loss: 0.3947114944458008, train_loss: 0.3182356059551239\n",
            "12590 val_loss: 0.3944476544857025, train_loss: 0.31803369522094727\n",
            "12600 val_loss: 0.3943191170692444, train_loss: 0.31786859035491943\n",
            "12610 val_loss: 0.39384549856185913, train_loss: 0.31731653213500977\n",
            "12620 val_loss: 0.3933497965335846, train_loss: 0.3168562352657318\n",
            "12630 val_loss: 0.3930985629558563, train_loss: 0.3168156147003174\n",
            "12640 val_loss: 0.3927915096282959, train_loss: 0.3164650797843933\n",
            "12650 val_loss: 0.39237770438194275, train_loss: 0.31618446111679077\n",
            "12660 val_loss: 0.39237484335899353, train_loss: 0.3158862888813019\n",
            "12670 val_loss: 0.3919437527656555, train_loss: 0.3154046833515167\n",
            "12680 val_loss: 0.39190489053726196, train_loss: 0.31517359614372253\n",
            "12690 val_loss: 0.39159253239631653, train_loss: 0.31485339999198914\n",
            "12700 val_loss: 0.3914342522621155, train_loss: 0.31450310349464417\n",
            "12710 val_loss: 0.39100682735443115, train_loss: 0.3142017126083374\n",
            "12720 val_loss: 0.39093106985092163, train_loss: 0.3140745460987091\n",
            "12730 val_loss: 0.3908348083496094, train_loss: 0.31384044885635376\n",
            "12740 val_loss: 0.3906172811985016, train_loss: 0.31353527307510376\n",
            "12750 val_loss: 0.3900288939476013, train_loss: 0.3132016360759735\n",
            "12760 val_loss: 0.38985925912857056, train_loss: 0.31291255354881287\n",
            "12770 val_loss: 0.3893744945526123, train_loss: 0.3124375641345978\n",
            "12780 val_loss: 0.38897696137428284, train_loss: 0.3120964765548706\n",
            "12790 val_loss: 0.38881751894950867, train_loss: 0.31185805797576904\n",
            "12800 val_loss: 0.388871967792511, train_loss: 0.3119035065174103\n",
            "12810 val_loss: 0.38853028416633606, train_loss: 0.31150802969932556\n",
            "12820 val_loss: 0.38970255851745605, train_loss: 0.312418133020401\n",
            "12830 val_loss: 0.3881416320800781, train_loss: 0.3112243413925171\n",
            "12840 val_loss: 0.3880818784236908, train_loss: 0.3110191524028778\n",
            "12850 val_loss: 0.3876425325870514, train_loss: 0.3107297718524933\n",
            "12860 val_loss: 0.3875124752521515, train_loss: 0.310558944940567\n",
            "12870 val_loss: 0.38736405968666077, train_loss: 0.31009429693222046\n",
            "12880 val_loss: 0.38696256279945374, train_loss: 0.3095085024833679\n",
            "12890 val_loss: 0.3867930471897125, train_loss: 0.3093244135379791\n",
            "12900 val_loss: 0.3863714337348938, train_loss: 0.3089064061641693\n",
            "12910 val_loss: 0.38624095916748047, train_loss: 0.30872198939323425\n",
            "12920 val_loss: 0.38593927025794983, train_loss: 0.3082638680934906\n",
            "12930 val_loss: 0.38572031259536743, train_loss: 0.3078325688838959\n",
            "12940 val_loss: 0.3855840563774109, train_loss: 0.3077133893966675\n",
            "12950 val_loss: 0.38539254665374756, train_loss: 0.3074597418308258\n",
            "12960 val_loss: 0.385259211063385, train_loss: 0.3071967661380768\n",
            "12970 val_loss: 0.3845997750759125, train_loss: 0.306832879781723\n",
            "12980 val_loss: 0.38471588492393494, train_loss: 0.3063059449195862\n",
            "12990 val_loss: 0.3844330608844757, train_loss: 0.3060566782951355\n",
            "13000 val_loss: 0.3839856684207916, train_loss: 0.30575031042099\n",
            "13010 val_loss: 0.3841704726219177, train_loss: 0.3061854839324951\n",
            "13020 val_loss: 0.3835142254829407, train_loss: 0.30544155836105347\n",
            "13030 val_loss: 0.3835074007511139, train_loss: 0.30542969703674316\n",
            "13040 val_loss: 0.383107990026474, train_loss: 0.3049323260784149\n",
            "13050 val_loss: 0.38308483362197876, train_loss: 0.3051658272743225\n",
            "13060 val_loss: 0.38281404972076416, train_loss: 0.30472928285598755\n",
            "13070 val_loss: 0.3825690746307373, train_loss: 0.30434125661849976\n",
            "13080 val_loss: 0.38242703676223755, train_loss: 0.3039616644382477\n",
            "13090 val_loss: 0.3822283148765564, train_loss: 0.3038782775402069\n",
            "13100 val_loss: 0.3818233609199524, train_loss: 0.3034239411354065\n",
            "13110 val_loss: 0.3818861246109009, train_loss: 0.3032839000225067\n",
            "13120 val_loss: 0.3816051483154297, train_loss: 0.30315127968788147\n",
            "13130 val_loss: 0.38133594393730164, train_loss: 0.3026128113269806\n",
            "13140 val_loss: 0.38202258944511414, train_loss: 0.3030458092689514\n",
            "13150 val_loss: 0.38089147210121155, train_loss: 0.30217403173446655\n",
            "13160 val_loss: 0.3805204927921295, train_loss: 0.3018037676811218\n",
            "13170 val_loss: 0.380673348903656, train_loss: 0.301673948764801\n",
            "13180 val_loss: 0.38043972849845886, train_loss: 0.3017786741256714\n",
            "13190 val_loss: 0.38035330176353455, train_loss: 0.301369309425354\n",
            "13200 val_loss: 0.38044771552085876, train_loss: 0.30135253071784973\n",
            "13210 val_loss: 0.37944668531417847, train_loss: 0.30094799399375916\n",
            "13220 val_loss: 0.3796636164188385, train_loss: 0.3009207248687744\n",
            "13230 val_loss: 0.379195898771286, train_loss: 0.3004719018936157\n",
            "13240 val_loss: 0.3789328932762146, train_loss: 0.3001329004764557\n",
            "13250 val_loss: 0.37873733043670654, train_loss: 0.3000757694244385\n",
            "13260 val_loss: 0.3784814476966858, train_loss: 0.29985812306404114\n",
            "13270 val_loss: 0.3780288100242615, train_loss: 0.2994648218154907\n",
            "13280 val_loss: 0.3779946565628052, train_loss: 0.29918164014816284\n",
            "13290 val_loss: 0.3778962790966034, train_loss: 0.29917147755622864\n",
            "13300 val_loss: 0.37788987159729004, train_loss: 0.2992383539676666\n",
            "13310 val_loss: 0.3777107000350952, train_loss: 0.2989518642425537\n",
            "13320 val_loss: 0.37768808007240295, train_loss: 0.2987203896045685\n",
            "13330 val_loss: 0.3780076801776886, train_loss: 0.29885628819465637\n",
            "13340 val_loss: 0.37690168619155884, train_loss: 0.29799777269363403\n",
            "13350 val_loss: 0.3767479360103607, train_loss: 0.2977330982685089\n",
            "13360 val_loss: 0.3766937553882599, train_loss: 0.29767587780952454\n",
            "13370 val_loss: 0.37615031003952026, train_loss: 0.2969426214694977\n",
            "13380 val_loss: 0.3758200705051422, train_loss: 0.29672548174858093\n",
            "13390 val_loss: 0.37598419189453125, train_loss: 0.296588659286499\n",
            "13400 val_loss: 0.37546804547309875, train_loss: 0.296284019947052\n",
            "13410 val_loss: 0.3751800060272217, train_loss: 0.29609960317611694\n",
            "13420 val_loss: 0.37526053190231323, train_loss: 0.2959330379962921\n",
            "13430 val_loss: 0.37493687868118286, train_loss: 0.2954498827457428\n",
            "13440 val_loss: 0.374808132648468, train_loss: 0.29544737935066223\n",
            "13450 val_loss: 0.3745731711387634, train_loss: 0.295230895280838\n",
            "13460 val_loss: 0.37438011169433594, train_loss: 0.29487061500549316\n",
            "13470 val_loss: 0.3743405044078827, train_loss: 0.29477205872535706\n",
            "13480 val_loss: 0.37423938512802124, train_loss: 0.2944856882095337\n",
            "13490 val_loss: 0.37397998571395874, train_loss: 0.29441389441490173\n",
            "13500 val_loss: 0.3735884726047516, train_loss: 0.29394179582595825\n",
            "13510 val_loss: 0.37345069646835327, train_loss: 0.29399803280830383\n",
            "13520 val_loss: 0.3733687102794647, train_loss: 0.2938149869441986\n",
            "13530 val_loss: 0.3733668923377991, train_loss: 0.29359954595565796\n",
            "13540 val_loss: 0.3729124963283539, train_loss: 0.29311972856521606\n",
            "13550 val_loss: 0.37261635065078735, train_loss: 0.29273685812950134\n",
            "13560 val_loss: 0.3724302053451538, train_loss: 0.29244956374168396\n",
            "13570 val_loss: 0.37200531363487244, train_loss: 0.29215025901794434\n",
            "13580 val_loss: 0.37172454595565796, train_loss: 0.2919449210166931\n",
            "13590 val_loss: 0.3716031312942505, train_loss: 0.2916576862335205\n",
            "13600 val_loss: 0.37178704142570496, train_loss: 0.2918021082878113\n",
            "13610 val_loss: 0.3717957139015198, train_loss: 0.2917066812515259\n",
            "13620 val_loss: 0.37203946709632874, train_loss: 0.29191088676452637\n",
            "13630 val_loss: 0.3707101345062256, train_loss: 0.29112038016319275\n",
            "13640 val_loss: 0.3705366551876068, train_loss: 0.29062896966934204\n",
            "13650 val_loss: 0.3703421354293823, train_loss: 0.29037952423095703\n",
            "13660 val_loss: 0.37047094106674194, train_loss: 0.29038751125335693\n",
            "13670 val_loss: 0.3702508807182312, train_loss: 0.2902257442474365\n",
            "13680 val_loss: 0.3698873519897461, train_loss: 0.29003942012786865\n",
            "13690 val_loss: 0.36945241689682007, train_loss: 0.28991568088531494\n",
            "13700 val_loss: 0.36941754817962646, train_loss: 0.2898048758506775\n",
            "13710 val_loss: 0.3691309690475464, train_loss: 0.2895662784576416\n",
            "13720 val_loss: 0.36889684200286865, train_loss: 0.28905802965164185\n",
            "13730 val_loss: 0.36876678466796875, train_loss: 0.28911951184272766\n",
            "13740 val_loss: 0.3687613904476166, train_loss: 0.2888215482234955\n",
            "13750 val_loss: 0.36832085251808167, train_loss: 0.2883964478969574\n",
            "13760 val_loss: 0.3682612478733063, train_loss: 0.2881179451942444\n",
            "13770 val_loss: 0.36858782172203064, train_loss: 0.2882560193538666\n",
            "13780 val_loss: 0.3682917058467865, train_loss: 0.2880415916442871\n",
            "13790 val_loss: 0.36799126863479614, train_loss: 0.2878301441669464\n",
            "13800 val_loss: 0.3680013418197632, train_loss: 0.2875540554523468\n",
            "13810 val_loss: 0.3674817681312561, train_loss: 0.28716573119163513\n",
            "13820 val_loss: 0.3672512471675873, train_loss: 0.28671935200691223\n",
            "13830 val_loss: 0.3672613799571991, train_loss: 0.28670430183410645\n",
            "13840 val_loss: 0.3668053150177002, train_loss: 0.28627270460128784\n",
            "13850 val_loss: 0.3664815127849579, train_loss: 0.28575652837753296\n",
            "13860 val_loss: 0.3661172091960907, train_loss: 0.28545430302619934\n",
            "13870 val_loss: 0.36595168709754944, train_loss: 0.285126268863678\n",
            "13880 val_loss: 0.3656345009803772, train_loss: 0.2849496603012085\n",
            "13890 val_loss: 0.36566901206970215, train_loss: 0.2851034998893738\n",
            "13900 val_loss: 0.36526912450790405, train_loss: 0.28472331166267395\n",
            "13910 val_loss: 0.3653368651866913, train_loss: 0.2844569683074951\n",
            "13920 val_loss: 0.36468565464019775, train_loss: 0.2839924097061157\n",
            "13930 val_loss: 0.3648030757904053, train_loss: 0.28386548161506653\n",
            "13940 val_loss: 0.3646768629550934, train_loss: 0.28356876969337463\n",
            "13950 val_loss: 0.36440038681030273, train_loss: 0.2834725081920624\n",
            "13960 val_loss: 0.36425328254699707, train_loss: 0.283484548330307\n",
            "13970 val_loss: 0.36395999789237976, train_loss: 0.283158540725708\n",
            "13980 val_loss: 0.3639141619205475, train_loss: 0.2830457389354706\n",
            "13990 val_loss: 0.36372846364974976, train_loss: 0.2827988564968109\n",
            "14000 val_loss: 0.36358320713043213, train_loss: 0.2826900780200958\n",
            "14010 val_loss: 0.36333030462265015, train_loss: 0.28245529532432556\n",
            "14020 val_loss: 0.3631754517555237, train_loss: 0.2821161448955536\n",
            "14030 val_loss: 0.3629092574119568, train_loss: 0.28198951482772827\n",
            "14040 val_loss: 0.3630588948726654, train_loss: 0.2819141745567322\n",
            "14050 val_loss: 0.36286473274230957, train_loss: 0.2816806435585022\n",
            "14060 val_loss: 0.36276936531066895, train_loss: 0.28148385882377625\n",
            "14070 val_loss: 0.3624502718448639, train_loss: 0.2812226712703705\n",
            "14080 val_loss: 0.36245104670524597, train_loss: 0.28128916025161743\n",
            "14090 val_loss: 0.3622523248195648, train_loss: 0.2810015380382538\n",
            "14100 val_loss: 0.36213356256484985, train_loss: 0.2806916832923889\n",
            "14110 val_loss: 0.36168304085731506, train_loss: 0.28032851219177246\n",
            "14120 val_loss: 0.3617084324359894, train_loss: 0.28021568059921265\n",
            "14130 val_loss: 0.3618081212043762, train_loss: 0.2801372706890106\n",
            "14140 val_loss: 0.3615790009498596, train_loss: 0.28001734614372253\n",
            "14150 val_loss: 0.36130017042160034, train_loss: 0.2796553075313568\n",
            "14160 val_loss: 0.3609082102775574, train_loss: 0.27931010723114014\n",
            "14170 val_loss: 0.36092689633369446, train_loss: 0.2792629301548004\n",
            "14180 val_loss: 0.3608429431915283, train_loss: 0.2789231836795807\n",
            "14190 val_loss: 0.36046990752220154, train_loss: 0.2786542773246765\n",
            "14200 val_loss: 0.3605673313140869, train_loss: 0.27861487865448\n",
            "14210 val_loss: 0.3603208363056183, train_loss: 0.278301477432251\n",
            "14220 val_loss: 0.360098659992218, train_loss: 0.27812156081199646\n",
            "14230 val_loss: 0.35983651876449585, train_loss: 0.2778461277484894\n",
            "14240 val_loss: 0.35968470573425293, train_loss: 0.2777886688709259\n",
            "14250 val_loss: 0.359512597322464, train_loss: 0.2773820757865906\n",
            "14260 val_loss: 0.35970088839530945, train_loss: 0.2773652970790863\n",
            "14270 val_loss: 0.359214723110199, train_loss: 0.2769019901752472\n",
            "14280 val_loss: 0.3591952919960022, train_loss: 0.2767679691314697\n",
            "14290 val_loss: 0.3588966727256775, train_loss: 0.2763594686985016\n",
            "14300 val_loss: 0.35924583673477173, train_loss: 0.2764970362186432\n",
            "14310 val_loss: 0.35916799306869507, train_loss: 0.276429682970047\n",
            "14320 val_loss: 0.35866808891296387, train_loss: 0.2760562598705292\n",
            "14330 val_loss: 0.3585429787635803, train_loss: 0.2758178412914276\n",
            "14340 val_loss: 0.3584035634994507, train_loss: 0.27556923031806946\n",
            "14350 val_loss: 0.3585294187068939, train_loss: 0.275500625371933\n",
            "14360 val_loss: 0.3585285544395447, train_loss: 0.2756476402282715\n",
            "14370 val_loss: 0.3581303656101227, train_loss: 0.2750694453716278\n",
            "14380 val_loss: 0.3578656315803528, train_loss: 0.2747347056865692\n",
            "14390 val_loss: 0.35742974281311035, train_loss: 0.2744499444961548\n",
            "14400 val_loss: 0.35729408264160156, train_loss: 0.27428174018859863\n",
            "14410 val_loss: 0.3572816550731659, train_loss: 0.2741489112377167\n",
            "14420 val_loss: 0.3572853207588196, train_loss: 0.273921400308609\n",
            "14430 val_loss: 0.3571280837059021, train_loss: 0.27383852005004883\n",
            "14440 val_loss: 0.35698360204696655, train_loss: 0.27363333106040955\n",
            "14450 val_loss: 0.3568904995918274, train_loss: 0.27338019013404846\n",
            "14460 val_loss: 0.35712429881095886, train_loss: 0.27355825901031494\n",
            "14470 val_loss: 0.35738691687583923, train_loss: 0.27364790439605713\n",
            "14480 val_loss: 0.35656166076660156, train_loss: 0.2731878459453583\n",
            "14490 val_loss: 0.3564327359199524, train_loss: 0.2729758322238922\n",
            "14500 val_loss: 0.35600122809410095, train_loss: 0.27262231707572937\n",
            "14510 val_loss: 0.3563026785850525, train_loss: 0.2728336751461029\n",
            "14520 val_loss: 0.35608676075935364, train_loss: 0.27255702018737793\n",
            "14530 val_loss: 0.3561132848262787, train_loss: 0.2725682556629181\n",
            "14540 val_loss: 0.3561176061630249, train_loss: 0.27276453375816345\n",
            "14550 val_loss: 0.35583004355430603, train_loss: 0.27202239632606506\n",
            "14560 val_loss: 0.3555358350276947, train_loss: 0.2715831398963928\n",
            "14570 val_loss: 0.3554156720638275, train_loss: 0.2716272175312042\n",
            "14580 val_loss: 0.3551253080368042, train_loss: 0.2713521718978882\n",
            "14590 val_loss: 0.3552325367927551, train_loss: 0.27149805426597595\n",
            "14600 val_loss: 0.3557591438293457, train_loss: 0.27176567912101746\n",
            "14610 val_loss: 0.3550602197647095, train_loss: 0.27126067876815796\n",
            "14620 val_loss: 0.3554422855377197, train_loss: 0.27125445008277893\n",
            "14630 val_loss: 0.3545668125152588, train_loss: 0.2705477774143219\n",
            "14640 val_loss: 0.3547864556312561, train_loss: 0.2704852521419525\n",
            "14650 val_loss: 0.3542574942111969, train_loss: 0.2697671055793762\n",
            "14660 val_loss: 0.35413840413093567, train_loss: 0.26952123641967773\n",
            "14670 val_loss: 0.3540728986263275, train_loss: 0.26934319734573364\n",
            "14680 val_loss: 0.35395151376724243, train_loss: 0.26958897709846497\n",
            "14690 val_loss: 0.3537967801094055, train_loss: 0.26915448904037476\n",
            "14700 val_loss: 0.3533869683742523, train_loss: 0.26873669028282166\n",
            "14710 val_loss: 0.35321298241615295, train_loss: 0.26852843165397644\n",
            "14720 val_loss: 0.3531375527381897, train_loss: 0.26837024092674255\n",
            "14730 val_loss: 0.35304704308509827, train_loss: 0.268386572599411\n",
            "14740 val_loss: 0.3535872995853424, train_loss: 0.2684313654899597\n",
            "14750 val_loss: 0.352600634098053, train_loss: 0.2677704989910126\n",
            "14760 val_loss: 0.3523654341697693, train_loss: 0.2675597667694092\n",
            "14770 val_loss: 0.3524470925331116, train_loss: 0.26732710003852844\n",
            "14780 val_loss: 0.3520730435848236, train_loss: 0.26690754294395447\n",
            "14790 val_loss: 0.35194021463394165, train_loss: 0.266823410987854\n",
            "14800 val_loss: 0.3519468903541565, train_loss: 0.26685190200805664\n",
            "14810 val_loss: 0.3520941734313965, train_loss: 0.26675117015838623\n",
            "14820 val_loss: 0.352013498544693, train_loss: 0.2665826380252838\n",
            "14830 val_loss: 0.35248225927352905, train_loss: 0.2668308615684509\n",
            "14840 val_loss: 0.35239365696907043, train_loss: 0.26652565598487854\n",
            "14850 val_loss: 0.3521880507469177, train_loss: 0.26592767238616943\n",
            "14860 val_loss: 0.3524654805660248, train_loss: 0.2656831443309784\n",
            "14870 val_loss: 0.35211122035980225, train_loss: 0.26572564244270325\n",
            "14880 val_loss: 0.3517894148826599, train_loss: 0.2652890086174011\n",
            "14890 val_loss: 0.35183006525039673, train_loss: 0.264914870262146\n",
            "14900 val_loss: 0.3513706624507904, train_loss: 0.2642081081867218\n",
            "14910 val_loss: 0.3514152765274048, train_loss: 0.2638460397720337\n",
            "14920 val_loss: 0.3507530689239502, train_loss: 0.2639116048812866\n",
            "14930 val_loss: 0.35086193680763245, train_loss: 0.2638285756111145\n",
            "14940 val_loss: 0.35047680139541626, train_loss: 0.2635628283023834\n",
            "14950 val_loss: 0.3507721424102783, train_loss: 0.2630384564399719\n",
            "14960 val_loss: 0.3504244387149811, train_loss: 0.2628744840621948\n",
            "14970 val_loss: 0.3504633605480194, train_loss: 0.262470006942749\n",
            "14980 val_loss: 0.3500562012195587, train_loss: 0.2621808350086212\n",
            "14990 val_loss: 0.3500042259693146, train_loss: 0.26179584860801697\n",
            "15000 val_loss: 0.35025468468666077, train_loss: 0.2616565227508545\n",
            "15010 val_loss: 0.3498741686344147, train_loss: 0.26140913367271423\n",
            "15020 val_loss: 0.3492874205112457, train_loss: 0.2614102363586426\n",
            "15030 val_loss: 0.3495416045188904, train_loss: 0.2609767019748688\n",
            "15040 val_loss: 0.3492051661014557, train_loss: 0.26090314984321594\n",
            "15050 val_loss: 0.34942954778671265, train_loss: 0.2605408728122711\n",
            "15060 val_loss: 0.3490165174007416, train_loss: 0.2607472538948059\n",
            "15070 val_loss: 0.3487488925457001, train_loss: 0.2601797878742218\n",
            "15080 val_loss: 0.3489963710308075, train_loss: 0.2597179710865021\n",
            "15090 val_loss: 0.34859371185302734, train_loss: 0.2596825957298279\n",
            "15100 val_loss: 0.3495124578475952, train_loss: 0.25928232073783875\n",
            "15110 val_loss: 0.3485606610774994, train_loss: 0.25886550545692444\n",
            "15120 val_loss: 0.348149836063385, train_loss: 0.25860312581062317\n",
            "15130 val_loss: 0.34841322898864746, train_loss: 0.25823044776916504\n",
            "15140 val_loss: 0.34768256545066833, train_loss: 0.257915735244751\n",
            "15150 val_loss: 0.34722229838371277, train_loss: 0.2577102482318878\n",
            "15160 val_loss: 0.34686362743377686, train_loss: 0.2575817108154297\n",
            "15170 val_loss: 0.346916526556015, train_loss: 0.2575368583202362\n",
            "15180 val_loss: 0.3485192060470581, train_loss: 0.2573234438896179\n",
            "15190 val_loss: 0.34693393111228943, train_loss: 0.2565706968307495\n",
            "15200 val_loss: 0.34756723046302795, train_loss: 0.2565166652202606\n",
            "15210 val_loss: 0.3470941483974457, train_loss: 0.25634992122650146\n",
            "15220 val_loss: 0.3467184603214264, train_loss: 0.25594577193260193\n",
            "15230 val_loss: 0.34643200039863586, train_loss: 0.2555519640445709\n",
            "15240 val_loss: 0.3459007143974304, train_loss: 0.25531506538391113\n",
            "15250 val_loss: 0.34638601541519165, train_loss: 0.25516852736473083\n",
            "15260 val_loss: 0.34591060876846313, train_loss: 0.25481289625167847\n",
            "15270 val_loss: 0.3456988036632538, train_loss: 0.25465649366378784\n",
            "15280 val_loss: 0.34587961435317993, train_loss: 0.2546692490577698\n",
            "15290 val_loss: 0.34641730785369873, train_loss: 0.2544555962085724\n",
            "15300 val_loss: 0.3465266227722168, train_loss: 0.25436294078826904\n",
            "15310 val_loss: 0.34480270743370056, train_loss: 0.2536543607711792\n",
            "15320 val_loss: 0.3445935547351837, train_loss: 0.25351089239120483\n",
            "15330 val_loss: 0.344940721988678, train_loss: 0.2529619634151459\n",
            "15340 val_loss: 0.3441484868526459, train_loss: 0.25286760926246643\n",
            "15350 val_loss: 0.3439686894416809, train_loss: 0.25236570835113525\n",
            "15360 val_loss: 0.3442934453487396, train_loss: 0.25218695402145386\n",
            "15370 val_loss: 0.34490305185317993, train_loss: 0.2520054280757904\n",
            "15380 val_loss: 0.34486353397369385, train_loss: 0.25172293186187744\n",
            "15390 val_loss: 0.3452708125114441, train_loss: 0.2516404092311859\n",
            "15400 val_loss: 0.3450368344783783, train_loss: 0.2513939142227173\n",
            "15410 val_loss: 0.3451415002346039, train_loss: 0.2512974441051483\n",
            "15420 val_loss: 0.3449941873550415, train_loss: 0.25110214948654175\n",
            "15430 val_loss: 0.34417787194252014, train_loss: 0.25065526366233826\n",
            "15440 val_loss: 0.34365004301071167, train_loss: 0.25022831559181213\n",
            "15450 val_loss: 0.3439846634864807, train_loss: 0.2499387562274933\n",
            "15460 val_loss: 0.34295547008514404, train_loss: 0.2498670518398285\n",
            "15470 val_loss: 0.3433302044868469, train_loss: 0.24938108026981354\n",
            "15480 val_loss: 0.343432754278183, train_loss: 0.24952973425388336\n",
            "15490 val_loss: 0.3426218032836914, train_loss: 0.24966508150100708\n",
            "15500 val_loss: 0.3450412154197693, train_loss: 0.24967163801193237\n",
            "15510 val_loss: 0.3428999185562134, train_loss: 0.24870789051055908\n",
            "15520 val_loss: 0.342651903629303, train_loss: 0.24861398339271545\n",
            "15530 val_loss: 0.3434518277645111, train_loss: 0.24831987917423248\n",
            "15540 val_loss: 0.3427562713623047, train_loss: 0.24800823628902435\n",
            "15550 val_loss: 0.3428729772567749, train_loss: 0.2477497160434723\n",
            "15560 val_loss: 0.34106481075286865, train_loss: 0.24729914963245392\n",
            "15570 val_loss: 0.3429771661758423, train_loss: 0.24751712381839752\n",
            "15580 val_loss: 0.3407975435256958, train_loss: 0.24778732657432556\n",
            "15590 val_loss: 0.3406834602355957, train_loss: 0.24678337574005127\n",
            "15600 val_loss: 0.3414868414402008, train_loss: 0.24653199315071106\n",
            "15610 val_loss: 0.34329143166542053, train_loss: 0.2470148801803589\n",
            "15620 val_loss: 0.3403100371360779, train_loss: 0.24674279987812042\n",
            "15630 val_loss: 0.34216856956481934, train_loss: 0.24611440300941467\n",
            "15640 val_loss: 0.34161463379859924, train_loss: 0.2456691563129425\n",
            "15650 val_loss: 0.3424883186817169, train_loss: 0.2454036921262741\n",
            "15660 val_loss: 0.3405012786388397, train_loss: 0.24513296782970428\n",
            "15670 val_loss: 0.3404170870780945, train_loss: 0.2447691559791565\n",
            "15680 val_loss: 0.3415336608886719, train_loss: 0.2443479299545288\n",
            "15690 val_loss: 0.34138578176498413, train_loss: 0.24425199627876282\n",
            "15700 val_loss: 0.34096863865852356, train_loss: 0.24394948780536652\n",
            "15710 val_loss: 0.3406887650489807, train_loss: 0.24348239600658417\n",
            "15720 val_loss: 0.3430570065975189, train_loss: 0.24409139156341553\n",
            "15730 val_loss: 0.34180372953414917, train_loss: 0.24355430901050568\n",
            "15740 val_loss: 0.34047189354896545, train_loss: 0.24310055375099182\n",
            "15750 val_loss: 0.3418061137199402, train_loss: 0.24311184883117676\n",
            "15760 val_loss: 0.33977261185646057, train_loss: 0.24270930886268616\n",
            "15770 val_loss: 0.34178096055984497, train_loss: 0.24241401255130768\n",
            "15780 val_loss: 0.34101176261901855, train_loss: 0.24206990003585815\n",
            "15790 val_loss: 0.3395242393016815, train_loss: 0.2415429651737213\n",
            "15800 val_loss: 0.3396938741207123, train_loss: 0.2413182407617569\n",
            "15810 val_loss: 0.3382929265499115, train_loss: 0.24098022282123566\n",
            "15820 val_loss: 0.3383443355560303, train_loss: 0.24055741727352142\n",
            "15830 val_loss: 0.3381522595882416, train_loss: 0.24019566178321838\n",
            "15840 val_loss: 0.33810800313949585, train_loss: 0.2404845654964447\n",
            "15850 val_loss: 0.3395145833492279, train_loss: 0.2400824874639511\n",
            "15860 val_loss: 0.3393822908401489, train_loss: 0.23987796902656555\n",
            "15870 val_loss: 0.3390357196331024, train_loss: 0.23946736752986908\n",
            "15880 val_loss: 0.3385358154773712, train_loss: 0.2388371080160141\n",
            "15890 val_loss: 0.3398990035057068, train_loss: 0.23925378918647766\n",
            "15900 val_loss: 0.3395012319087982, train_loss: 0.23904113471508026\n",
            "15910 val_loss: 0.3392631411552429, train_loss: 0.23863837122917175\n",
            "15920 val_loss: 0.340555340051651, train_loss: 0.2384214848279953\n",
            "15930 val_loss: 0.3381066918373108, train_loss: 0.23779499530792236\n",
            "15940 val_loss: 0.3403962254524231, train_loss: 0.23816922307014465\n",
            "15950 val_loss: 0.3403778672218323, train_loss: 0.23797999322414398\n",
            "15960 val_loss: 0.33896738290786743, train_loss: 0.23745958507061005\n",
            "15970 val_loss: 0.33758655190467834, train_loss: 0.23690509796142578\n",
            "15980 val_loss: 0.33635133504867554, train_loss: 0.23666706681251526\n",
            "15990 val_loss: 0.3393127918243408, train_loss: 0.23670928180217743\n",
            "16000 val_loss: 0.3391035199165344, train_loss: 0.23622086644172668\n",
            "16010 val_loss: 0.3403017222881317, train_loss: 0.23617517948150635\n",
            "16020 val_loss: 0.33769720792770386, train_loss: 0.23525559902191162\n",
            "16030 val_loss: 0.33884894847869873, train_loss: 0.2351975291967392\n",
            "16040 val_loss: 0.34030881524086, train_loss: 0.235203817486763\n",
            "16050 val_loss: 0.33871299028396606, train_loss: 0.23404742777347565\n",
            "16060 val_loss: 0.3376289904117584, train_loss: 0.23349516093730927\n",
            "16070 val_loss: 0.3356010317802429, train_loss: 0.23339298367500305\n",
            "16080 val_loss: 0.3376905918121338, train_loss: 0.23281578719615936\n",
            "16090 val_loss: 0.33957788348197937, train_loss: 0.23310725390911102\n",
            "16100 val_loss: 0.33829784393310547, train_loss: 0.23254820704460144\n",
            "16110 val_loss: 0.3385382890701294, train_loss: 0.23204544186592102\n",
            "16120 val_loss: 0.3372509777545929, train_loss: 0.23141887784004211\n",
            "16130 val_loss: 0.33678945899009705, train_loss: 0.23134447634220123\n",
            "16140 val_loss: 0.33644595742225647, train_loss: 0.23095780611038208\n",
            "16150 val_loss: 0.3380240797996521, train_loss: 0.23134151101112366\n",
            "16160 val_loss: 0.3382973372936249, train_loss: 0.2308763712644577\n",
            "16170 val_loss: 0.3392292559146881, train_loss: 0.23073960840702057\n",
            "16180 val_loss: 0.3352356553077698, train_loss: 0.23025216162204742\n",
            "16190 val_loss: 0.33748677372932434, train_loss: 0.22968889772891998\n",
            "16200 val_loss: 0.33709031343460083, train_loss: 0.22941970825195312\n",
            "16210 val_loss: 0.3354448974132538, train_loss: 0.22911368310451508\n",
            "16220 val_loss: 0.33902621269226074, train_loss: 0.22913359105587006\n",
            "16230 val_loss: 0.33798229694366455, train_loss: 0.2285420000553131\n",
            "16240 val_loss: 0.3367651104927063, train_loss: 0.2287474274635315\n",
            "16250 val_loss: 0.3366619646549225, train_loss: 0.22837689518928528\n",
            "16260 val_loss: 0.3348143994808197, train_loss: 0.22841127216815948\n",
            "16270 val_loss: 0.33567261695861816, train_loss: 0.22767160832881927\n",
            "16280 val_loss: 0.3386231064796448, train_loss: 0.22772197425365448\n",
            "16290 val_loss: 0.33475393056869507, train_loss: 0.2268480807542801\n",
            "16300 val_loss: 0.33538728952407837, train_loss: 0.22674119472503662\n",
            "16310 val_loss: 0.33550965785980225, train_loss: 0.22642938792705536\n",
            "16320 val_loss: 0.33772537112236023, train_loss: 0.2263496220111847\n",
            "16330 val_loss: 0.3368435502052307, train_loss: 0.22581462562084198\n",
            "16340 val_loss: 0.33587169647216797, train_loss: 0.2254020720720291\n",
            "16350 val_loss: 0.3366909623146057, train_loss: 0.2251567840576172\n",
            "16360 val_loss: 0.33540523052215576, train_loss: 0.22494705021381378\n",
            "16370 val_loss: 0.3390839099884033, train_loss: 0.22527897357940674\n",
            "16380 val_loss: 0.33379560708999634, train_loss: 0.22425618767738342\n",
            "16390 val_loss: 0.33575814962387085, train_loss: 0.22398661077022552\n",
            "16400 val_loss: 0.33315309882164, train_loss: 0.22387224435806274\n",
            "16410 val_loss: 0.33130815625190735, train_loss: 0.22396759688854218\n",
            "16420 val_loss: 0.33761507272720337, train_loss: 0.22395716607570648\n",
            "16430 val_loss: 0.33408474922180176, train_loss: 0.22314110398292542\n",
            "16440 val_loss: 0.3358900845050812, train_loss: 0.2231084704399109\n",
            "16450 val_loss: 0.3366663157939911, train_loss: 0.2228272557258606\n",
            "16460 val_loss: 0.33338406682014465, train_loss: 0.22189240157604218\n",
            "16470 val_loss: 0.3323300778865814, train_loss: 0.22183144092559814\n",
            "16480 val_loss: 0.3325604498386383, train_loss: 0.22128981351852417\n",
            "16490 val_loss: 0.33224520087242126, train_loss: 0.2210627645254135\n",
            "16500 val_loss: 0.3347541391849518, train_loss: 0.22114506363868713\n",
            "16510 val_loss: 0.3360283076763153, train_loss: 0.2213522046804428\n",
            "16520 val_loss: 0.33899131417274475, train_loss: 0.22194325923919678\n",
            "16530 val_loss: 0.3346828818321228, train_loss: 0.22079098224639893\n",
            "16540 val_loss: 0.3321212828159332, train_loss: 0.21991553902626038\n",
            "16550 val_loss: 0.3366686999797821, train_loss: 0.2200111299753189\n",
            "16560 val_loss: 0.3335553705692291, train_loss: 0.21969261765480042\n",
            "16570 val_loss: 0.33087944984436035, train_loss: 0.21938782930374146\n",
            "16580 val_loss: 0.33168715238571167, train_loss: 0.21910084784030914\n",
            "16590 val_loss: 0.3321560323238373, train_loss: 0.2186141312122345\n",
            "16600 val_loss: 0.33478477597236633, train_loss: 0.2186872512102127\n",
            "16610 val_loss: 0.33092960715293884, train_loss: 0.21863828599452972\n",
            "16620 val_loss: 0.33045700192451477, train_loss: 0.21808743476867676\n",
            "16630 val_loss: 0.3329918384552002, train_loss: 0.21805094182491302\n",
            "16640 val_loss: 0.32911333441734314, train_loss: 0.21803291141986847\n",
            "16650 val_loss: 0.3325658142566681, train_loss: 0.21721096336841583\n",
            "16660 val_loss: 0.34016260504722595, train_loss: 0.21919484436511993\n",
            "16670 val_loss: 0.33533939719200134, train_loss: 0.21727505326271057\n",
            "16680 val_loss: 0.33378809690475464, train_loss: 0.21692612767219543\n",
            "16690 val_loss: 0.33175164461135864, train_loss: 0.2164974957704544\n",
            "16700 val_loss: 0.3299618363380432, train_loss: 0.21596679091453552\n",
            "16710 val_loss: 0.3304517865180969, train_loss: 0.21604347229003906\n",
            "16720 val_loss: 0.3297170400619507, train_loss: 0.21559211611747742\n",
            "16730 val_loss: 0.32913702726364136, train_loss: 0.21549543738365173\n",
            "16740 val_loss: 0.33171334862709045, train_loss: 0.21531395614147186\n",
            "16750 val_loss: 0.33328959345817566, train_loss: 0.2152763456106186\n",
            "16760 val_loss: 0.32913053035736084, train_loss: 0.21533800661563873\n",
            "16770 val_loss: 0.3318435549736023, train_loss: 0.21506056189537048\n",
            "16780 val_loss: 0.33069148659706116, train_loss: 0.21437931060791016\n",
            "16790 val_loss: 0.3332902789115906, train_loss: 0.21466884016990662\n",
            "16800 val_loss: 0.33118700981140137, train_loss: 0.21381129324436188\n",
            "16810 val_loss: 0.32951104640960693, train_loss: 0.2138558030128479\n",
            "16820 val_loss: 0.3285362124443054, train_loss: 0.2135227769613266\n",
            "16830 val_loss: 0.333111435174942, train_loss: 0.2137143462896347\n",
            "16840 val_loss: 0.325662761926651, train_loss: 0.21349027752876282\n",
            "16850 val_loss: 0.33261650800704956, train_loss: 0.21373692154884338\n",
            "16860 val_loss: 0.3357490003108978, train_loss: 0.21386641263961792\n",
            "16870 val_loss: 0.33100923895835876, train_loss: 0.21291910111904144\n",
            "16880 val_loss: 0.3275676965713501, train_loss: 0.21333420276641846\n",
            "16890 val_loss: 0.32596850395202637, train_loss: 0.21320714056491852\n",
            "16900 val_loss: 0.3287373185157776, train_loss: 0.21219992637634277\n",
            "16910 val_loss: 0.3305746018886566, train_loss: 0.21235327422618866\n",
            "16920 val_loss: 0.3327522277832031, train_loss: 0.21261721849441528\n",
            "16930 val_loss: 0.3323988616466522, train_loss: 0.2120838165283203\n",
            "16940 val_loss: 0.3292244076728821, train_loss: 0.2111218273639679\n",
            "16950 val_loss: 0.329728901386261, train_loss: 0.21093925833702087\n",
            "16960 val_loss: 0.33069366216659546, train_loss: 0.2109217792749405\n",
            "16970 val_loss: 0.333222359418869, train_loss: 0.21111099421977997\n",
            "16980 val_loss: 0.3345701992511749, train_loss: 0.21127116680145264\n",
            "16990 val_loss: 0.3269037902355194, train_loss: 0.2102990299463272\n",
            "17000 val_loss: 0.3318251967430115, train_loss: 0.21039073169231415\n",
            "17010 val_loss: 0.33055949211120605, train_loss: 0.2099796086549759\n",
            "17020 val_loss: 0.33242887258529663, train_loss: 0.2099592387676239\n",
            "17030 val_loss: 0.32814329862594604, train_loss: 0.20945441722869873\n",
            "17040 val_loss: 0.33045706152915955, train_loss: 0.20923970639705658\n",
            "17050 val_loss: 0.3325847089290619, train_loss: 0.20964665710926056\n",
            "17060 val_loss: 0.33059296011924744, train_loss: 0.20940715074539185\n",
            "17070 val_loss: 0.3318606913089752, train_loss: 0.20903518795967102\n",
            "17080 val_loss: 0.3273533582687378, train_loss: 0.20860043168067932\n",
            "17090 val_loss: 0.33179008960723877, train_loss: 0.208778515458107\n",
            "17100 val_loss: 0.32763707637786865, train_loss: 0.20798227190971375\n",
            "17110 val_loss: 0.3349681496620178, train_loss: 0.20938821136951447\n",
            "17120 val_loss: 0.32798999547958374, train_loss: 0.20752085745334625\n",
            "17130 val_loss: 0.32741495966911316, train_loss: 0.20735815167427063\n",
            "17140 val_loss: 0.3262536823749542, train_loss: 0.2070206254720688\n",
            "17150 val_loss: 0.32517093420028687, train_loss: 0.20690427720546722\n",
            "17160 val_loss: 0.3331826627254486, train_loss: 0.20776844024658203\n",
            "17170 val_loss: 0.32875514030456543, train_loss: 0.20668639242649078\n",
            "17180 val_loss: 0.32724085450172424, train_loss: 0.20630910992622375\n",
            "17190 val_loss: 0.3329199254512787, train_loss: 0.2072959840297699\n",
            "17200 val_loss: 0.3324734568595886, train_loss: 0.206280916929245\n",
            "17210 val_loss: 0.3303912878036499, train_loss: 0.20568077266216278\n",
            "17220 val_loss: 0.32940366864204407, train_loss: 0.20591530203819275\n",
            "17230 val_loss: 0.32607510685920715, train_loss: 0.20512118935585022\n",
            "17240 val_loss: 0.33190977573394775, train_loss: 0.20535193383693695\n",
            "17250 val_loss: 0.33071190118789673, train_loss: 0.2049400806427002\n",
            "17260 val_loss: 0.33530113101005554, train_loss: 0.20566493272781372\n",
            "17270 val_loss: 0.32522493600845337, train_loss: 0.20433983206748962\n",
            "17280 val_loss: 0.3367912173271179, train_loss: 0.2058330625295639\n",
            "17290 val_loss: 0.33205142617225647, train_loss: 0.20455673336982727\n",
            "17300 val_loss: 0.3281899392604828, train_loss: 0.20354044437408447\n",
            "17310 val_loss: 0.33463144302368164, train_loss: 0.2043200135231018\n",
            "17320 val_loss: 0.3298085033893585, train_loss: 0.20363971590995789\n",
            "17330 val_loss: 0.3244834542274475, train_loss: 0.2032669633626938\n",
            "17340 val_loss: 0.33171629905700684, train_loss: 0.2033856064081192\n",
            "17350 val_loss: 0.33353981375694275, train_loss: 0.20378002524375916\n",
            "17360 val_loss: 0.32697129249572754, train_loss: 0.2024480104446411\n",
            "17370 val_loss: 0.32594773173332214, train_loss: 0.20241808891296387\n",
            "17380 val_loss: 0.32610899209976196, train_loss: 0.2019134759902954\n",
            "17390 val_loss: 0.32822808623313904, train_loss: 0.2017756998538971\n",
            "17400 val_loss: 0.33098429441452026, train_loss: 0.2016988843679428\n",
            "17410 val_loss: 0.3250388503074646, train_loss: 0.20123590528964996\n",
            "17420 val_loss: 0.3284953832626343, train_loss: 0.20107905566692352\n",
            "17430 val_loss: 0.32716089487075806, train_loss: 0.2014322280883789\n",
            "17440 val_loss: 0.3311367332935333, train_loss: 0.20125484466552734\n",
            "17450 val_loss: 0.33974480628967285, train_loss: 0.2031029462814331\n",
            "17460 val_loss: 0.3279772102832794, train_loss: 0.20029962062835693\n",
            "17470 val_loss: 0.33051738142967224, train_loss: 0.2000793218612671\n",
            "17480 val_loss: 0.33012035489082336, train_loss: 0.19990098476409912\n",
            "17490 val_loss: 0.3308389186859131, train_loss: 0.1997675597667694\n",
            "17500 val_loss: 0.3324702978134155, train_loss: 0.20002470910549164\n",
            "17510 val_loss: 0.32810941338539124, train_loss: 0.19964422285556793\n",
            "17520 val_loss: 0.3335593640804291, train_loss: 0.20011526346206665\n",
            "17530 val_loss: 0.32295963168144226, train_loss: 0.19932380318641663\n",
            "17540 val_loss: 0.3256755769252777, train_loss: 0.198553204536438\n",
            "17550 val_loss: 0.33512935042381287, train_loss: 0.20021900534629822\n",
            "17560 val_loss: 0.3209885060787201, train_loss: 0.1987144500017166\n",
            "17570 val_loss: 0.33060890436172485, train_loss: 0.19854754209518433\n",
            "17580 val_loss: 0.3278433382511139, train_loss: 0.19810810685157776\n",
            "17590 val_loss: 0.32636183500289917, train_loss: 0.19787004590034485\n",
            "17600 val_loss: 0.3256697356700897, train_loss: 0.19803236424922943\n",
            "17610 val_loss: 0.3252066969871521, train_loss: 0.1977105438709259\n",
            "17620 val_loss: 0.3301825225353241, train_loss: 0.19799844920635223\n",
            "17630 val_loss: 0.3261980712413788, train_loss: 0.19747556746006012\n",
            "17640 val_loss: 0.3191179633140564, train_loss: 0.1975313276052475\n",
            "17650 val_loss: 0.3265377879142761, train_loss: 0.19779491424560547\n",
            "17660 val_loss: 0.3327990174293518, train_loss: 0.19842331111431122\n",
            "17670 val_loss: 0.3280738592147827, train_loss: 0.19735197722911835\n",
            "17680 val_loss: 0.3234444260597229, train_loss: 0.196188285946846\n",
            "17690 val_loss: 0.3281913995742798, train_loss: 0.19667738676071167\n",
            "17700 val_loss: 0.3258609175682068, train_loss: 0.19628405570983887\n",
            "17710 val_loss: 0.3248770833015442, train_loss: 0.19606058299541473\n",
            "17720 val_loss: 0.32692280411720276, train_loss: 0.19570070505142212\n",
            "17730 val_loss: 0.3189701437950134, train_loss: 0.19557586312294006\n",
            "17740 val_loss: 0.33275631070137024, train_loss: 0.19659805297851562\n",
            "17750 val_loss: 0.32542458176612854, train_loss: 0.19523169100284576\n",
            "17760 val_loss: 0.322041779756546, train_loss: 0.19503916800022125\n",
            "17770 val_loss: 0.32088035345077515, train_loss: 0.19505855441093445\n",
            "17780 val_loss: 0.3249821364879608, train_loss: 0.19501541554927826\n",
            "17790 val_loss: 0.32983481884002686, train_loss: 0.1958475112915039\n",
            "17800 val_loss: 0.326079785823822, train_loss: 0.1948126256465912\n",
            "17810 val_loss: 0.32672181725502014, train_loss: 0.19492293894290924\n",
            "17820 val_loss: 0.3271268308162689, train_loss: 0.19463172554969788\n",
            "17830 val_loss: 0.3215610980987549, train_loss: 0.19391007721424103\n",
            "17840 val_loss: 0.31946468353271484, train_loss: 0.19424889981746674\n",
            "17850 val_loss: 0.3246966302394867, train_loss: 0.19408662617206573\n",
            "17860 val_loss: 0.32475823163986206, train_loss: 0.19390921294689178\n",
            "17870 val_loss: 0.33076155185699463, train_loss: 0.1938474178314209\n",
            "17880 val_loss: 0.32617706060409546, train_loss: 0.1938725858926773\n",
            "17890 val_loss: 0.32314518094062805, train_loss: 0.19304096698760986\n",
            "17900 val_loss: 0.327105849981308, train_loss: 0.19321665167808533\n",
            "17910 val_loss: 0.31446149945259094, train_loss: 0.19364778697490692\n",
            "17920 val_loss: 0.32747581601142883, train_loss: 0.19315433502197266\n",
            "17930 val_loss: 0.3248414695262909, train_loss: 0.19240552186965942\n",
            "17940 val_loss: 0.3274698257446289, train_loss: 0.19253121316432953\n",
            "17950 val_loss: 0.32601529359817505, train_loss: 0.19225890934467316\n",
            "17960 val_loss: 0.32488465309143066, train_loss: 0.1922711730003357\n",
            "17970 val_loss: 0.32351794838905334, train_loss: 0.19161121547222137\n",
            "17980 val_loss: 0.3331085741519928, train_loss: 0.19357866048812866\n",
            "17990 val_loss: 0.3277825713157654, train_loss: 0.19218546152114868\n",
            "18000 val_loss: 0.3310798108577728, train_loss: 0.19310730695724487\n",
            "18010 val_loss: 0.3222070336341858, train_loss: 0.19136467576026917\n",
            "18020 val_loss: 0.31609615683555603, train_loss: 0.1909928172826767\n",
            "18030 val_loss: 0.33464112877845764, train_loss: 0.1933005005121231\n",
            "18040 val_loss: 0.31733131408691406, train_loss: 0.1906615048646927\n",
            "18050 val_loss: 0.3187292516231537, train_loss: 0.18987853825092316\n",
            "18060 val_loss: 0.32119032740592957, train_loss: 0.19020813703536987\n",
            "18070 val_loss: 0.32191792130470276, train_loss: 0.18970096111297607\n",
            "18080 val_loss: 0.32356736063957214, train_loss: 0.19006779789924622\n",
            "18090 val_loss: 0.3198768198490143, train_loss: 0.1895246058702469\n",
            "18100 val_loss: 0.32219016551971436, train_loss: 0.1889709234237671\n",
            "18110 val_loss: 0.32719454169273376, train_loss: 0.19006969034671783\n",
            "18120 val_loss: 0.32370203733444214, train_loss: 0.18907271325588226\n",
            "18130 val_loss: 0.3193054795265198, train_loss: 0.18789221346378326\n",
            "18140 val_loss: 0.31985726952552795, train_loss: 0.18842913210391998\n",
            "18150 val_loss: 0.31971725821495056, train_loss: 0.1879505068063736\n",
            "18160 val_loss: 0.32363399863243103, train_loss: 0.18869329988956451\n",
            "18170 val_loss: 0.3195079565048218, train_loss: 0.18807975947856903\n",
            "18180 val_loss: 0.3216899633407593, train_loss: 0.18801146745681763\n",
            "18190 val_loss: 0.319246381521225, train_loss: 0.1878557801246643\n",
            "18200 val_loss: 0.32218417525291443, train_loss: 0.18815797567367554\n",
            "18210 val_loss: 0.3146410286426544, train_loss: 0.18750330805778503\n",
            "18220 val_loss: 0.3201596438884735, train_loss: 0.1874081790447235\n",
            "18230 val_loss: 0.32294219732284546, train_loss: 0.1870364099740982\n",
            "18240 val_loss: 0.3234628140926361, train_loss: 0.18750643730163574\n",
            "18250 val_loss: 0.31950506567955017, train_loss: 0.18630635738372803\n",
            "18260 val_loss: 0.31670716404914856, train_loss: 0.18576355278491974\n",
            "18270 val_loss: 0.31552720069885254, train_loss: 0.18518461287021637\n",
            "18280 val_loss: 0.3172202706336975, train_loss: 0.18525786697864532\n",
            "18290 val_loss: 0.3307664394378662, train_loss: 0.18765728175640106\n",
            "18300 val_loss: 0.3178699016571045, train_loss: 0.18493366241455078\n",
            "18310 val_loss: 0.318024218082428, train_loss: 0.18499897420406342\n",
            "18320 val_loss: 0.3305172622203827, train_loss: 0.18716289103031158\n",
            "18330 val_loss: 0.3250461518764496, train_loss: 0.18625709414482117\n",
            "18340 val_loss: 0.3172908127307892, train_loss: 0.18512164056301117\n",
            "18350 val_loss: 0.31582048535346985, train_loss: 0.1849527806043625\n",
            "18360 val_loss: 0.3199009895324707, train_loss: 0.18490509688854218\n",
            "18370 val_loss: 0.3186703622341156, train_loss: 0.1842467486858368\n",
            "18380 val_loss: 0.3179645538330078, train_loss: 0.18408353626728058\n",
            "18390 val_loss: 0.32222357392311096, train_loss: 0.18464674055576324\n",
            "18400 val_loss: 0.3098352253437042, train_loss: 0.18520121276378632\n",
            "18410 val_loss: 0.31961187720298767, train_loss: 0.18435898423194885\n",
            "18420 val_loss: 0.3205549418926239, train_loss: 0.18394382297992706\n",
            "18430 val_loss: 0.32706284523010254, train_loss: 0.18500250577926636\n",
            "18440 val_loss: 0.3197098672389984, train_loss: 0.1837313175201416\n",
            "18450 val_loss: 0.3141244351863861, train_loss: 0.18368631601333618\n",
            "18460 val_loss: 0.3209649622440338, train_loss: 0.18392620980739594\n",
            "18470 val_loss: 0.31988975405693054, train_loss: 0.18320275843143463\n",
            "18480 val_loss: 0.3133818209171295, train_loss: 0.18258202075958252\n",
            "18490 val_loss: 0.3157946765422821, train_loss: 0.18217328190803528\n",
            "18500 val_loss: 0.3240868151187897, train_loss: 0.18347834050655365\n",
            "18510 val_loss: 0.320319265127182, train_loss: 0.18237370252609253\n",
            "18520 val_loss: 0.3231544494628906, train_loss: 0.1833202838897705\n",
            "18530 val_loss: 0.3128422200679779, train_loss: 0.18194012343883514\n",
            "18540 val_loss: 0.31864237785339355, train_loss: 0.1821754425764084\n",
            "18550 val_loss: 0.3174692690372467, train_loss: 0.1819201111793518\n",
            "18560 val_loss: 0.32135090231895447, train_loss: 0.1824977993965149\n",
            "18570 val_loss: 0.3126041293144226, train_loss: 0.18114317953586578\n",
            "18580 val_loss: 0.31323638558387756, train_loss: 0.18127459287643433\n",
            "18590 val_loss: 0.3239152729511261, train_loss: 0.18216729164123535\n",
            "18600 val_loss: 0.3362058103084564, train_loss: 0.18524055182933807\n",
            "18610 val_loss: 0.31420573592185974, train_loss: 0.18045590817928314\n",
            "18620 val_loss: 0.31213584542274475, train_loss: 0.17964136600494385\n",
            "18630 val_loss: 0.31919726729393005, train_loss: 0.17977343499660492\n",
            "18640 val_loss: 0.3205101490020752, train_loss: 0.18000145256519318\n",
            "18650 val_loss: 0.3202849328517914, train_loss: 0.17990532517433167\n",
            "18660 val_loss: 0.3148145079612732, train_loss: 0.1793341338634491\n",
            "18670 val_loss: 0.3238564133644104, train_loss: 0.1804858297109604\n",
            "18680 val_loss: 0.31996166706085205, train_loss: 0.1800074726343155\n",
            "18690 val_loss: 0.32244154810905457, train_loss: 0.1802728772163391\n",
            "18700 val_loss: 0.3190496861934662, train_loss: 0.17907623946666718\n",
            "18710 val_loss: 0.3210398256778717, train_loss: 0.17933157086372375\n",
            "18720 val_loss: 0.3198093771934509, train_loss: 0.17864619195461273\n",
            "18730 val_loss: 0.3226664066314697, train_loss: 0.17909200489521027\n",
            "18740 val_loss: 0.31711870431900024, train_loss: 0.17854464054107666\n",
            "18750 val_loss: 0.31657087802886963, train_loss: 0.1777193695306778\n",
            "18760 val_loss: 0.32174357771873474, train_loss: 0.17797671258449554\n",
            "18770 val_loss: 0.31746700406074524, train_loss: 0.1766977459192276\n",
            "18780 val_loss: 0.3154802620410919, train_loss: 0.17655974626541138\n",
            "18790 val_loss: 0.31988680362701416, train_loss: 0.17713946104049683\n",
            "18800 val_loss: 0.3152301013469696, train_loss: 0.17626921832561493\n",
            "18810 val_loss: 0.3137829899787903, train_loss: 0.17577548325061798\n",
            "18820 val_loss: 0.31450018286705017, train_loss: 0.17591315507888794\n",
            "18830 val_loss: 0.31796231865882874, train_loss: 0.1762399524450302\n",
            "18840 val_loss: 0.3129540979862213, train_loss: 0.17579694092273712\n",
            "18850 val_loss: 0.3161450922489166, train_loss: 0.1754922866821289\n",
            "18860 val_loss: 0.315860390663147, train_loss: 0.17528916895389557\n",
            "18870 val_loss: 0.32011738419532776, train_loss: 0.17579849064350128\n",
            "18880 val_loss: 0.30947449803352356, train_loss: 0.17545185983181\n",
            "18890 val_loss: 0.31778988242149353, train_loss: 0.17480336129665375\n",
            "18900 val_loss: 0.3256949782371521, train_loss: 0.17612852156162262\n",
            "18910 val_loss: 0.3213086724281311, train_loss: 0.17457394301891327\n",
            "18920 val_loss: 0.32314813137054443, train_loss: 0.17494530975818634\n",
            "18930 val_loss: 0.3155212104320526, train_loss: 0.17356890439987183\n",
            "18940 val_loss: 0.3271327018737793, train_loss: 0.17454132437705994\n",
            "18950 val_loss: 0.32298389077186584, train_loss: 0.1740112453699112\n",
            "18960 val_loss: 0.31355249881744385, train_loss: 0.17311739921569824\n",
            "18970 val_loss: 0.311453640460968, train_loss: 0.17319342494010925\n",
            "18980 val_loss: 0.31909459829330444, train_loss: 0.17285500466823578\n",
            "18990 val_loss: 0.3172592520713806, train_loss: 0.17213396728038788\n",
            "19000 val_loss: 0.31775963306427, train_loss: 0.17189431190490723\n",
            "19010 val_loss: 0.31709957122802734, train_loss: 0.171423077583313\n",
            "19020 val_loss: 0.31231746077537537, train_loss: 0.1711725890636444\n",
            "19030 val_loss: 0.3094402551651001, train_loss: 0.17113007605075836\n",
            "19040 val_loss: 0.3098055124282837, train_loss: 0.17025971412658691\n",
            "19050 val_loss: 0.3098597824573517, train_loss: 0.169697567820549\n",
            "19060 val_loss: 0.3106295168399811, train_loss: 0.16883137822151184\n",
            "19070 val_loss: 0.3042747974395752, train_loss: 0.16892501711845398\n",
            "19080 val_loss: 0.3092358708381653, train_loss: 0.16711024940013885\n",
            "19090 val_loss: 0.30152666568756104, train_loss: 0.16686534881591797\n",
            "19100 val_loss: 0.30490100383758545, train_loss: 0.16534246504306793\n",
            "19110 val_loss: 0.30065569281578064, train_loss: 0.16425463557243347\n",
            "19120 val_loss: 0.2998979091644287, train_loss: 0.1630616933107376\n",
            "19130 val_loss: 0.2946259081363678, train_loss: 0.16259244084358215\n",
            "19140 val_loss: 0.2943035960197449, train_loss: 0.1617467999458313\n",
            "19150 val_loss: 0.2915080487728119, train_loss: 0.1608358472585678\n",
            "19160 val_loss: 0.2905753552913666, train_loss: 0.1598270833492279\n",
            "19170 val_loss: 0.2901669144630432, train_loss: 0.15910539031028748\n",
            "19180 val_loss: 0.2872423827648163, train_loss: 0.15812984108924866\n",
            "19190 val_loss: 0.28939786553382874, train_loss: 0.15794916450977325\n",
            "19200 val_loss: 0.2867947518825531, train_loss: 0.15714752674102783\n",
            "19210 val_loss: 0.28849586844444275, train_loss: 0.1566597819328308\n",
            "19220 val_loss: 0.2863888442516327, train_loss: 0.15599438548088074\n",
            "19230 val_loss: 0.28513726592063904, train_loss: 0.15553143620491028\n",
            "19240 val_loss: 0.2839718461036682, train_loss: 0.15481096506118774\n",
            "19250 val_loss: 0.28041255474090576, train_loss: 0.1544797122478485\n",
            "19260 val_loss: 0.28412336111068726, train_loss: 0.1541600227355957\n",
            "19270 val_loss: 0.2816780209541321, train_loss: 0.15288332104682922\n",
            "19280 val_loss: 0.2845960259437561, train_loss: 0.15237510204315186\n",
            "19290 val_loss: 0.28170257806777954, train_loss: 0.1518861949443817\n",
            "19300 val_loss: 0.2812277376651764, train_loss: 0.1511663794517517\n",
            "19310 val_loss: 0.279670387506485, train_loss: 0.15063941478729248\n",
            "19320 val_loss: 0.27811798453330994, train_loss: 0.15003091096878052\n",
            "19330 val_loss: 0.2785692512989044, train_loss: 0.14971913397312164\n",
            "19340 val_loss: 0.2803371548652649, train_loss: 0.149533212184906\n",
            "19350 val_loss: 0.27626481652259827, train_loss: 0.1488744020462036\n",
            "19360 val_loss: 0.2765597403049469, train_loss: 0.14826805889606476\n",
            "19370 val_loss: 0.27940434217453003, train_loss: 0.14806994795799255\n",
            "19380 val_loss: 0.28263944387435913, train_loss: 0.1485191285610199\n",
            "19390 val_loss: 0.2791144847869873, train_loss: 0.14629700779914856\n",
            "19400 val_loss: 0.28289249539375305, train_loss: 0.14638076722621918\n",
            "19410 val_loss: 0.28315994143486023, train_loss: 0.1453813761472702\n",
            "19420 val_loss: 0.281343936920166, train_loss: 0.14447124302387238\n",
            "19430 val_loss: 0.27828535437583923, train_loss: 0.14379173517227173\n",
            "19440 val_loss: 0.27969440817832947, train_loss: 0.1435004323720932\n",
            "19450 val_loss: 0.2785339653491974, train_loss: 0.14291241765022278\n",
            "19460 val_loss: 0.27437567710876465, train_loss: 0.14237986505031586\n",
            "19470 val_loss: 0.27277857065200806, train_loss: 0.14239254593849182\n",
            "19480 val_loss: 0.2742113769054413, train_loss: 0.14197444915771484\n",
            "19490 val_loss: 0.2719632685184479, train_loss: 0.1411028653383255\n",
            "19500 val_loss: 0.2731458842754364, train_loss: 0.14053715765476227\n",
            "19510 val_loss: 0.27258121967315674, train_loss: 0.1400553286075592\n",
            "19520 val_loss: 0.28580442070961, train_loss: 0.1435612589120865\n",
            "19530 val_loss: 0.2709081172943115, train_loss: 0.13918636739253998\n",
            "19540 val_loss: 0.2708858251571655, train_loss: 0.13957801461219788\n",
            "19550 val_loss: 0.2736181914806366, train_loss: 0.13971254229545593\n",
            "19560 val_loss: 0.2666029632091522, train_loss: 0.13819725811481476\n",
            "19570 val_loss: 0.27143654227256775, train_loss: 0.1375240683555603\n",
            "19580 val_loss: 0.2693583071231842, train_loss: 0.13689951598644257\n",
            "19590 val_loss: 0.2684547007083893, train_loss: 0.13662323355674744\n",
            "19600 val_loss: 0.2686612606048584, train_loss: 0.13650943338871002\n",
            "19610 val_loss: 0.2744094431400299, train_loss: 0.13630886375904083\n",
            "19620 val_loss: 0.2732055187225342, train_loss: 0.1358804553747177\n",
            "19630 val_loss: 0.271633505821228, train_loss: 0.13582460582256317\n",
            "19640 val_loss: 0.2661483585834503, train_loss: 0.1348065882921219\n",
            "19650 val_loss: 0.26443591713905334, train_loss: 0.13455790281295776\n",
            "19660 val_loss: 0.26545947790145874, train_loss: 0.1356230080127716\n",
            "19670 val_loss: 0.26344427466392517, train_loss: 0.1336347758769989\n",
            "19680 val_loss: 0.26369327306747437, train_loss: 0.13363991677761078\n",
            "19690 val_loss: 0.2632720470428467, train_loss: 0.1333869844675064\n",
            "19700 val_loss: 0.26426512002944946, train_loss: 0.13254371285438538\n",
            "19710 val_loss: 0.26490288972854614, train_loss: 0.13260066509246826\n",
            "19720 val_loss: 0.26591065526008606, train_loss: 0.13206757605075836\n",
            "19730 val_loss: 0.2639565169811249, train_loss: 0.13142362236976624\n",
            "19740 val_loss: 0.2617771327495575, train_loss: 0.1312885731458664\n",
            "19750 val_loss: 0.2696029245853424, train_loss: 0.1319955736398697\n",
            "19760 val_loss: 0.26417985558509827, train_loss: 0.1306149959564209\n",
            "19770 val_loss: 0.2609197199344635, train_loss: 0.13257797062397003\n",
            "19780 val_loss: 0.26180100440979004, train_loss: 0.1297987550497055\n",
            "19790 val_loss: 0.25977209210395813, train_loss: 0.13257934153079987\n",
            "19800 val_loss: 0.26615238189697266, train_loss: 0.12977245450019836\n",
            "19810 val_loss: 0.2583794891834259, train_loss: 0.1292787790298462\n",
            "19820 val_loss: 0.26186403632164, train_loss: 0.12905928492546082\n",
            "19830 val_loss: 0.26158633828163147, train_loss: 0.1284673660993576\n",
            "19840 val_loss: 0.26341140270233154, train_loss: 0.12877768278121948\n",
            "19850 val_loss: 0.2600732445716858, train_loss: 0.12793706357479095\n",
            "19860 val_loss: 0.25805193185806274, train_loss: 0.1273454874753952\n",
            "19870 val_loss: 0.260414719581604, train_loss: 0.12708435952663422\n",
            "19880 val_loss: 0.2651147246360779, train_loss: 0.12698198854923248\n",
            "19890 val_loss: 0.258569598197937, train_loss: 0.1264861524105072\n",
            "19900 val_loss: 0.2615211606025696, train_loss: 0.12633493542671204\n",
            "19910 val_loss: 0.2601798176765442, train_loss: 0.1257755160331726\n",
            "19920 val_loss: 0.26028403639793396, train_loss: 0.12577830255031586\n",
            "19930 val_loss: 0.2600327432155609, train_loss: 0.12491597980260849\n",
            "19940 val_loss: 0.26192641258239746, train_loss: 0.1252811700105667\n",
            "19950 val_loss: 0.2630928158760071, train_loss: 0.12758763134479523\n",
            "19960 val_loss: 0.2536783814430237, train_loss: 0.12476634979248047\n",
            "19970 val_loss: 0.25569674372673035, train_loss: 0.12745067477226257\n",
            "19980 val_loss: 0.26652559638023376, train_loss: 0.12362789362668991\n",
            "19990 val_loss: 0.25821685791015625, train_loss: 0.12325432896614075\n",
            "20000 val_loss: 0.2687126696109772, train_loss: 0.12323673069477081\n",
            "20010 val_loss: 0.2581940293312073, train_loss: 0.12243203073740005\n",
            "20020 val_loss: 0.2537000775337219, train_loss: 0.1243608370423317\n",
            "20030 val_loss: 0.256895512342453, train_loss: 0.12284177541732788\n",
            "20040 val_loss: 0.25735723972320557, train_loss: 0.122027687728405\n",
            "20050 val_loss: 0.26153242588043213, train_loss: 0.12191247195005417\n",
            "20060 val_loss: 0.26706257462501526, train_loss: 0.1225319504737854\n",
            "20070 val_loss: 0.25843483209609985, train_loss: 0.12070826441049576\n",
            "20080 val_loss: 0.2675822675228119, train_loss: 0.12242908775806427\n",
            "20090 val_loss: 0.2563299536705017, train_loss: 0.12101977318525314\n",
            "20100 val_loss: 0.2580515444278717, train_loss: 0.12048148363828659\n",
            "20110 val_loss: 0.26185575127601624, train_loss: 0.12070468813180923\n",
            "20120 val_loss: 0.25619444251060486, train_loss: 0.11978273093700409\n",
            "20130 val_loss: 0.2557329535484314, train_loss: 0.1202789843082428\n",
            "20140 val_loss: 0.2536402642726898, train_loss: 0.11966149508953094\n",
            "20150 val_loss: 0.25452008843421936, train_loss: 0.11924324184656143\n",
            "20160 val_loss: 0.2672933340072632, train_loss: 0.12203064560890198\n",
            "20170 val_loss: 0.2518247961997986, train_loss: 0.12296494841575623\n",
            "20180 val_loss: 0.25403186678886414, train_loss: 0.1188177689909935\n",
            "20190 val_loss: 0.2524462938308716, train_loss: 0.11878033727407455\n",
            "20200 val_loss: 0.26047006249427795, train_loss: 0.11815962195396423\n",
            "20210 val_loss: 0.25810518860816956, train_loss: 0.11753153055906296\n",
            "20220 val_loss: 0.2577368915081024, train_loss: 0.11771669238805771\n",
            "20230 val_loss: 0.2557049095630646, train_loss: 0.11795192956924438\n",
            "20240 val_loss: 0.2550959885120392, train_loss: 0.11733529716730118\n",
            "20250 val_loss: 0.2648356556892395, train_loss: 0.12075668573379517\n",
            "20260 val_loss: 0.25612059235572815, train_loss: 0.1173056811094284\n",
            "20270 val_loss: 0.2495225965976715, train_loss: 0.1171262338757515\n",
            "20280 val_loss: 0.2549766004085541, train_loss: 0.11634620279073715\n",
            "20290 val_loss: 0.2551054358482361, train_loss: 0.11630821973085403\n",
            "20300 val_loss: 0.2638270854949951, train_loss: 0.11874338984489441\n",
            "20310 val_loss: 0.266160249710083, train_loss: 0.11700821667909622\n",
            "20320 val_loss: 0.26079699397087097, train_loss: 0.11618045717477798\n",
            "20330 val_loss: 0.25314903259277344, train_loss: 0.11549285799264908\n",
            "20340 val_loss: 0.25283142924308777, train_loss: 0.11560636758804321\n",
            "20350 val_loss: 0.25858503580093384, train_loss: 0.1160011813044548\n",
            "20360 val_loss: 0.25418028235435486, train_loss: 0.1151777058839798\n",
            "20370 val_loss: 0.2535727024078369, train_loss: 0.11483220010995865\n",
            "20380 val_loss: 0.25247102975845337, train_loss: 0.11491359025239944\n",
            "20390 val_loss: 0.26130884885787964, train_loss: 0.11481232196092606\n",
            "20400 val_loss: 0.25432977080345154, train_loss: 0.11560802906751633\n",
            "20410 val_loss: 0.25457072257995605, train_loss: 0.11409575492143631\n",
            "20420 val_loss: 0.2566964328289032, train_loss: 0.11390362679958344\n",
            "20430 val_loss: 0.2550964951515198, train_loss: 0.11354990303516388\n",
            "20440 val_loss: 0.2527860999107361, train_loss: 0.11325068026781082\n",
            "20450 val_loss: 0.2518429160118103, train_loss: 0.11322277784347534\n",
            "20460 val_loss: 0.2580511271953583, train_loss: 0.11439096927642822\n",
            "20470 val_loss: 0.25253644585609436, train_loss: 0.11261789500713348\n",
            "20480 val_loss: 0.2562006413936615, train_loss: 0.11328300088644028\n",
            "20490 val_loss: 0.2577430009841919, train_loss: 0.11295608431100845\n",
            "20500 val_loss: 0.249266117811203, train_loss: 0.11239521205425262\n",
            "20510 val_loss: 0.253421813249588, train_loss: 0.11193285137414932\n",
            "20520 val_loss: 0.25231924653053284, train_loss: 0.11163884401321411\n",
            "20530 val_loss: 0.25350624322891235, train_loss: 0.11158855259418488\n",
            "20540 val_loss: 0.250884473323822, train_loss: 0.11272300034761429\n",
            "20550 val_loss: 0.25304439663887024, train_loss: 0.11220694333314896\n",
            "20560 val_loss: 0.2524189054965973, train_loss: 0.11110030114650726\n",
            "20570 val_loss: 0.25169044733047485, train_loss: 0.11096643656492233\n",
            "20580 val_loss: 0.25211942195892334, train_loss: 0.11170367896556854\n",
            "20590 val_loss: 0.2481721043586731, train_loss: 0.11086496710777283\n",
            "20600 val_loss: 0.25519058108329773, train_loss: 0.11091429740190506\n",
            "20610 val_loss: 0.25495827198028564, train_loss: 0.11101797968149185\n",
            "20620 val_loss: 0.24923484027385712, train_loss: 0.11047109961509705\n",
            "20630 val_loss: 0.2621355354785919, train_loss: 0.11192475259304047\n",
            "20640 val_loss: 0.24491842091083527, train_loss: 0.1144162267446518\n",
            "20650 val_loss: 0.25622615218162537, train_loss: 0.1120179072022438\n",
            "20660 val_loss: 0.2566496729850769, train_loss: 0.10927597433328629\n",
            "20670 val_loss: 0.25084400177001953, train_loss: 0.10939006507396698\n",
            "20680 val_loss: 0.2551877498626709, train_loss: 0.108615942299366\n",
            "20690 val_loss: 0.2765122652053833, train_loss: 0.11621420085430145\n",
            "20700 val_loss: 0.25135916471481323, train_loss: 0.10813245177268982\n",
            "20710 val_loss: 0.2552727162837982, train_loss: 0.10852763056755066\n",
            "20720 val_loss: 0.24819642305374146, train_loss: 0.10746704787015915\n",
            "20730 val_loss: 0.24593092501163483, train_loss: 0.10820939391851425\n",
            "20740 val_loss: 0.2501789629459381, train_loss: 0.10742119699716568\n",
            "20750 val_loss: 0.2529671788215637, train_loss: 0.1079907938838005\n",
            "20760 val_loss: 0.25504592061042786, train_loss: 0.10775914788246155\n",
            "20770 val_loss: 0.2472396194934845, train_loss: 0.107907734811306\n",
            "20780 val_loss: 0.2567910850048065, train_loss: 0.10831965506076813\n",
            "20790 val_loss: 0.24519841372966766, train_loss: 0.10712867230176926\n",
            "20800 val_loss: 0.2482149451971054, train_loss: 0.10606320947408676\n",
            "20810 val_loss: 0.25219446420669556, train_loss: 0.10624867677688599\n",
            "20820 val_loss: 0.24509379267692566, train_loss: 0.1054474264383316\n",
            "20830 val_loss: 0.2502402365207672, train_loss: 0.10522521287202835\n",
            "20840 val_loss: 0.24608416855335236, train_loss: 0.10496772825717926\n",
            "20850 val_loss: 0.2486470490694046, train_loss: 0.10469336062669754\n",
            "20860 val_loss: 0.25294268131256104, train_loss: 0.10453158617019653\n",
            "20870 val_loss: 0.25342702865600586, train_loss: 0.1045728251338005\n",
            "20880 val_loss: 0.2524254024028778, train_loss: 0.10531282424926758\n",
            "20890 val_loss: 0.2472073882818222, train_loss: 0.1038864254951477\n",
            "20900 val_loss: 0.2481435388326645, train_loss: 0.10361190885305405\n",
            "20910 val_loss: 0.2473401129245758, train_loss: 0.10343403369188309\n",
            "20920 val_loss: 0.25205495953559875, train_loss: 0.10301557928323746\n",
            "20930 val_loss: 0.24568183720111847, train_loss: 0.10311712324619293\n",
            "20940 val_loss: 0.2546543478965759, train_loss: 0.10392074286937714\n",
            "20950 val_loss: 0.26392242312431335, train_loss: 0.10783836990594864\n",
            "20960 val_loss: 0.24436278641223907, train_loss: 0.10308563709259033\n",
            "20970 val_loss: 0.25451111793518066, train_loss: 0.10300804674625397\n",
            "20980 val_loss: 0.2473095953464508, train_loss: 0.10202518850564957\n",
            "20990 val_loss: 0.25461727380752563, train_loss: 0.10102175921201706\n",
            "21000 val_loss: 0.2455846220254898, train_loss: 0.10135161131620407\n",
            "21010 val_loss: 0.24760696291923523, train_loss: 0.10112088173627853\n",
            "21020 val_loss: 0.24636727571487427, train_loss: 0.10058193653821945\n",
            "21030 val_loss: 0.2425880879163742, train_loss: 0.1017356663942337\n",
            "21040 val_loss: 0.2541271448135376, train_loss: 0.10172133892774582\n",
            "21050 val_loss: 0.24871514737606049, train_loss: 0.09954841434955597\n",
            "21060 val_loss: 0.24794837832450867, train_loss: 0.099461130797863\n",
            "21070 val_loss: 0.2501269578933716, train_loss: 0.09892778843641281\n",
            "21080 val_loss: 0.25614434480667114, train_loss: 0.09961706399917603\n",
            "21090 val_loss: 0.2543133795261383, train_loss: 0.09935315698385239\n",
            "21100 val_loss: 0.2513585090637207, train_loss: 0.09682665765285492\n",
            "21110 val_loss: 0.26026126742362976, train_loss: 0.09775254875421524\n",
            "21120 val_loss: 0.24994032084941864, train_loss: 0.09757605195045471\n",
            "21130 val_loss: 0.24783647060394287, train_loss: 0.09611450880765915\n",
            "21140 val_loss: 0.2440120428800583, train_loss: 0.09675343334674835\n",
            "21150 val_loss: 0.2530902326107025, train_loss: 0.09650483727455139\n",
            "21160 val_loss: 0.24756202101707458, train_loss: 0.09574231505393982\n",
            "21170 val_loss: 0.24933727085590363, train_loss: 0.09504178166389465\n",
            "21180 val_loss: 0.24116690456867218, train_loss: 0.09527282416820526\n",
            "21190 val_loss: 0.24857079982757568, train_loss: 0.09455662220716476\n",
            "21200 val_loss: 0.2526242136955261, train_loss: 0.09681578725576401\n",
            "21210 val_loss: 0.24758538603782654, train_loss: 0.09476716071367264\n",
            "21220 val_loss: 0.24762217700481415, train_loss: 0.09350262582302094\n",
            "21230 val_loss: 0.24629801511764526, train_loss: 0.09357987344264984\n",
            "21240 val_loss: 0.24660982191562653, train_loss: 0.09266475588083267\n",
            "21250 val_loss: 0.2524290978908539, train_loss: 0.09510785341262817\n",
            "21260 val_loss: 0.2375887930393219, train_loss: 0.09483925253152847\n",
            "21270 val_loss: 0.2469235211610794, train_loss: 0.09278688579797745\n",
            "21280 val_loss: 0.24518030881881714, train_loss: 0.09235656261444092\n",
            "21290 val_loss: 0.25256648659706116, train_loss: 0.0928402692079544\n",
            "21300 val_loss: 0.24410438537597656, train_loss: 0.09210414439439774\n",
            "21310 val_loss: 0.24568778276443481, train_loss: 0.09286469966173172\n",
            "21320 val_loss: 0.24309898912906647, train_loss: 0.09163528680801392\n",
            "21330 val_loss: 0.2443949431180954, train_loss: 0.09199848026037216\n",
            "21340 val_loss: 0.2476753443479538, train_loss: 0.09046509861946106\n",
            "21350 val_loss: 0.24764660000801086, train_loss: 0.09313446283340454\n",
            "21360 val_loss: 0.24653160572052002, train_loss: 0.09042438864707947\n",
            "21370 val_loss: 0.24838431179523468, train_loss: 0.091323621571064\n",
            "21380 val_loss: 0.2628435790538788, train_loss: 0.0979212075471878\n",
            "21390 val_loss: 0.24371983110904694, train_loss: 0.09099089354276657\n",
            "21400 val_loss: 0.2401794195175171, train_loss: 0.0916169136762619\n",
            "21410 val_loss: 0.24829280376434326, train_loss: 0.09033368527889252\n",
            "21420 val_loss: 0.24132229387760162, train_loss: 0.08979250490665436\n",
            "21430 val_loss: 0.24432511627674103, train_loss: 0.08957678079605103\n",
            "21440 val_loss: 0.24313519895076752, train_loss: 0.08921229839324951\n",
            "21450 val_loss: 0.2501351237297058, train_loss: 0.08950882405042648\n",
            "21460 val_loss: 0.2478678822517395, train_loss: 0.08870767056941986\n",
            "21470 val_loss: 0.2405710220336914, train_loss: 0.08855907618999481\n",
            "21480 val_loss: 0.2501619756221771, train_loss: 0.08822096884250641\n",
            "21490 val_loss: 0.24132563173770905, train_loss: 0.08823073655366898\n",
            "21500 val_loss: 0.24387450516223907, train_loss: 0.08803477883338928\n",
            "21510 val_loss: 0.24168309569358826, train_loss: 0.08891691267490387\n",
            "21520 val_loss: 0.245815709233284, train_loss: 0.08810432255268097\n",
            "21530 val_loss: 0.24489034712314606, train_loss: 0.08730901777744293\n",
            "21540 val_loss: 0.25016531348228455, train_loss: 0.09102204442024231\n",
            "21550 val_loss: 0.24836304783821106, train_loss: 0.08744782954454422\n",
            "21560 val_loss: 0.24417546391487122, train_loss: 0.086869977414608\n",
            "21570 val_loss: 0.25104281306266785, train_loss: 0.08778923749923706\n",
            "21580 val_loss: 0.23751288652420044, train_loss: 0.08769328892230988\n",
            "21590 val_loss: 0.24244582653045654, train_loss: 0.08628155291080475\n",
            "21600 val_loss: 0.24196961522102356, train_loss: 0.08629313111305237\n",
            "21610 val_loss: 0.2485911101102829, train_loss: 0.08692631125450134\n",
            "21620 val_loss: 0.24072547256946564, train_loss: 0.08636121451854706\n",
            "21630 val_loss: 0.24602726101875305, train_loss: 0.08564597368240356\n",
            "21640 val_loss: 0.24910952150821686, train_loss: 0.08583191782236099\n",
            "21650 val_loss: 0.2450653463602066, train_loss: 0.08517628908157349\n",
            "21660 val_loss: 0.25056028366088867, train_loss: 0.08653360605239868\n",
            "21670 val_loss: 0.23478169739246368, train_loss: 0.08683408051729202\n",
            "21680 val_loss: 0.27160704135894775, train_loss: 0.0961194559931755\n",
            "21690 val_loss: 0.2412382811307907, train_loss: 0.08507168292999268\n",
            "21700 val_loss: 0.2431114763021469, train_loss: 0.08552642911672592\n",
            "21710 val_loss: 0.2436138540506363, train_loss: 0.0847049281001091\n",
            "21720 val_loss: 0.2407994270324707, train_loss: 0.08460407704114914\n",
            "21730 val_loss: 0.24090854823589325, train_loss: 0.08456578850746155\n",
            "21740 val_loss: 0.2579204738140106, train_loss: 0.09012486785650253\n",
            "21750 val_loss: 0.24361814558506012, train_loss: 0.08343840390443802\n",
            "21760 val_loss: 0.23450899124145508, train_loss: 0.08567947149276733\n",
            "21770 val_loss: 0.23692485690116882, train_loss: 0.08729197084903717\n",
            "21780 val_loss: 0.24007923901081085, train_loss: 0.08382468670606613\n",
            "21790 val_loss: 0.24345672130584717, train_loss: 0.08385667949914932\n",
            "21800 val_loss: 0.24680030345916748, train_loss: 0.08380094915628433\n",
            "21810 val_loss: 0.23876473307609558, train_loss: 0.082654669880867\n",
            "21820 val_loss: 0.244697704911232, train_loss: 0.08225182443857193\n",
            "21830 val_loss: 0.23710834980010986, train_loss: 0.084614098072052\n",
            "21840 val_loss: 0.24542853236198425, train_loss: 0.08355595916509628\n",
            "21850 val_loss: 0.24005192518234253, train_loss: 0.08304911106824875\n",
            "21860 val_loss: 0.24234454333782196, train_loss: 0.08213894814252853\n",
            "21870 val_loss: 0.24732771515846252, train_loss: 0.08317438513040543\n",
            "21880 val_loss: 0.2385796308517456, train_loss: 0.08182837069034576\n",
            "21890 val_loss: 0.24614779651165009, train_loss: 0.08316285163164139\n",
            "21900 val_loss: 0.23883724212646484, train_loss: 0.08145792782306671\n",
            "21910 val_loss: 0.24101608991622925, train_loss: 0.0815945565700531\n",
            "21920 val_loss: 0.23715946078300476, train_loss: 0.08173512667417526\n",
            "21930 val_loss: 0.23934181034564972, train_loss: 0.08230654150247574\n",
            "21940 val_loss: 0.23821350932121277, train_loss: 0.08149035274982452\n",
            "21950 val_loss: 0.2471158355474472, train_loss: 0.08242524415254593\n",
            "21960 val_loss: 0.24665185809135437, train_loss: 0.08075620979070663\n",
            "21970 val_loss: 0.24088945984840393, train_loss: 0.08063698559999466\n",
            "21980 val_loss: 0.23773440718650818, train_loss: 0.08056677877902985\n",
            "21990 val_loss: 0.23735536634922028, train_loss: 0.08066295832395554\n",
            "22000 val_loss: 0.2411661148071289, train_loss: 0.07982407510280609\n",
            "22010 val_loss: 0.23916283249855042, train_loss: 0.08009881526231766\n",
            "22020 val_loss: 0.2407260239124298, train_loss: 0.07998140156269073\n",
            "22030 val_loss: 0.24737244844436646, train_loss: 0.08131171017885208\n",
            "22040 val_loss: 0.24535390734672546, train_loss: 0.08117279410362244\n",
            "22050 val_loss: 0.24215289950370789, train_loss: 0.07900701463222504\n",
            "22060 val_loss: 0.2521042823791504, train_loss: 0.08065825700759888\n",
            "22070 val_loss: 0.24242350459098816, train_loss: 0.07917968928813934\n",
            "22080 val_loss: 0.24628981947898865, train_loss: 0.07935500890016556\n",
            "22090 val_loss: 0.24201470613479614, train_loss: 0.07899629324674606\n",
            "22100 val_loss: 0.24623489379882812, train_loss: 0.0786905512213707\n",
            "22110 val_loss: 0.2501143515110016, train_loss: 0.07870811969041824\n",
            "22120 val_loss: 0.24484674632549286, train_loss: 0.07826629281044006\n",
            "22130 val_loss: 0.23971231281757355, train_loss: 0.07845532149076462\n",
            "22140 val_loss: 0.2617117762565613, train_loss: 0.0822114571928978\n",
            "22150 val_loss: 0.24730293452739716, train_loss: 0.07848741859197617\n",
            "22160 val_loss: 0.24710237979888916, train_loss: 0.07804416120052338\n",
            "22170 val_loss: 0.23914983868598938, train_loss: 0.07795627415180206\n",
            "22180 val_loss: 0.23884253203868866, train_loss: 0.07775741815567017\n",
            "22190 val_loss: 0.2369173765182495, train_loss: 0.07876867055892944\n",
            "22200 val_loss: 0.2431175410747528, train_loss: 0.07752154767513275\n",
            "22210 val_loss: 0.2398279309272766, train_loss: 0.07773225009441376\n",
            "22220 val_loss: 0.24965150654315948, train_loss: 0.0782540887594223\n",
            "22230 val_loss: 0.24640382826328278, train_loss: 0.07786273211240768\n",
            "22240 val_loss: 0.23986414074897766, train_loss: 0.07750964909791946\n",
            "22250 val_loss: 0.23757536709308624, train_loss: 0.0791783556342125\n",
            "22260 val_loss: 0.24388955533504486, train_loss: 0.07741916179656982\n",
            "22270 val_loss: 0.2463892102241516, train_loss: 0.07652140408754349\n",
            "22280 val_loss: 0.24866530299186707, train_loss: 0.0782768577337265\n",
            "22290 val_loss: 0.2513667643070221, train_loss: 0.07833108305931091\n",
            "22300 val_loss: 0.2461593747138977, train_loss: 0.07658883184194565\n",
            "22310 val_loss: 0.24044860899448395, train_loss: 0.07739558815956116\n",
            "22320 val_loss: 0.23541468381881714, train_loss: 0.07661299407482147\n",
            "22330 val_loss: 0.24634890258312225, train_loss: 0.07567325234413147\n",
            "22340 val_loss: 0.24003350734710693, train_loss: 0.07668235152959824\n",
            "22350 val_loss: 0.24525898694992065, train_loss: 0.07725891470909119\n",
            "22360 val_loss: 0.2338077276945114, train_loss: 0.07614267617464066\n",
            "22370 val_loss: 0.24077506363391876, train_loss: 0.07574988156557083\n",
            "22380 val_loss: 0.24994780123233795, train_loss: 0.07687937468290329\n",
            "22390 val_loss: 0.24382922053337097, train_loss: 0.07648533582687378\n",
            "22400 val_loss: 0.23796336352825165, train_loss: 0.07608311623334885\n",
            "22410 val_loss: 0.23564842343330383, train_loss: 0.07643783837556839\n",
            "22420 val_loss: 0.2448352873325348, train_loss: 0.07626619189977646\n",
            "22430 val_loss: 0.24609874188899994, train_loss: 0.07927269488573074\n",
            "22440 val_loss: 0.2337782233953476, train_loss: 0.08002634346485138\n",
            "22450 val_loss: 0.24211502075195312, train_loss: 0.0760650560259819\n",
            "22460 val_loss: 0.2403184324502945, train_loss: 0.07461757212877274\n",
            "22470 val_loss: 0.23748251795768738, train_loss: 0.07530075311660767\n",
            "22480 val_loss: 0.25604766607284546, train_loss: 0.0768161192536354\n",
            "22490 val_loss: 0.23948800563812256, train_loss: 0.07447664439678192\n",
            "22500 val_loss: 0.2402527630329132, train_loss: 0.07512237876653671\n",
            "22510 val_loss: 0.23429332673549652, train_loss: 0.0746462419629097\n",
            "22520 val_loss: 0.23926839232444763, train_loss: 0.07544200122356415\n",
            "22530 val_loss: 0.2575155198574066, train_loss: 0.07718019187450409\n",
            "22540 val_loss: 0.23829664289951324, train_loss: 0.07420163601636887\n",
            "22550 val_loss: 0.23671582341194153, train_loss: 0.07477479428052902\n",
            "22560 val_loss: 0.24172265827655792, train_loss: 0.07428083568811417\n",
            "22570 val_loss: 0.24638807773590088, train_loss: 0.07371879369020462\n",
            "22580 val_loss: 0.24264231324195862, train_loss: 0.07440271973609924\n",
            "22590 val_loss: 0.2345755398273468, train_loss: 0.07386673986911774\n",
            "22600 val_loss: 0.23791758716106415, train_loss: 0.07362477481365204\n",
            "22610 val_loss: 0.24979393184185028, train_loss: 0.07487491518259048\n",
            "22620 val_loss: 0.2534392178058624, train_loss: 0.07832677662372589\n",
            "22630 val_loss: 0.23886874318122864, train_loss: 0.07502812147140503\n",
            "22640 val_loss: 0.2412402480840683, train_loss: 0.07403325289487839\n",
            "22650 val_loss: 0.24337442219257355, train_loss: 0.07448442280292511\n",
            "22660 val_loss: 0.25423792004585266, train_loss: 0.0756835862994194\n",
            "22670 val_loss: 0.2509269118309021, train_loss: 0.07517766952514648\n",
            "22680 val_loss: 0.23967769742012024, train_loss: 0.07348015159368515\n",
            "22690 val_loss: 0.2342497557401657, train_loss: 0.07380697876214981\n",
            "22700 val_loss: 0.2477182149887085, train_loss: 0.0735895112156868\n",
            "22710 val_loss: 0.23522844910621643, train_loss: 0.07363160699605942\n",
            "22720 val_loss: 0.2382059097290039, train_loss: 0.07358197122812271\n",
            "22730 val_loss: 0.2339838594198227, train_loss: 0.07261054962873459\n",
            "22740 val_loss: 0.25001558661460876, train_loss: 0.07483178377151489\n",
            "22750 val_loss: 0.23238813877105713, train_loss: 0.07305870205163956\n",
            "22760 val_loss: 0.23177137970924377, train_loss: 0.0723043829202652\n",
            "22770 val_loss: 0.24809108674526215, train_loss: 0.0745093822479248\n",
            "22780 val_loss: 0.23409803211688995, train_loss: 0.07286768406629562\n",
            "22790 val_loss: 0.2317483276128769, train_loss: 0.07280188053846359\n",
            "22800 val_loss: 0.24277496337890625, train_loss: 0.07420430332422256\n",
            "22810 val_loss: 0.24502651393413544, train_loss: 0.07237683236598969\n",
            "22820 val_loss: 0.2387637197971344, train_loss: 0.07203013449907303\n",
            "22830 val_loss: 0.23745107650756836, train_loss: 0.07173897325992584\n",
            "22840 val_loss: 0.2374102771282196, train_loss: 0.07191120088100433\n",
            "22850 val_loss: 0.23955830931663513, train_loss: 0.0723922848701477\n",
            "22860 val_loss: 0.24016736447811127, train_loss: 0.0717909038066864\n",
            "22870 val_loss: 0.2326529175043106, train_loss: 0.0722675696015358\n",
            "22880 val_loss: 0.2340656965970993, train_loss: 0.07216239720582962\n",
            "22890 val_loss: 0.2361537665128708, train_loss: 0.07184360176324844\n",
            "22900 val_loss: 0.23996007442474365, train_loss: 0.07172421365976334\n",
            "22910 val_loss: 0.23664960265159607, train_loss: 0.07123205810785294\n",
            "22920 val_loss: 0.2354288101196289, train_loss: 0.0712001696228981\n",
            "22930 val_loss: 0.2408735454082489, train_loss: 0.07168697565793991\n",
            "22940 val_loss: 0.24730341136455536, train_loss: 0.07216925173997879\n",
            "22950 val_loss: 0.23700793087482452, train_loss: 0.0710688978433609\n",
            "22960 val_loss: 0.23865440487861633, train_loss: 0.07095833122730255\n",
            "22970 val_loss: 0.23093344271183014, train_loss: 0.07237216085195541\n",
            "22980 val_loss: 0.23283179104328156, train_loss: 0.07127723097801208\n",
            "22990 val_loss: 0.23144733905792236, train_loss: 0.07071070373058319\n",
            "23000 val_loss: 0.231889545917511, train_loss: 0.07085120677947998\n",
            "23010 val_loss: 0.23911240696907043, train_loss: 0.07161509990692139\n",
            "23020 val_loss: 0.243451327085495, train_loss: 0.07069364935159683\n",
            "23030 val_loss: 0.23533006012439728, train_loss: 0.07046488672494888\n",
            "23040 val_loss: 0.23900815844535828, train_loss: 0.07056114822626114\n",
            "23050 val_loss: 0.2396533340215683, train_loss: 0.07149739563465118\n",
            "23060 val_loss: 0.23323523998260498, train_loss: 0.07024174928665161\n",
            "23070 val_loss: 0.23284681141376495, train_loss: 0.07039394974708557\n",
            "23080 val_loss: 0.24069026112556458, train_loss: 0.0705762505531311\n",
            "23090 val_loss: 0.23725482821464539, train_loss: 0.07077577710151672\n",
            "23100 val_loss: 0.24616046249866486, train_loss: 0.07039782404899597\n",
            "23110 val_loss: 0.23471809923648834, train_loss: 0.0701524168252945\n",
            "23120 val_loss: 0.22825472056865692, train_loss: 0.07028300315141678\n",
            "23130 val_loss: 0.23531198501586914, train_loss: 0.06955038756132126\n",
            "23140 val_loss: 0.23410698771476746, train_loss: 0.06957021355628967\n",
            "23150 val_loss: 0.23568132519721985, train_loss: 0.06911927461624146\n",
            "23160 val_loss: 0.23644889891147614, train_loss: 0.06888247281312943\n",
            "23170 val_loss: 0.2333870381116867, train_loss: 0.06961993873119354\n",
            "23180 val_loss: 0.24696826934814453, train_loss: 0.06939950585365295\n",
            "23190 val_loss: 0.2326488196849823, train_loss: 0.06933130323886871\n",
            "23200 val_loss: 0.22907784581184387, train_loss: 0.06953895092010498\n",
            "23210 val_loss: 0.2354469746351242, train_loss: 0.06901709735393524\n",
            "23220 val_loss: 0.22361469268798828, train_loss: 0.06975860893726349\n",
            "23230 val_loss: 0.23147380352020264, train_loss: 0.06859774142503738\n",
            "23240 val_loss: 0.23466847836971283, train_loss: 0.06924410909414291\n",
            "23250 val_loss: 0.22674855589866638, train_loss: 0.0681542158126831\n",
            "23260 val_loss: 0.23888596892356873, train_loss: 0.06820516288280487\n",
            "23270 val_loss: 0.23595015704631805, train_loss: 0.0691935122013092\n",
            "23280 val_loss: 0.24730075895786285, train_loss: 0.06977919489145279\n",
            "23290 val_loss: 0.2266831398010254, train_loss: 0.06775286793708801\n",
            "23300 val_loss: 0.2484007626771927, train_loss: 0.0702633336186409\n",
            "23310 val_loss: 0.2327812761068344, train_loss: 0.06838372349739075\n",
            "23320 val_loss: 0.22882218658924103, train_loss: 0.06862383335828781\n",
            "23330 val_loss: 0.23605333268642426, train_loss: 0.06741702556610107\n",
            "23340 val_loss: 0.2358045130968094, train_loss: 0.06786622107028961\n",
            "23350 val_loss: 0.23470976948738098, train_loss: 0.06772282719612122\n",
            "23360 val_loss: 0.2281375378370285, train_loss: 0.07431492209434509\n",
            "23370 val_loss: 0.22630445659160614, train_loss: 0.06835709512233734\n",
            "23380 val_loss: 0.23440617322921753, train_loss: 0.06715048849582672\n",
            "23390 val_loss: 0.2299949824810028, train_loss: 0.06743007898330688\n",
            "23400 val_loss: 0.22967325150966644, train_loss: 0.06773609668016434\n",
            "23410 val_loss: 0.23451045155525208, train_loss: 0.0676405057311058\n",
            "23420 val_loss: 0.23195800185203552, train_loss: 0.06765226274728775\n",
            "23430 val_loss: 0.22774410247802734, train_loss: 0.06800346821546555\n",
            "23440 val_loss: 0.23687414824962616, train_loss: 0.06747497618198395\n",
            "23450 val_loss: 0.23449788987636566, train_loss: 0.06761293858289719\n",
            "23460 val_loss: 0.23080503940582275, train_loss: 0.06728605180978775\n",
            "23470 val_loss: 0.23012486100196838, train_loss: 0.06723328679800034\n",
            "23480 val_loss: 0.2376866638660431, train_loss: 0.06727149337530136\n",
            "23490 val_loss: 0.23776310682296753, train_loss: 0.06663065403699875\n",
            "23500 val_loss: 0.25761064887046814, train_loss: 0.06867320090532303\n",
            "23510 val_loss: 0.25104957818984985, train_loss: 0.0692339539527893\n",
            "23520 val_loss: 0.23421630263328552, train_loss: 0.06657079607248306\n",
            "23530 val_loss: 0.23662497103214264, train_loss: 0.0662030279636383\n",
            "23540 val_loss: 0.23209813237190247, train_loss: 0.06829804927110672\n",
            "23550 val_loss: 0.23109790682792664, train_loss: 0.06681352853775024\n",
            "23560 val_loss: 0.23184964060783386, train_loss: 0.06713010370731354\n",
            "23570 val_loss: 0.23353521525859833, train_loss: 0.06619668751955032\n",
            "23580 val_loss: 0.2740586996078491, train_loss: 0.07324083149433136\n",
            "23590 val_loss: 0.22669236361980438, train_loss: 0.06616031378507614\n",
            "23600 val_loss: 0.22529059648513794, train_loss: 0.06686287373304367\n",
            "23610 val_loss: 0.22841708362102509, train_loss: 0.06652561575174332\n",
            "23620 val_loss: 0.23272046446800232, train_loss: 0.06601638346910477\n",
            "23630 val_loss: 0.2365494668483734, train_loss: 0.06555527448654175\n",
            "23640 val_loss: 0.2295180708169937, train_loss: 0.06517345458269119\n",
            "23650 val_loss: 0.22868601977825165, train_loss: 0.06509120762348175\n",
            "23660 val_loss: 0.23384223878383636, train_loss: 0.06528881937265396\n",
            "23670 val_loss: 0.22458752989768982, train_loss: 0.06479940563440323\n",
            "23680 val_loss: 0.22877810895442963, train_loss: 0.06470412760972977\n",
            "23690 val_loss: 0.22495707869529724, train_loss: 0.06572441011667252\n",
            "23700 val_loss: 0.23194517195224762, train_loss: 0.06493690609931946\n",
            "23710 val_loss: 0.22755305469036102, train_loss: 0.06500500440597534\n",
            "23720 val_loss: 0.23211443424224854, train_loss: 0.06445752084255219\n",
            "23730 val_loss: 0.24139273166656494, train_loss: 0.06572513282299042\n",
            "23740 val_loss: 0.23095551133155823, train_loss: 0.06396643817424774\n",
            "23750 val_loss: 0.23826459050178528, train_loss: 0.06390724331140518\n",
            "23760 val_loss: 0.22356489300727844, train_loss: 0.06289952993392944\n",
            "23770 val_loss: 0.22338758409023285, train_loss: 0.062025558203458786\n",
            "23780 val_loss: 0.23158186674118042, train_loss: 0.06049991026520729\n",
            "23790 val_loss: 0.22537028789520264, train_loss: 0.06099919229745865\n",
            "23800 val_loss: 0.23247818648815155, train_loss: 0.06089995801448822\n",
            "23810 val_loss: 0.22427436709403992, train_loss: 0.05946706235408783\n",
            "23820 val_loss: 0.23372268676757812, train_loss: 0.059343159198760986\n",
            "23830 val_loss: 0.24205997586250305, train_loss: 0.05905931442975998\n",
            "23840 val_loss: 0.23347647488117218, train_loss: 0.05539019778370857\n",
            "23850 val_loss: 0.233910471200943, train_loss: 0.05665658041834831\n",
            "23860 val_loss: 0.22550423443317413, train_loss: 0.05534825101494789\n",
            "23870 val_loss: 0.22613070905208588, train_loss: 0.054289910942316055\n",
            "23880 val_loss: 0.22703173756599426, train_loss: 0.0536825992166996\n",
            "23890 val_loss: 0.22126440703868866, train_loss: 0.05377280339598656\n",
            "23900 val_loss: 0.222754567861557, train_loss: 0.058530695736408234\n",
            "23910 val_loss: 0.21443410217761993, train_loss: 0.05350061506032944\n",
            "23920 val_loss: 0.23037633299827576, train_loss: 0.05418132245540619\n",
            "23930 val_loss: 0.2317546159029007, train_loss: 0.054237980395555496\n",
            "23940 val_loss: 0.23060278594493866, train_loss: 0.05232395976781845\n",
            "23950 val_loss: 0.2167506217956543, train_loss: 0.05053851380944252\n",
            "23960 val_loss: 0.22667309641838074, train_loss: 0.05230070650577545\n",
            "23970 val_loss: 0.2213503122329712, train_loss: 0.051742177456617355\n",
            "23980 val_loss: 0.21502597630023956, train_loss: 0.04970923811197281\n",
            "23990 val_loss: 0.22506679594516754, train_loss: 0.05076490715146065\n",
            "24000 val_loss: 0.21919254958629608, train_loss: 0.04904568940401077\n",
            "24010 val_loss: 0.2167387753725052, train_loss: 0.05007535219192505\n",
            "24020 val_loss: 0.21006645262241364, train_loss: 0.05006493255496025\n",
            "24030 val_loss: 0.21989606320858002, train_loss: 0.04865158349275589\n",
            "24040 val_loss: 0.22741945087909698, train_loss: 0.050501104444265366\n",
            "24050 val_loss: 0.21505892276763916, train_loss: 0.048346951603889465\n",
            "24060 val_loss: 0.22664156556129456, train_loss: 0.048080313950777054\n",
            "24070 val_loss: 0.21852219104766846, train_loss: 0.04932086914777756\n",
            "24080 val_loss: 0.22208066284656525, train_loss: 0.04830895736813545\n",
            "24090 val_loss: 0.21168752014636993, train_loss: 0.04789404943585396\n",
            "24100 val_loss: 0.21838419139385223, train_loss: 0.0500699058175087\n",
            "24110 val_loss: 0.22558607161045074, train_loss: 0.04877061769366264\n",
            "24120 val_loss: 0.21235136687755585, train_loss: 0.04715617001056671\n",
            "24130 val_loss: 0.21275624632835388, train_loss: 0.0481320358812809\n",
            "24140 val_loss: 0.21922162175178528, train_loss: 0.04657263681292534\n",
            "24150 val_loss: 0.20910583436489105, train_loss: 0.04587087407708168\n",
            "24160 val_loss: 0.2220257967710495, train_loss: 0.04708217456936836\n",
            "24170 val_loss: 0.21950115263462067, train_loss: 0.0450768917798996\n",
            "24180 val_loss: 0.21393337845802307, train_loss: 0.04434781149029732\n",
            "24190 val_loss: 0.20618145167827606, train_loss: 0.045121967792510986\n",
            "24200 val_loss: 0.2156570702791214, train_loss: 0.04428189992904663\n",
            "24210 val_loss: 0.21885576844215393, train_loss: 0.046002715826034546\n",
            "24220 val_loss: 0.21376001834869385, train_loss: 0.04563559219241142\n",
            "24230 val_loss: 0.21510019898414612, train_loss: 0.04686405882239342\n",
            "24240 val_loss: 0.20718005299568176, train_loss: 0.04346391558647156\n",
            "24250 val_loss: 0.20301453769207, train_loss: 0.04460713639855385\n",
            "24260 val_loss: 0.2221817523241043, train_loss: 0.04568726569414139\n",
            "24270 val_loss: 0.21474075317382812, train_loss: 0.04401445388793945\n",
            "24280 val_loss: 0.2116599977016449, train_loss: 0.04352003335952759\n",
            "24290 val_loss: 0.21364541351795197, train_loss: 0.04261508584022522\n",
            "24300 val_loss: 0.19393478333950043, train_loss: 0.04406066983938217\n",
            "24310 val_loss: 0.21213072538375854, train_loss: 0.04367464408278465\n",
            "24320 val_loss: 0.21345146000385284, train_loss: 0.0440545454621315\n",
            "24330 val_loss: 0.20735767483711243, train_loss: 0.04500846564769745\n",
            "24340 val_loss: 0.21062807738780975, train_loss: 0.042447879910469055\n",
            "24350 val_loss: 0.21306069195270538, train_loss: 0.042621172964572906\n",
            "24360 val_loss: 0.22540156543254852, train_loss: 0.04329061880707741\n",
            "24370 val_loss: 0.20033177733421326, train_loss: 0.04351088032126427\n",
            "24380 val_loss: 0.20568729937076569, train_loss: 0.042450711131095886\n",
            "24390 val_loss: 0.20765525102615356, train_loss: 0.04281356558203697\n",
            "24400 val_loss: 0.20492853224277496, train_loss: 0.041522104293107986\n",
            "24410 val_loss: 0.21576769649982452, train_loss: 0.04270293191075325\n",
            "24420 val_loss: 0.2075597494840622, train_loss: 0.04323594272136688\n",
            "24430 val_loss: 0.2164187878370285, train_loss: 0.04048576578497887\n",
            "24440 val_loss: 0.19909007847309113, train_loss: 0.041272301226854324\n",
            "24450 val_loss: 0.20725293457508087, train_loss: 0.04074561968445778\n",
            "24460 val_loss: 0.20416978001594543, train_loss: 0.04096946120262146\n",
            "24470 val_loss: 0.21315093338489532, train_loss: 0.04086139798164368\n",
            "24480 val_loss: 0.2135886698961258, train_loss: 0.04029100760817528\n",
            "24490 val_loss: 0.21255914866924286, train_loss: 0.04138091579079628\n",
            "24500 val_loss: 0.2183622568845749, train_loss: 0.04213062673807144\n",
            "24510 val_loss: 0.2062690556049347, train_loss: 0.04007204249501228\n",
            "24520 val_loss: 0.21300390362739563, train_loss: 0.04318966343998909\n",
            "24530 val_loss: 0.2131815254688263, train_loss: 0.04014746844768524\n",
            "24540 val_loss: 0.19559629261493683, train_loss: 0.0404386967420578\n",
            "24550 val_loss: 0.21412809193134308, train_loss: 0.03887886181473732\n",
            "24560 val_loss: 0.20538781583309174, train_loss: 0.03860468789935112\n",
            "24570 val_loss: 0.2130117416381836, train_loss: 0.039305876940488815\n",
            "24580 val_loss: 0.24168053269386292, train_loss: 0.04784347116947174\n",
            "24590 val_loss: 0.20360848307609558, train_loss: 0.0393412746489048\n",
            "24600 val_loss: 0.2048061043024063, train_loss: 0.038560084998607635\n",
            "24610 val_loss: 0.19739922881126404, train_loss: 0.03905615210533142\n",
            "24620 val_loss: 0.2117127627134323, train_loss: 0.0387638621032238\n",
            "24630 val_loss: 0.2014952003955841, train_loss: 0.038271207362413406\n",
            "24640 val_loss: 0.1972099095582962, train_loss: 0.038801975548267365\n",
            "24650 val_loss: 0.20272259414196014, train_loss: 0.03862909600138664\n",
            "24660 val_loss: 0.19595234096050262, train_loss: 0.03852963075041771\n",
            "24670 val_loss: 0.20024894177913666, train_loss: 0.03804286941885948\n",
            "24680 val_loss: 0.20687709748744965, train_loss: 0.03752419725060463\n",
            "24690 val_loss: 0.212273508310318, train_loss: 0.03750535473227501\n",
            "24700 val_loss: 0.20533907413482666, train_loss: 0.03841501474380493\n",
            "24710 val_loss: 0.20620830357074738, train_loss: 0.03813078626990318\n",
            "24720 val_loss: 0.19808943569660187, train_loss: 0.037156589329242706\n",
            "24730 val_loss: 0.20942556858062744, train_loss: 0.03924127668142319\n",
            "24740 val_loss: 0.26869913935661316, train_loss: 0.057131022214889526\n",
            "24750 val_loss: 0.21249476075172424, train_loss: 0.03805210068821907\n",
            "24760 val_loss: 0.20439399778842926, train_loss: 0.03684931993484497\n",
            "24770 val_loss: 0.2119901329278946, train_loss: 0.03669898211956024\n",
            "24780 val_loss: 0.19614246487617493, train_loss: 0.03656064718961716\n",
            "24790 val_loss: 0.20566511154174805, train_loss: 0.037231702357530594\n",
            "24800 val_loss: 0.20822221040725708, train_loss: 0.03656221553683281\n",
            "24810 val_loss: 0.20680968463420868, train_loss: 0.03684716671705246\n",
            "24820 val_loss: 0.20205533504486084, train_loss: 0.03636791184544563\n",
            "24830 val_loss: 0.20480310916900635, train_loss: 0.0387970469892025\n",
            "24840 val_loss: 0.20358414947986603, train_loss: 0.036086469888687134\n",
            "24850 val_loss: 0.20887744426727295, train_loss: 0.037655215710401535\n",
            "24860 val_loss: 0.20938891172409058, train_loss: 0.036832138895988464\n",
            "24870 val_loss: 0.20682989060878754, train_loss: 0.0357942208647728\n",
            "24880 val_loss: 0.21264226734638214, train_loss: 0.03688759356737137\n",
            "24890 val_loss: 0.19739115238189697, train_loss: 0.03535447642207146\n",
            "24900 val_loss: 0.21263417601585388, train_loss: 0.03631806746125221\n",
            "24910 val_loss: 0.1973879188299179, train_loss: 0.03666745498776436\n",
            "24920 val_loss: 0.19947068393230438, train_loss: 0.035996656864881516\n",
            "24930 val_loss: 0.20681989192962646, train_loss: 0.03659665957093239\n",
            "24940 val_loss: 0.20344622433185577, train_loss: 0.03583982214331627\n",
            "24950 val_loss: 0.2102900892496109, train_loss: 0.03549522906541824\n",
            "24960 val_loss: 0.2139098048210144, train_loss: 0.040660396218299866\n",
            "24970 val_loss: 0.21192452311515808, train_loss: 0.035339828580617905\n",
            "24980 val_loss: 0.20569215714931488, train_loss: 0.03542248532176018\n",
            "24990 val_loss: 0.19701679050922394, train_loss: 0.03550305962562561\n",
            "25000 val_loss: 0.21275945007801056, train_loss: 0.03634806349873543\n",
            "25010 val_loss: 0.19936947524547577, train_loss: 0.03524412587285042\n",
            "25020 val_loss: 0.20694267749786377, train_loss: 0.034724265336990356\n",
            "25030 val_loss: 0.20567594468593597, train_loss: 0.03459269180893898\n",
            "25040 val_loss: 0.2131744623184204, train_loss: 0.034205999225378036\n",
            "25050 val_loss: 0.21020208299160004, train_loss: 0.03530789911746979\n",
            "25060 val_loss: 0.20136797428131104, train_loss: 0.03403579443693161\n",
            "25070 val_loss: 0.21371303498744965, train_loss: 0.035284966230392456\n",
            "25080 val_loss: 0.21123048663139343, train_loss: 0.037895187735557556\n",
            "25090 val_loss: 0.20098529756069183, train_loss: 0.03414540737867355\n",
            "25100 val_loss: 0.20351266860961914, train_loss: 0.03368451073765755\n",
            "25110 val_loss: 0.21162259578704834, train_loss: 0.03435872867703438\n",
            "25120 val_loss: 0.20715738832950592, train_loss: 0.035219620913267136\n",
            "25130 val_loss: 0.20977531373500824, train_loss: 0.0333021879196167\n",
            "25140 val_loss: 0.2127794474363327, train_loss: 0.03346317261457443\n",
            "25150 val_loss: 0.2151523381471634, train_loss: 0.03293614089488983\n",
            "25160 val_loss: 0.21901221573352814, train_loss: 0.035125188529491425\n",
            "25170 val_loss: 0.22344258427619934, train_loss: 0.033828672021627426\n",
            "25180 val_loss: 0.20827558636665344, train_loss: 0.032738298177719116\n",
            "25190 val_loss: 0.20676720142364502, train_loss: 0.03298459202051163\n",
            "25200 val_loss: 0.2165125608444214, train_loss: 0.032618772238492966\n",
            "25210 val_loss: 0.21287289261817932, train_loss: 0.032924775034189224\n",
            "25220 val_loss: 0.21491926908493042, train_loss: 0.032239869236946106\n",
            "25230 val_loss: 0.20812086760997772, train_loss: 0.03482870012521744\n",
            "25240 val_loss: 0.20097215473651886, train_loss: 0.03256721794605255\n",
            "25250 val_loss: 0.2090483009815216, train_loss: 0.03246806189417839\n",
            "25260 val_loss: 0.1932796984910965, train_loss: 0.03337608277797699\n",
            "25270 val_loss: 0.19958986341953278, train_loss: 0.03222569078207016\n",
            "25280 val_loss: 0.21206362545490265, train_loss: 0.03337150067090988\n",
            "25290 val_loss: 0.2005547136068344, train_loss: 0.032323773950338364\n",
            "25300 val_loss: 0.20958073437213898, train_loss: 0.032207224518060684\n",
            "25310 val_loss: 0.20867611467838287, train_loss: 0.032217126339673996\n",
            "25320 val_loss: 0.21443650126457214, train_loss: 0.03206213563680649\n",
            "25330 val_loss: 0.21050608158111572, train_loss: 0.03192711994051933\n",
            "25340 val_loss: 0.21156653761863708, train_loss: 0.031636472791433334\n",
            "25350 val_loss: 0.24273695051670074, train_loss: 0.04100463166832924\n",
            "25360 val_loss: 0.20358683168888092, train_loss: 0.03293393924832344\n",
            "25370 val_loss: 0.21651728451251984, train_loss: 0.033625390380620956\n",
            "25380 val_loss: 0.22604955732822418, train_loss: 0.032774731516838074\n",
            "25390 val_loss: 0.21142828464508057, train_loss: 0.031488630920648575\n",
            "25400 val_loss: 0.19593217968940735, train_loss: 0.03127430006861687\n",
            "25410 val_loss: 0.22533991932868958, train_loss: 0.03371378779411316\n",
            "25420 val_loss: 0.20629090070724487, train_loss: 0.033225614577531815\n",
            "25430 val_loss: 0.20846465229988098, train_loss: 0.03780093789100647\n",
            "25440 val_loss: 0.20356205105781555, train_loss: 0.03253287822008133\n",
            "25450 val_loss: 0.21158243715763092, train_loss: 0.03175225481390953\n",
            "25460 val_loss: 0.2062849998474121, train_loss: 0.03183932974934578\n",
            "25470 val_loss: 0.2219485342502594, train_loss: 0.031420912593603134\n",
            "25480 val_loss: 0.2076157182455063, train_loss: 0.03136948496103287\n",
            "25490 val_loss: 0.21656282246112823, train_loss: 0.03261983022093773\n",
            "25500 val_loss: 0.19365975260734558, train_loss: 0.03147229179739952\n",
            "25510 val_loss: 0.1944476068019867, train_loss: 0.031139502301812172\n",
            "25520 val_loss: 0.2095770388841629, train_loss: 0.030642038211226463\n",
            "25530 val_loss: 0.21457034349441528, train_loss: 0.031543079763650894\n",
            "25540 val_loss: 0.19206063449382782, train_loss: 0.03199653699994087\n",
            "25550 val_loss: 0.19644655287265778, train_loss: 0.03125949203968048\n",
            "25560 val_loss: 0.20105378329753876, train_loss: 0.030754420906305313\n",
            "25570 val_loss: 0.20284807682037354, train_loss: 0.030517449602484703\n",
            "25580 val_loss: 0.20404717326164246, train_loss: 0.03013237565755844\n",
            "25590 val_loss: 0.1937803030014038, train_loss: 0.031285159289836884\n",
            "25600 val_loss: 0.1994399130344391, train_loss: 0.030015280470252037\n",
            "25610 val_loss: 0.21001963317394257, train_loss: 0.029786445200443268\n",
            "25620 val_loss: 0.2026316374540329, train_loss: 0.02958032488822937\n",
            "25630 val_loss: 0.21183742582798004, train_loss: 0.030807770788669586\n",
            "25640 val_loss: 0.19237717986106873, train_loss: 0.030192920938134193\n",
            "25650 val_loss: 0.21621139347553253, train_loss: 0.03047017753124237\n",
            "25660 val_loss: 0.21166837215423584, train_loss: 0.030199194326996803\n",
            "25670 val_loss: 0.2129964530467987, train_loss: 0.029240449890494347\n",
            "25680 val_loss: 0.20825538039207458, train_loss: 0.02974349819123745\n",
            "25690 val_loss: 0.20830675959587097, train_loss: 0.02979467809200287\n",
            "25700 val_loss: 0.21018703281879425, train_loss: 0.029259398579597473\n",
            "25710 val_loss: 0.22641919553279877, train_loss: 0.030101696029305458\n",
            "25720 val_loss: 0.1960485726594925, train_loss: 0.028920168057084084\n",
            "25730 val_loss: 0.21354958415031433, train_loss: 0.029035942628979683\n",
            "25740 val_loss: 0.20144285261631012, train_loss: 0.029103534296154976\n",
            "25750 val_loss: 0.21577292680740356, train_loss: 0.029309354722499847\n",
            "25760 val_loss: 0.20434415340423584, train_loss: 0.0298796147108078\n",
            "25770 val_loss: 0.19883687794208527, train_loss: 0.029044872149825096\n",
            "25780 val_loss: 0.2069455087184906, train_loss: 0.02876773476600647\n",
            "25790 val_loss: 0.21105453372001648, train_loss: 0.028282059356570244\n",
            "25800 val_loss: 0.2192276567220688, train_loss: 0.03241952881217003\n",
            "25810 val_loss: 0.2209114283323288, train_loss: 0.030016377568244934\n",
            "25820 val_loss: 0.21429027616977692, train_loss: 0.029051844030618668\n",
            "25830 val_loss: 0.2003825306892395, train_loss: 0.028557492420077324\n",
            "25840 val_loss: 0.1969500035047531, train_loss: 0.028731832280755043\n",
            "25850 val_loss: 0.21011488139629364, train_loss: 0.029562553390860558\n",
            "25860 val_loss: 0.21656717360019684, train_loss: 0.028798239305615425\n",
            "25870 val_loss: 0.22058075666427612, train_loss: 0.029019411653280258\n",
            "25880 val_loss: 0.20265205204486847, train_loss: 0.02886868268251419\n",
            "25890 val_loss: 0.21149341762065887, train_loss: 0.02915208227932453\n",
            "25900 val_loss: 0.20670145750045776, train_loss: 0.028675343841314316\n",
            "25910 val_loss: 0.20433689653873444, train_loss: 0.02834763564169407\n",
            "25920 val_loss: 0.21309569478034973, train_loss: 0.0294902715831995\n",
            "25930 val_loss: 0.1938270628452301, train_loss: 0.028340008109807968\n",
            "25940 val_loss: 0.20054274797439575, train_loss: 0.028540536761283875\n",
            "25950 val_loss: 0.2000306248664856, train_loss: 0.028098970651626587\n",
            "25960 val_loss: 0.2021455019712448, train_loss: 0.028281865641474724\n",
            "25970 val_loss: 0.18930742144584656, train_loss: 0.028226930648088455\n",
            "25980 val_loss: 0.19598892331123352, train_loss: 0.02849007211625576\n",
            "25990 val_loss: 0.21468500792980194, train_loss: 0.027663474902510643\n",
            "26000 val_loss: 0.21278569102287292, train_loss: 0.02792811393737793\n",
            "26010 val_loss: 0.21263492107391357, train_loss: 0.028580011799931526\n",
            "26020 val_loss: 0.20244906842708588, train_loss: 0.02769657038152218\n",
            "26030 val_loss: 0.21853502094745636, train_loss: 0.02802889049053192\n",
            "26040 val_loss: 0.2132791131734848, train_loss: 0.02730037458240986\n",
            "26050 val_loss: 0.2039186805486679, train_loss: 0.02745940536260605\n",
            "26060 val_loss: 0.19774194061756134, train_loss: 0.027507567778229713\n",
            "26070 val_loss: 0.20825304090976715, train_loss: 0.02711835689842701\n",
            "26080 val_loss: 0.21101222932338715, train_loss: 0.026972977444529533\n",
            "26090 val_loss: 0.21100866794586182, train_loss: 0.03255382180213928\n",
            "26100 val_loss: 0.19192788004875183, train_loss: 0.027825508266687393\n",
            "26110 val_loss: 0.19421499967575073, train_loss: 0.027090126648545265\n",
            "26120 val_loss: 0.18416255712509155, train_loss: 0.028333265334367752\n",
            "26130 val_loss: 0.22237496078014374, train_loss: 0.027649754658341408\n",
            "26140 val_loss: 0.21495530009269714, train_loss: 0.027333900332450867\n",
            "26150 val_loss: 0.20836859941482544, train_loss: 0.027161993086338043\n",
            "26160 val_loss: 0.20654137432575226, train_loss: 0.026964377611875534\n",
            "26170 val_loss: 0.21686360239982605, train_loss: 0.02720663696527481\n",
            "26180 val_loss: 0.20641596615314484, train_loss: 0.026965832337737083\n",
            "26190 val_loss: 0.21529266238212585, train_loss: 0.02723010443150997\n",
            "26200 val_loss: 0.2226095199584961, train_loss: 0.028442636132240295\n",
            "26210 val_loss: 0.21428269147872925, train_loss: 0.027816183865070343\n",
            "26220 val_loss: 0.2092316448688507, train_loss: 0.026929738000035286\n",
            "26230 val_loss: 0.20352251827716827, train_loss: 0.027064092457294464\n",
            "26240 val_loss: 0.21312573552131653, train_loss: 0.026754919439554214\n",
            "26250 val_loss: 0.21787980198860168, train_loss: 0.0269427839666605\n",
            "26260 val_loss: 0.20786814391613007, train_loss: 0.02657490409910679\n",
            "26270 val_loss: 0.19917747378349304, train_loss: 0.026923619210720062\n",
            "26280 val_loss: 0.22108018398284912, train_loss: 0.026457851752638817\n",
            "26290 val_loss: 0.2123800665140152, train_loss: 0.02839767187833786\n",
            "26300 val_loss: 0.19311124086380005, train_loss: 0.02901352010667324\n",
            "26310 val_loss: 0.20910675823688507, train_loss: 0.026612374931573868\n",
            "26320 val_loss: 0.1984499990940094, train_loss: 0.027332689613103867\n",
            "26330 val_loss: 0.2019539177417755, train_loss: 0.026348326355218887\n",
            "26340 val_loss: 0.1998288333415985, train_loss: 0.02630036696791649\n",
            "26350 val_loss: 0.20201323926448822, train_loss: 0.026626992970705032\n",
            "26360 val_loss: 0.20734284818172455, train_loss: 0.026227498427033424\n",
            "26370 val_loss: 0.20065966248512268, train_loss: 0.02613324299454689\n",
            "26380 val_loss: 0.20449095964431763, train_loss: 0.026008240878582\n",
            "26390 val_loss: 0.19629427790641785, train_loss: 0.026302101090550423\n",
            "26400 val_loss: 0.21212559938430786, train_loss: 0.02587355114519596\n",
            "26410 val_loss: 0.22213447093963623, train_loss: 0.026130085811018944\n",
            "26420 val_loss: 0.21743465960025787, train_loss: 0.025993146002292633\n",
            "26430 val_loss: 0.2213483303785324, train_loss: 0.02618381939828396\n",
            "26440 val_loss: 0.2045353651046753, train_loss: 0.026255423203110695\n",
            "26450 val_loss: 0.20949701964855194, train_loss: 0.026123855262994766\n",
            "26460 val_loss: 0.20964109897613525, train_loss: 0.025862369686365128\n",
            "26470 val_loss: 0.2108728587627411, train_loss: 0.02648404985666275\n",
            "26480 val_loss: 0.2078850120306015, train_loss: 0.026154473423957825\n",
            "26490 val_loss: 0.21287286281585693, train_loss: 0.02631930634379387\n",
            "26500 val_loss: 0.22448162734508514, train_loss: 0.026252349838614464\n",
            "26510 val_loss: 0.22572898864746094, train_loss: 0.025765806436538696\n",
            "26520 val_loss: 0.22900263965129852, train_loss: 0.026286188513040543\n",
            "26530 val_loss: 0.22383278608322144, train_loss: 0.025737473741173744\n",
            "26540 val_loss: 0.20234377682209015, train_loss: 0.026089755818247795\n",
            "26550 val_loss: 0.20617572963237762, train_loss: 0.026135189458727837\n",
            "26560 val_loss: 0.20935501158237457, train_loss: 0.025354474782943726\n",
            "26570 val_loss: 0.20825287699699402, train_loss: 0.025744078680872917\n",
            "26580 val_loss: 0.2267819344997406, train_loss: 0.02637193724513054\n",
            "26590 val_loss: 0.20650656521320343, train_loss: 0.02538336254656315\n",
            "26600 val_loss: 0.23456630110740662, train_loss: 0.02764599584043026\n",
            "26610 val_loss: 0.2119356393814087, train_loss: 0.025329526513814926\n",
            "26620 val_loss: 0.20731914043426514, train_loss: 0.02518591843545437\n",
            "26630 val_loss: 0.21044397354125977, train_loss: 0.025075014680624008\n",
            "26640 val_loss: 0.21272429823875427, train_loss: 0.02545757032930851\n",
            "26650 val_loss: 0.20733854174613953, train_loss: 0.025689348578453064\n",
            "26660 val_loss: 0.2440641075372696, train_loss: 0.027800966054201126\n",
            "26670 val_loss: 0.2059730589389801, train_loss: 0.025332171469926834\n",
            "26680 val_loss: 0.20122064650058746, train_loss: 0.025981420651078224\n",
            "26690 val_loss: 0.22867368161678314, train_loss: 0.026647550985217094\n",
            "26700 val_loss: 0.2158823013305664, train_loss: 0.027617011219263077\n",
            "26710 val_loss: 0.21354489028453827, train_loss: 0.025097304955124855\n",
            "26720 val_loss: 0.2171287089586258, train_loss: 0.025168299674987793\n",
            "26730 val_loss: 0.20742911100387573, train_loss: 0.025233520194888115\n",
            "26740 val_loss: 0.22225116193294525, train_loss: 0.025309331715106964\n",
            "26750 val_loss: 0.21397148072719574, train_loss: 0.024968864396214485\n",
            "26760 val_loss: 0.21791943907737732, train_loss: 0.025814106687903404\n",
            "26770 val_loss: 0.19928905367851257, train_loss: 0.024985576048493385\n",
            "26780 val_loss: 0.19342800974845886, train_loss: 0.026273498311638832\n",
            "26790 val_loss: 0.2057226449251175, train_loss: 0.024917274713516235\n",
            "26800 val_loss: 0.26587989926338196, train_loss: 0.04260055348277092\n",
            "26810 val_loss: 0.2273755818605423, train_loss: 0.026404667645692825\n",
            "26820 val_loss: 0.1976310908794403, train_loss: 0.02507324516773224\n",
            "26830 val_loss: 0.2067294418811798, train_loss: 0.024817798286676407\n",
            "26840 val_loss: 0.2026320844888687, train_loss: 0.025142889469861984\n",
            "26850 val_loss: 0.19408763945102692, train_loss: 0.025134116411209106\n",
            "26860 val_loss: 0.20250943303108215, train_loss: 0.02501653879880905\n",
            "26870 val_loss: 0.19792872667312622, train_loss: 0.02646450698375702\n",
            "26880 val_loss: 0.2022734135389328, train_loss: 0.02482210285961628\n",
            "26890 val_loss: 0.23403319716453552, train_loss: 0.02598126046359539\n",
            "26900 val_loss: 0.22248132526874542, train_loss: 0.024803338572382927\n",
            "26910 val_loss: 0.22087958455085754, train_loss: 0.026354026049375534\n",
            "26920 val_loss: 0.250692218542099, train_loss: 0.028362860903143883\n",
            "26930 val_loss: 0.20813517272472382, train_loss: 0.02547065168619156\n",
            "26940 val_loss: 0.2140597105026245, train_loss: 0.024396168068051338\n",
            "26950 val_loss: 0.20748856663703918, train_loss: 0.024437744170427322\n",
            "26960 val_loss: 0.21425849199295044, train_loss: 0.0244152769446373\n",
            "26970 val_loss: 0.20566856861114502, train_loss: 0.025578338652849197\n",
            "26980 val_loss: 0.2211511880159378, train_loss: 0.02405724674463272\n",
            "26990 val_loss: 0.20479175448417664, train_loss: 0.024242129176855087\n",
            "27000 val_loss: 0.22215761244297028, train_loss: 0.023885542526841164\n",
            "27010 val_loss: 0.23092839121818542, train_loss: 0.024905890226364136\n",
            "27020 val_loss: 0.1987641155719757, train_loss: 0.023972107097506523\n",
            "27030 val_loss: 0.19898183643817902, train_loss: 0.02423986606299877\n",
            "27040 val_loss: 0.21454933285713196, train_loss: 0.02413453534245491\n",
            "27050 val_loss: 0.21432268619537354, train_loss: 0.02443738281726837\n",
            "27060 val_loss: 0.20389172434806824, train_loss: 0.024317264556884766\n",
            "27070 val_loss: 0.21821454167366028, train_loss: 0.024094875901937485\n",
            "27080 val_loss: 0.22492313385009766, train_loss: 0.024080965667963028\n",
            "27090 val_loss: 0.20423655211925507, train_loss: 0.02450818195939064\n",
            "27100 val_loss: 0.2035934329032898, train_loss: 0.02487124688923359\n",
            "27110 val_loss: 0.20348380506038666, train_loss: 0.025947529822587967\n",
            "Early stopping! min step : 26120\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assumes `t_losses` and `val_losses` are lists of loss values\n",
        "plt.plot(t_losses, label='Training loss')\n",
        "plt.plot(val_losses, label='Validation loss')\n",
        "plt.title('Training and Validation Losses')\n",
        "plt.xlabel('Steps (x100)')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "uoQHct7qsmxU",
        "outputId": "721a8b42-42b5-4a9f-e01c-c1ce1bffadac"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}