{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN+lm3hAFm/FVVDWduvxAbp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JunHyeong-data/ML-DL-Study/blob/main/Basic-Deep-Learning/20_CNN%2C_DNN_%EA%B8%B0%EC%B4%88_%2C_Data_augmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Augmentation (ë°ì´í„° ì–´ê·¸ë©˜í…Œì´ì…˜)\n",
        "\n",
        "## 1. ì™œ ë°ì´í„° ì–´ê·¸ë©˜í…Œì´ì…˜ì´ í•„ìš”í•œê°€?\n",
        "\n",
        "ë‰´ëŸ´ ë„¤íŠ¸ì›Œí¬ íŠ¸ë ˆì´ë‹ì—ì„œ ë§¤ìš° ì¤‘ìš”í•œ ë¬¸ì œ ì¤‘ í•˜ë‚˜ëŠ” **ì˜¤ë²„í”¼íŒ…(Overfitting)** ê³¼  \n",
        "**ì œë„ˆëŸ´ë¼ì´ì œì´ì…˜(Generalization) ë¬¸ì œ**ì´ë‹¤.\n",
        "\n",
        "- íŠ¸ë ˆì´ë‹ ë°ì´í„°ì—ì„œëŠ” ì„±ëŠ¥ì´ ì¢‹ìŒ\n",
        "- í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ì„œëŠ” ì„±ëŠ¥ì´ ê¸‰ê²©íˆ ë–¨ì–´ì§  \n",
        "â†’ ì´ë¥¼ **Generalization Problem**ì´ë¼ê³  í•œë‹¤\n",
        "\n",
        "ì´ ë¬¸ì œë¥¼ ì™„í™”í•˜ê¸° ìœ„í•´:\n",
        "- Dropout\n",
        "- Regularization  \n",
        "ê°™ì€ ê¸°ë²•ë“¤ì„ ì‚¬ìš©í•˜ì§€ë§Œ,\n",
        "\n",
        "**ê°€ì¥ ê·¼ë³¸ì ì¸ í•´ê²° ë°©ë²•ì€ ë” ë§ê³  ë‹¤ì–‘í•œ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒ**ì´ë‹¤.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. í˜„ì‹¤ì ì¸ ë¬¸ì œ\n",
        "\n",
        "ì´ë¡ ì ìœ¼ë¡œëŠ”:\n",
        "- ë°ì´í„°ê°€ ë§ì„ìˆ˜ë¡\n",
        "- ë‹¤ì–‘í• ìˆ˜ë¡  \n",
        "ë‰´ëŸ´ ë„¤íŠ¸ì›Œí¬ëŠ” ë” ì˜ í•™ìŠµí•œë‹¤\n",
        "\n",
        "í•˜ì§€ë§Œ í˜„ì‹¤ì—ì„œëŠ”:\n",
        "- ë°ì´í„° ìˆ˜ì§‘ ë¹„ìš©ì´ í¼\n",
        "- ë¼ë²¨ë§ì— ì‹œê°„ê³¼ ì¸ë ¥ì´ í•„ìš”í•¨\n",
        "\n",
        "â†’ ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ê°„ë‹¨í•˜ê³  ê°•ë ¥í•œ ë°©ë²•ì´  \n",
        "**Data Augmentation**ì´ë‹¤.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. ë°ì´í„° ì–´ê·¸ë©˜í…Œì´ì…˜ì´ë€?\n",
        "\n",
        "**ë°ì´í„° ì–´ê·¸ë©˜í…Œì´ì…˜(Data Augmentation)** ì€  \n",
        "ê¸°ì¡´ íŠ¸ë ˆì´ë‹ ë°ì´í„°ë¥¼ ë³€í˜•í•˜ì—¬  \n",
        "**ë°ì´í„°ì˜ ì–‘ì´ ë§ì•„ ë³´ì´ë„ë¡ ë§Œë“œëŠ” ê¸°ë²•**ì´ë‹¤.\n",
        "\n",
        "í•µì‹¬ ì•„ì´ë””ì–´:\n",
        "> ì˜ë¯¸ëŠ” ê°™ì§€ë§Œ, í˜•íƒœê°€ ë‹¤ë¥¸ ë°ì´í„°ë¥¼ ë§Œë“¤ì–´ë‚¸ë‹¤\n",
        "\n",
        "---\n",
        "\n",
        "## 4. ì´ë¯¸ì§€ ë°ì´í„° ì˜ˆì‹œ (ê³ ì–‘ì´ ì´ë¯¸ì§€)\n",
        "\n",
        "í•˜ë‚˜ì˜ ê³ ì–‘ì´ ì´ë¯¸ì§€ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë‹¤ìŒê³¼ ê°™ì€ ë³€í˜•ì„ ì ìš©í•  ìˆ˜ ìˆë‹¤.\n",
        "\n",
        "### 4.1 ê³µê°„ ë³€í˜• (Geometric Transform)\n",
        "- ì¢Œìš° ë°˜ì „ (Horizontal Flip)\n",
        "- ìƒí•˜ ë°˜ì „ (Vertical Flip)\n",
        "- íšŒì „ (Rotation)\n",
        "- í™•ëŒ€ / ì¶•ì†Œ (Zoom In / Out)\n",
        "- í¬ë¡­ (Crop)\n",
        "\n",
        "ğŸ‘‰ ëª¨ë‘ **ê³ ì–‘ì´ í´ë˜ìŠ¤**ë¡œ ìœ ì§€ ê°€ëŠ¥\n",
        "\n",
        "---\n",
        "\n",
        "### 4.2 í˜•íƒœ ë³€í˜• (Deformation)\n",
        "- íŠ¹ì • ì˜ì—­ë§Œ ëŠ˜ë¦¬ê±°ë‚˜ ì¤„ì´ê¸°\n",
        "- ë¶€ë¶„ì ì¸ ì™œê³¡ ì ìš©\n",
        "\n",
        "---\n",
        "\n",
        "### 4.3 ë…¸ì´ì¦ˆ ì¶”ê°€ (Noise)\n",
        "- ëœë¤ ë…¸ì´ì¦ˆ ì‚½ì…\n",
        "- ì´ë¯¸ì§€ í’ˆì§ˆì´ ì¡°ê¸ˆ ë‚˜ë¹ ì ¸ë„ í´ë˜ìŠ¤ ìœ ì§€ ê°€ëŠ¥\n",
        "\n",
        "---\n",
        "\n",
        "### 4.4 ìƒ‰ìƒ ë³€í˜• (Color Jitter)\n",
        "- ë°ê¸° ë³€ê²½\n",
        "- ëŒ€ë¹„ ì¡°ì ˆ\n",
        "- ìƒ‰ìƒ ë³€í™”\n",
        "- íŠ¹ì • ë¶€ìœ„ ìƒ‰ìƒ ê°•ì¡°\n",
        "\n",
        "---\n",
        "\n",
        "## 5. íš¨ê³¼\n",
        "\n",
        "- í•˜ë‚˜ì˜ ì´ë¯¸ì§€ë¡œë¶€í„°\n",
        "  - 4ë°°\n",
        "  - 8ë°°\n",
        "  - ê·¸ ì´ìƒì˜ ë°ì´í„° ìƒì„± ê°€ëŠ¥\n",
        "- ë°ì´í„° ë¶„í¬ ë‹¤ì–‘í™”\n",
        "- ì˜¤ë²„í”¼íŒ… ê°ì†Œ\n",
        "- ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ\n",
        "\n",
        "---\n",
        "\n",
        "## 6. PyTorchì—ì„œì˜ Data Augmentation\n",
        "\n",
        ":contentReference[oaicite:0]{index=0}ì™€  \n",
        ":contentReference[oaicite:1]{index=1}ì—ì„œëŠ”  \n",
        "ì´ë¯¸ì§€ ì–´ê·¸ë©˜í…Œì´ì…˜ì„ ìœ„í•œ ë‹¤ì–‘í•œ íŠ¸ëœìŠ¤í¬ë©”ì´ì…˜ì„ ì œê³µí•œë‹¤.\n",
        "\n",
        "### ì˜ˆì‹œ ê¸°ëŠ¥\n",
        "- Padding\n",
        "- Resize\n",
        "- Random Crop\n",
        "- Random Flip\n",
        "- Rotation\n",
        "- Color Jitter\n",
        "\n",
        "---\n",
        "\n",
        "## 7. Train / Test ë°ì´í„° êµ¬ë¶„ (ì¤‘ìš” âš ï¸)\n",
        "\n",
        "### âœ” Train Dataset\n",
        "- **Data Augmentation ì ìš©**\n",
        "- ëª¨ë¸ì´ ë‹¤ì–‘í•œ ìƒí™©ì„ í•™ìŠµí•˜ë„ë¡ ìœ ë„\n",
        "\n",
        "### âŒ Test Dataset\n",
        "- **ì›ë³¸ ë°ì´í„° ê·¸ëŒ€ë¡œ ì‚¬ìš©**\n",
        "- ì‹¤ì œ ì„±ëŠ¥ì„ ê³µì •í•˜ê²Œ í‰ê°€í•˜ê¸° ìœ„í•¨\n",
        "\n",
        "ë”°ë¼ì„œ:\n",
        "- `train_transform`\n",
        "- `test_transform`  \n",
        "ì„ **ë°˜ë“œì‹œ ë¶„ë¦¬í•´ì„œ ì •ì˜**í•´ì•¼ í•œë‹¤.\n",
        "\n",
        "---\n",
        "\n",
        "## 8. ì •ë¦¬\n",
        "\n",
        "ì´ë²ˆ ê°•ì˜ì˜ í•µì‹¬ í¬ì¸íŠ¸:\n",
        "\n",
        "- Data Augmentationì€ ì ì€ ë¹„ìš©ìœ¼ë¡œ ë°ì´í„° ì–‘ì„ ëŠ˜ë¦¬ëŠ” ë°©ë²•\n",
        "- ì˜¤ë²„í”¼íŒ…ê³¼ ì œë„ˆëŸ´ë¼ì´ì œì´ì…˜ ë¬¸ì œ í•´ê²°ì— ë§¤ìš° íš¨ê³¼ì \n",
        "- ì»´í“¨í„° ë¹„ì „ CNN íŠ¸ë ˆì´ë‹ì—ì„œëŠ” **ê±°ì˜ í•„ìˆ˜ ê¸°ë²•**\n",
        "- Train ë°ì´í„°ì—ë§Œ ì ìš©í•´ì•¼ í•¨\n",
        "\n",
        "---\n",
        "\n",
        "## 9. ë‹¤ìŒ ê°•ì˜ ì˜ˆê³ \n",
        "\n",
        "ë‹¤ìŒ ì‹œê°„ì—ëŠ”:\n",
        "- **ResNet**\n",
        "- ê·¸ ì™¸ ìœ ëª…í•œ CNN ì•„í‚¤í…ì²˜ë“¤\n",
        "\n",
        "ì— ëŒ€í•´ ì•Œì•„ë³¼ ì˜ˆì •ì´ë‹¤.\n"
      ],
      "metadata": {
        "id": "mzHLbFw1LWWX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACVNBQc1LRmC",
        "outputId": "0efd3826-8223-4c91-a164-eab2c231162b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "if torch.backends.mps.is_available():\n",
        "  my_device = torch.device('cuda')\n",
        "else:\n",
        "  my_device = torch.device('cpu')\n",
        "\n",
        "print(my_device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Load CIFAR10 dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.RandomRotation(30),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=8, shuffle=True, num_workers=8)\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=8, shuffle=False, num_workers=8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GgVd1p0rL1s_",
        "outputId": "7ccef2eb-1188-4e52-b28e-b6d0add1e229"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170M/170M [00:02<00:00, 63.0MB/s]\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DoubleConvBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super().__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x)\n",
        "\n",
        "class ModernGAPCNN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "\n",
        "        self.features = nn.Sequential(\n",
        "            DoubleConvBlock(3, 64),\n",
        "            DoubleConvBlock(64, 128, stride=2),\n",
        "            DoubleConvBlock(128, 256, stride=2),\n",
        "        )\n",
        "\n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.classifier = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.global_avg_pool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "# Instantiate the network\n",
        "net = ModernGAPCNN(num_classes=10)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.0001)"
      ],
      "metadata": {
        "id": "xcBlOg4JNecb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net.to(my_device)\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    net.train()\n",
        "    for batch_idx, (data, label) in enumerate(trainloader):\n",
        "        data, label = data.to(my_device), label.to(my_device)\n",
        "        scores = net(data)\n",
        "        loss = criterion(scores, label)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    net.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, label in testloader:\n",
        "            data, label = data.to(my_device), label.to(my_device)\n",
        "            scores = net(data)\n",
        "            loss = criterion(scores, label)\n",
        "            val_loss += loss.item() * data.size(0)\n",
        "\n",
        "            predicted = scores.argmax(dim=1)\n",
        "            correct += predicted.eq(label).sum().item()\n",
        "\n",
        "    val_loss /= len(testloader.dataset)\n",
        "    val_accuracy = 100. * correct / len(testloader.dataset)\n",
        "\n",
        "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Training Loss: {loss.item():.4f}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")"
      ],
      "metadata": {
        "id": "PzVUoF_eNpfi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}